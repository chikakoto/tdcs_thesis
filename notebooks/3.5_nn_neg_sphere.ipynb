{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pickle\n",
    "# import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/\"\n",
    "# dir_path = \"/Users/mriworkshop/Documents/TDCS/code/tdcs_thesis/\"\n",
    "save_path = dir_path+\"data/raw/\"\n",
    "img_path =  dir_path+\"data/processed/\"\n",
    "model_path = dir_path+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmap mean all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_sphere = save_path+\"fmap_mean_neg_amp_42.txt\"\n",
    "columns_mean =['exp', 'mini_exp', 'i', 'j', 'k', 'amp', 'neg', 'mean0', 'mean1', 'mean2', 'theory']\n",
    "data = np.loadtxt(file_sphere);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081339</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.865481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081340</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.910840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081341</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.957465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081342</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.005388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081343</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.054645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1081344 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg  mean0  mean1  mean2    theory\n",
       "0         42         1   0   0   0    2    0    0.0    0.0    0.0  0.578396\n",
       "1         42         1   0   0   1    2    0    0.0    0.0    0.0  0.584610\n",
       "2         42         1   0   0   2    2    0    0.0    0.0    0.0  0.590888\n",
       "3         42         1   0   0   3    2    0    0.0    0.0    0.0  0.597230\n",
       "4         42         1   0   0   4    2    0    0.0    0.0    0.0  0.603638\n",
       "...      ...       ...  ..  ..  ..  ...  ...    ...    ...    ...       ...\n",
       "1081339   42         3  43  63  59    2    1    0.0    0.0    0.0  1.865481\n",
       "1081340   42         3  43  63  60    2    1    0.0    0.0    0.0  1.910840\n",
       "1081341   42         3  43  63  61    2    1    0.0    0.0    0.0  1.957465\n",
       "1081342   42         3  43  63  62    2    1    0.0    0.0    0.0  2.005388\n",
       "1081343   42         3  43  63  63    2    1    0.0    0.0    0.0  2.054645\n",
       "\n",
       "[1081344 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=columns_mean)\n",
    "df = df.astype({\"exp\": int, \"i\": int, \"j\": int, \"k\": int, \"mini_exp\": int, \"amp\":int, \"neg\": int})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1081344 entries, 0 to 1081343\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count    Dtype  \n",
      "---  ------    --------------    -----  \n",
      " 0   exp       1081344 non-null  int64  \n",
      " 1   mini_exp  1081344 non-null  int64  \n",
      " 2   i         1081344 non-null  int64  \n",
      " 3   j         1081344 non-null  int64  \n",
      " 4   k         1081344 non-null  int64  \n",
      " 5   amp       1081344 non-null  int64  \n",
      " 6   neg       1081344 non-null  int64  \n",
      " 7   mean0     1081344 non-null  float64\n",
      " 8   mean1     1081344 non-null  float64\n",
      " 9   mean2     1081344 non-null  float64\n",
      " 10  theory    1081344 non-null  float64\n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 90.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 1. fmap mean all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train = df[~((df['exp']==42) & ((df['mini_exp']==2) | (df['mini_exp']==3)))]\n",
    "df1_val =  df[(df['exp']==42) & (df['mini_exp']==2)]\n",
    "df1_test =  df[(df['exp']==42) & (df['mini_exp']==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360448, 11)\n",
      "(360448, 11)\n",
      "(360448, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df1_train.shape)\n",
    "print(df1_val.shape)\n",
    "print(df1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360443</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360444</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360445</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360446</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360447</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360448 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        neg  mean0  mean1  mean2\n",
       "0         0    0.0    0.0    0.0\n",
       "1         0    0.0    0.0    0.0\n",
       "2         0    0.0    0.0    0.0\n",
       "3         0    0.0    0.0    0.0\n",
       "4         0    0.0    0.0    0.0\n",
       "...     ...    ...    ...    ...\n",
       "360443    1    0.0    0.0    0.0\n",
       "360444    1    0.0    0.0    0.0\n",
       "360445    1    0.0    0.0    0.0\n",
       "360446    1    0.0    0.0    0.0\n",
       "360447    1    0.0    0.0    0.0\n",
       "\n",
       "[360448 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_train.iloc[:, 6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = df1_train.iloc[:, 6:-1].values \n",
    "y1_train = df1_train['theory'].values\n",
    "\n",
    "X1_test = df1_val.iloc[:, 6:-1].values \n",
    "y1_test = df1_val['theory'].values\n",
    "\n",
    "X1_pred = df1_test.iloc[:, 6:-1].values \n",
    "y1_pred = df1_val['theory'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360448, 4)\n",
      "(360448,)\n",
      "(360448, 4)\n",
      "(360448,)\n",
      "(360448, 4)\n",
      "(360448,)\n"
     ]
    }
   ],
   "source": [
    "print(X1_train.shape)\n",
    "print(y1_train.shape)\n",
    "print(X1_test.shape)\n",
    "print(y1_test.shape)\n",
    "print(X1_pred.shape)\n",
    "print(y1_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data4: None zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonzero = df[(df['mean0']!=0.0) & (df['mean1']!=0.0) & (df['mean2']!=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10074</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-984.707153</td>\n",
       "      <td>-970.159729</td>\n",
       "      <td>-974.554504</td>\n",
       "      <td>0.798312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10076</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-333.152374</td>\n",
       "      <td>-270.754669</td>\n",
       "      <td>-102.585724</td>\n",
       "      <td>0.819827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10137</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-788.129700</td>\n",
       "      <td>-787.770996</td>\n",
       "      <td>-808.987488</td>\n",
       "      <td>0.786963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1038.672119</td>\n",
       "      <td>-1019.283020</td>\n",
       "      <td>-1018.710083</td>\n",
       "      <td>0.797582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-352.913147</td>\n",
       "      <td>-328.458252</td>\n",
       "      <td>-345.069885</td>\n",
       "      <td>0.837726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066912</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>531.209412</td>\n",
       "      <td>524.617249</td>\n",
       "      <td>528.361572</td>\n",
       "      <td>1.120325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066965</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-738.791687</td>\n",
       "      <td>-309.708313</td>\n",
       "      <td>248.529739</td>\n",
       "      <td>0.942770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066966</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-854.326355</td>\n",
       "      <td>-827.500061</td>\n",
       "      <td>-840.415222</td>\n",
       "      <td>0.957650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067028</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-524.266296</td>\n",
       "      <td>-949.864380</td>\n",
       "      <td>-932.718628</td>\n",
       "      <td>0.927170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067029</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-783.382935</td>\n",
       "      <td>-823.340027</td>\n",
       "      <td>-807.056702</td>\n",
       "      <td>0.941901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162768 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg        mean0        mean1  \\\n",
       "10074     42         1   2  29  26    2    0  -984.707153  -970.159729   \n",
       "10076     42         1   2  29  28    2    0  -333.152374  -270.754669   \n",
       "10137     42         1   2  30  25    2    0  -788.129700  -787.770996   \n",
       "10138     42         1   2  30  26    2    0 -1038.672119 -1019.283020   \n",
       "13853     42         1   3  24  29    2    0  -352.913147  -328.458252   \n",
       "...      ...       ...  ..  ..  ..  ...  ...          ...          ...   \n",
       "1066912   42         3  40  30  32    2    1   531.209412   524.617249   \n",
       "1066965   42         3  40  31  21    2    1  -738.791687  -309.708313   \n",
       "1066966   42         3  40  31  22    2    1  -854.326355  -827.500061   \n",
       "1067028   42         3  40  32  20    2    1  -524.266296  -949.864380   \n",
       "1067029   42         3  40  32  21    2    1  -783.382935  -823.340027   \n",
       "\n",
       "               mean2    theory  \n",
       "10074    -974.554504  0.798312  \n",
       "10076    -102.585724  0.819827  \n",
       "10137    -808.987488  0.786963  \n",
       "10138   -1018.710083  0.797582  \n",
       "13853    -345.069885  0.837726  \n",
       "...              ...       ...  \n",
       "1066912   528.361572  1.120325  \n",
       "1066965   248.529739  0.942770  \n",
       "1066966  -840.415222  0.957650  \n",
       "1067028  -932.718628  0.927170  \n",
       "1067029  -807.056702  0.941901  \n",
       "\n",
       "[162768 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_train = df_nonzero[~((df_nonzero['exp']==42) & ((df_nonzero['mini_exp']==2) | (df_nonzero['mini_exp']==3)))]\n",
    "df4_test =  df_nonzero[(df_nonzero['exp']==42) & (df_nonzero['mini_exp']==2)]\n",
    "df4_pred =  df_nonzero[(df_nonzero['exp']==42) & (df_nonzero['mini_exp']==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train = df4_train.iloc[:, 6:-1].values\n",
    "y4_train = df4_train['theory'].values\n",
    "\n",
    "X4_test = df4_test.iloc[:, 6:-1].values\n",
    "y4_test = df4_test['theory'].values\n",
    "\n",
    "X4_pred = df4_pred.iloc[:, 6:-1].values\n",
    "y4_pred = df4_pred['theory'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54256, 4)\n",
      "(54256,)\n",
      "(54256, 4)\n",
      "(54256,)\n",
      "(54256, 4)\n",
      "(54256,)\n"
     ]
    }
   ],
   "source": [
    "print(X4_train.shape)\n",
    "print(y4_train.shape)\n",
    "print(X4_test.shape)\n",
    "print(y4_test.shape) \n",
    "print(X4_pred.shape)\n",
    "print(y4_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X1_train\n",
    "y_train = y1_train\n",
    "X_test = X1_test\n",
    "y_test = y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (len(X_train[0]),)\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=shape)) \n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='RMSprop', loss='mse', metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(model_path+'best_model_nn_42.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=[X_test, y_test], callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "print(mse(train_pred, y_train))\n",
    "print(mape(train_pred, y_train))\n",
    "test_pred = model.predict(X_test)\n",
    "print(mse(test_pred, y_test))\n",
    "print(mape(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(test_pred.flatten(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(history.history)\n",
    "model_df[['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_df[['accuracy', 'val_accuracy']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuray Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN for NonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X4_train\n",
    "y_train = y4_train\n",
    "X_test = X4_test\n",
    "y_test = y4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (len(X_train[0]),)\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=shape)) \n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='RMSprop', loss='mse', metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(model_path+'best_model_nn_42_nonzero.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 1: val_loss improved from 0.01734 to 0.01733, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_42_nonzero.h5\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 2/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 2: val_loss did not improve from 0.01733\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 3/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0207 - mse: 0.0207\n",
      "Epoch 3: val_loss did not improve from 0.01733\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0205 - mse: 0.0205 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 4/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 4: val_loss did not improve from 0.01733\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 5/300\n",
      "805/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 5: val_loss did not improve from 0.01733\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 6/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 6: val_loss improved from 0.01733 to 0.01732, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_42_nonzero.h5\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 7/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 7: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 8/300\n",
      "804/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 8: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 9/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 9: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 10/300\n",
      "824/848 [============================>.] - ETA: 0s - loss: 0.0186 - mse: 0.0186\n",
      "Epoch 10: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 11/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 11: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 12/300\n",
      "838/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 12: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 13/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 13: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 14/300\n",
      "831/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 14: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 15/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0182 - mse: 0.0182\n",
      "Epoch 15: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 16/300\n",
      "838/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 16: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 17/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 17: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 18/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 18: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 19/300\n",
      "804/848 [===========================>..] - ETA: 0s - loss: 0.0182 - mse: 0.0182\n",
      "Epoch 19: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 20/300\n",
      "831/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 20: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 21/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 21: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 22/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0185 - mse: 0.0185\n",
      "Epoch 22: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0184 - mse: 0.0184 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 23/300\n",
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 23: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 24/300\n",
      "847/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 24: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0185 - val_mse: 0.0185\n",
      "Epoch 25/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0182 - mse: 0.0182\n",
      "Epoch 25: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 26/300\n",
      "803/848 [===========================>..] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 26: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 27/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 27: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 28/300\n",
      "841/848 [============================>.] - ETA: 0s - loss: 0.0289 - mse: 0.0289\n",
      "Epoch 28: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0288 - mse: 0.0288 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 29/300\n",
      "813/848 [===========================>..] - ETA: 0s - loss: 0.0182 - mse: 0.0182\n",
      "Epoch 29: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 30/300\n",
      "834/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 30: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 31/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0209 - mse: 0.0209\n",
      "Epoch 31: val_loss did not improve from 0.01732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0209 - mse: 0.0209 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 32/300\n",
      "842/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 32: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 33/300\n",
      "813/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 33: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0191 - val_mse: 0.0191\n",
      "Epoch 34/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0206 - mse: 0.0206\n",
      "Epoch 34: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 35/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 35: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 36/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 36: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 37/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 37: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 38/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0181 - mse: 0.0181\n",
      "Epoch 38: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 39/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 39: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 40/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 40: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 41/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 41: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 42/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 42: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 43/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 43: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 44/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 44: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0192 - val_mse: 0.0192\n",
      "Epoch 45/300\n",
      "803/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 45: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 46/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 46: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 47/300\n",
      "838/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 47: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 48/300\n",
      "847/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 48: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 49/300\n",
      "809/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 49: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 50/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 50: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 51/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 51: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0187 - val_mse: 0.0187\n",
      "Epoch 52/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 52: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 53/300\n",
      "816/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 53: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0199 - val_mse: 0.0199\n",
      "Epoch 54/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 54: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0194 - val_mse: 0.0194\n",
      "Epoch 55/300\n",
      "836/848 [============================>.] - ETA: 0s - loss: 0.0186 - mse: 0.0186\n",
      "Epoch 55: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0186 - mse: 0.0186 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 56/300\n",
      "822/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 56: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 57/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 57: val_loss did not improve from 0.01732\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 58/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 58: val_loss improved from 0.01732 to 0.01731, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_42_nonzero.h5\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 59/300\n",
      "813/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 59: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0188 - val_mse: 0.0188\n",
      "Epoch 60/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 60: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 61/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 61: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 62/300\n",
      "841/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 62: val_loss did not improve from 0.01731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 63/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 63: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 64/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 64: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 65/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 65: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 66/300\n",
      "804/848 [===========================>..] - ETA: 0s - loss: 0.0184 - mse: 0.0184\n",
      "Epoch 66: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0183 - mse: 0.0183 - val_loss: 0.0303 - val_mse: 0.0303\n",
      "Epoch 67/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 67: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0203 - val_mse: 0.0203\n",
      "Epoch 68/300\n",
      "834/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 68: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0196 - val_mse: 0.0196\n",
      "Epoch 69/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0192 - mse: 0.0192\n",
      "Epoch 69: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 70/300\n",
      "798/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 70: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 71/300\n",
      "834/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 71: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 72/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 72: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 73/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 73: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 74/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 74: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 75/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 75: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 76/300\n",
      "830/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 76: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 77/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 77: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 78/300\n",
      "795/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 78: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 79/300\n",
      "819/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 79: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 80/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 80: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 81/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 81: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 1ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 82/300\n",
      "841/848 [============================>.] - ETA: 0s - loss: 0.0177 - mse: 0.0177\n",
      "Epoch 82: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 83/300\n",
      "796/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 83: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 84/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 84: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 85/300\n",
      "822/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 85: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 86/300\n",
      "794/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 86: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 87/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 87: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 88/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 88: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 89/300\n",
      "805/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 89: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 90/300\n",
      "797/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 90: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0179 - val_mse: 0.0179\n",
      "Epoch 91/300\n",
      "803/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 91: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 92/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 92: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 93/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 93: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300\n",
      "819/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 94: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 95/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 95: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 96/300\n",
      "826/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 96: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 97/300\n",
      "805/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 97: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 98/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 98: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 99/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0177 - mse: 0.0177\n",
      "Epoch 99: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 100/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 100: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 101/300\n",
      "847/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 101: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0186 - val_mse: 0.0186\n",
      "Epoch 102/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0220 - mse: 0.0220\n",
      "Epoch 102: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0220 - mse: 0.0220 - val_loss: 0.0313 - val_mse: 0.0313\n",
      "Epoch 103/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
      "Epoch 103: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 104/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 104: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 105/300\n",
      "845/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 105: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 106/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 106: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 107/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 107: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 108/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 108: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 109/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
      "Epoch 109: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 110/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 110: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 111/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0218 - mse: 0.0218\n",
      "Epoch 111: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0217 - mse: 0.0217 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 112/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 112: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 113/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 113: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 114/300\n",
      "792/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 114: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 115/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 115: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 116/300\n",
      "797/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 116: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 117/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 117: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 118/300\n",
      "822/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 118: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 119/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 119: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 120/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 120: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 121/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 121: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 122/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 122: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 123/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 123: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 124/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 124: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0218 - val_mse: 0.0218\n",
      "Epoch 125/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "799/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 125: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 126/300\n",
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 126: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 127/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 127: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 128/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 128: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 129/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 129: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 130/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 130: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 131/300\n",
      "797/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 131: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0205 - val_mse: 0.0205\n",
      "Epoch 132/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 132: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0195 - mse: 0.0195 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 133/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 133: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0221 - val_mse: 0.0221\n",
      "Epoch 134/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 134: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 135/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 135: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 136/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 136: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 137/300\n",
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 137: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0214 - val_mse: 0.0214\n",
      "Epoch 138/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 138: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 139/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 139: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 140/300\n",
      "819/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 140: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 141/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0177 - mse: 0.0177\n",
      "Epoch 141: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 142/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 142: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0697 - val_mse: 0.0697\n",
      "Epoch 143/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0205 - mse: 0.0205\n",
      "Epoch 143: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0204 - mse: 0.0204 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 144/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 144: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0190 - val_mse: 0.0190\n",
      "Epoch 145/300\n",
      "845/848 [============================>.] - ETA: 0s - loss: 0.0176 - mse: 0.0176\n",
      "Epoch 145: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0176 - mse: 0.0176 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 146/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 146: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 147/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 147: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 148/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 148: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 149/300\n",
      "809/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 149: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 150/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 150: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 151/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 151: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 152/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 152: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 153/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 153: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 154/300\n",
      "845/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 154: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 155/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0183 - mse: 0.0183\n",
      "Epoch 155: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 156/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 156: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0285 - val_mse: 0.0285\n",
      "Epoch 157/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 157: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 158/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 158: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 159/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 159: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 160/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 160: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 161/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 161: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 162/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0180 - mse: 0.0180\n",
      "Epoch 162: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0180 - mse: 0.0180 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 163/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 163: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 164/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 164: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 165/300\n",
      "822/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 165: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 166/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 166: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0181 - val_mse: 0.0181\n",
      "Epoch 167/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 167: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 168/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 168: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 169/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 169: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 170/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 170: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 171/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 171: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 172/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 172: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 173/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 173: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 174/300\n",
      "797/848 [===========================>..] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 174: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0177 - mse: 0.0177 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 175/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 175: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 176/300\n",
      "813/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 176: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 177/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 177: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 178/300\n",
      "809/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 178: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 179/300\n",
      "798/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 179: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 180/300\n",
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0183 - mse: 0.0183\n",
      "Epoch 180: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0182 - mse: 0.0182 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 181/300\n",
      "838/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 181: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 182/300\n",
      "793/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 182: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 183/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 183: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0180 - val_mse: 0.0180\n",
      "Epoch 184/300\n",
      "794/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 184: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 185/300\n",
      "797/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 185: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 186/300\n",
      "819/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 186: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 187/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "829/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 187: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 188/300\n",
      "816/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 188: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 189/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 189: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 190/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 190: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 191/300\n",
      "808/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 191: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 192/300\n",
      "826/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 192: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 193/300\n",
      "800/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 193: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 194/300\n",
      "824/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 194: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 195/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 195: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 196/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 196: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 197/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 197: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 198/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 198: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 199/300\n",
      "806/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 199: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 200/300\n",
      "828/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 200: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 201/300\n",
      "799/848 [===========================>..] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 201: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0212 - val_mse: 0.0212\n",
      "Epoch 202/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 202: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 203/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 203: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 204/300\n",
      "833/848 [============================>.] - ETA: 0s - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 204: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0207 - mse: 0.0207 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 205/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0186 - mse: 0.0186\n",
      "Epoch 205: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 206/300\n",
      "832/848 [============================>.] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 206: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0178 - val_mse: 0.0178\n",
      "Epoch 207/300\n",
      "794/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 207: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 208/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 208: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 209/300\n",
      "823/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 209: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 210/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 210: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 211/300\n",
      "822/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 211: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 212/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 212: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 213/300\n",
      "815/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 213: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 214/300\n",
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 214: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 215/300\n",
      "824/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 215: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 216/300\n",
      "838/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 216: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 217/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 217: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 218/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 218: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 219/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 219: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 220/300\n",
      "804/848 [===========================>..] - ETA: 0s - loss: 0.0179 - mse: 0.0179\n",
      "Epoch 220: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0179 - mse: 0.0179 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 221/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 221: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 222/300\n",
      "847/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 222: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 223/300\n",
      "807/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 223: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 224/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 224: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 225/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 225: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 226/300\n",
      "816/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 226: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 227/300\n",
      "833/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 227: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 228/300\n",
      "826/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 228: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 229/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 229: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 230/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 230: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 231/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 231: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 232/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0178 - mse: 0.0178\n",
      "Epoch 232: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0178 - mse: 0.0178 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 233/300\n",
      "839/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 233: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 234/300\n",
      "835/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 234: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 235/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 235: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 236/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 236: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 237/300\n",
      "836/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 237: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 238/300\n",
      "828/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 238: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 239/300\n",
      "837/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 239: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 240/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 240: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 241/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 241: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 242/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0181 - mse: 0.0181\n",
      "Epoch 242: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0181 - mse: 0.0181 - val_loss: 0.0215 - val_mse: 0.0215\n",
      "Epoch 243/300\n",
      "818/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 243: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 244/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 244: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 245/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 245: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 246/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 246: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 247/300\n",
      "836/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 247: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 248/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 248: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 249/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 249: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 250/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 250: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 251/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 251: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 252/300\n",
      "816/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 252: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 253/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 253: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 254/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 254: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 255/300\n",
      "824/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 255: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 256/300\n",
      "824/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 256: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 257/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 257: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0175 - val_mse: 0.0175\n",
      "Epoch 258/300\n",
      "819/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 258: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 259/300\n",
      "810/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 259: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 260/300\n",
      "821/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 260: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 261/300\n",
      "817/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 261: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 262/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 262: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 263/300\n",
      "834/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 263: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 264/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 264: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 265/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0183 - mse: 0.0183\n",
      "Epoch 265: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0191 - mse: 0.0191 - val_loss: 0.3339 - val_mse: 0.3339\n",
      "Epoch 266/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0187 - mse: 0.0187\n",
      "Epoch 266: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0187 - mse: 0.0187 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 267/300\n",
      "805/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 267: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 268/300\n",
      "802/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 268: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 269/300\n",
      "844/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 269: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 270/300\n",
      "831/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 270: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 271/300\n",
      "798/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 271: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 272/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 272: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 273/300\n",
      "804/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 273: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 274/300\n",
      "801/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 274: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 275/300\n",
      "845/848 [============================>.] - ETA: 0s - loss: 0.0175 - mse: 0.0175\n",
      "Epoch 275: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0175 - mse: 0.0175 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 276/300\n",
      "845/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 276: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 277/300\n",
      "828/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 277: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 278/300\n",
      "825/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 278: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 279/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 279: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 280/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 280: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 281/300\n",
      "814/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 281: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 282/300\n",
      "827/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 282: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 283/300\n",
      "833/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 283: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 284/300\n",
      "843/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 284: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 285/300\n",
      "836/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 285: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 2s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 286/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 286: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 287/300\n",
      "809/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 287: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 288/300\n",
      "829/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 288: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0248 - val_mse: 0.0248\n",
      "Epoch 289/300\n",
      "816/848 [===========================>..] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 289: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 290/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 290: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 291/300\n",
      "820/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 291: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 292/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 292: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 293/300\n",
      "831/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 293: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0176 - val_mse: 0.0176\n",
      "Epoch 294/300\n",
      "840/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 294: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 295/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 295: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0173 - val_mse: 0.0173\n",
      "Epoch 296/300\n",
      "846/848 [============================>.] - ETA: 0s - loss: 0.0174 - mse: 0.0174\n",
      "Epoch 296: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0174 - mse: 0.0174 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 297/300\n",
      "848/848 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 297: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 298/300\n",
      "813/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 298: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0174 - val_mse: 0.0174\n",
      "Epoch 299/300\n",
      "792/848 [===========================>..] - ETA: 0s - loss: 0.0173 - mse: 0.0173\n",
      "Epoch 299: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0173 - mse: 0.0173 - val_loss: 0.0177 - val_mse: 0.0177\n",
      "Epoch 300/300\n",
      "812/848 [===========================>..] - ETA: 0s - loss: 0.0191 - mse: 0.0191\n",
      "Epoch 300: val_loss did not improve from 0.01731\n",
      "848/848 [==============================] - 1s 2ms/step - loss: 0.0190 - mse: 0.0190 - val_loss: 0.0176 - val_mse: 0.0176\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300, batch_size=64, validation_data=[X_test, y_test], callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696/1696 [==============================] - 1s 693us/step\n",
      "0.017336122938802923\n",
      "0.11153647488828944\n",
      "1696/1696 [==============================] - 1s 690us/step\n",
      "0.017568773611605037\n",
      "0.11164505975978797\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "print(mse(train_pred, y_train))\n",
    "print(mape(train_pred, y_train))\n",
    "test_pred = model.predict(X_test)\n",
    "print(mse(test_pred, y_test))\n",
    "print(mape(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696/1696 [==============================] - 1s 787us/step - loss: 0.0176 - mse: 0.0176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.01756878010928631, 0.01756878010928631]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.01459331],\n",
       "       [0.01459331, 1.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(test_pred.flatten(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61\n",
      "Trainable params: 61\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.017412154003977776,\n",
       "  0.01745527982711792,\n",
       "  0.020502636209130287,\n",
       "  0.01845235377550125,\n",
       "  0.017491541802883148,\n",
       "  0.017568254843354225,\n",
       "  0.017568467184901237,\n",
       "  0.017501942813396454,\n",
       "  0.017383545637130737,\n",
       "  0.018518580123782158,\n",
       "  0.017336400225758553,\n",
       "  0.017528321593999863,\n",
       "  0.017618738114833832,\n",
       "  0.017364483326673508,\n",
       "  0.018170535564422607,\n",
       "  0.01734144240617752,\n",
       "  0.01736733317375183,\n",
       "  0.017332125455141068,\n",
       "  0.018134094774723053,\n",
       "  0.01756437122821808,\n",
       "  0.017333585768938065,\n",
       "  0.01842622086405754,\n",
       "  0.017916390672326088,\n",
       "  0.017631376162171364,\n",
       "  0.018152352422475815,\n",
       "  0.017534272745251656,\n",
       "  0.017458802089095116,\n",
       "  0.028800196945667267,\n",
       "  0.018195824697613716,\n",
       "  0.01751874014735222,\n",
       "  0.020862704142928123,\n",
       "  0.017375728115439415,\n",
       "  0.017332162708044052,\n",
       "  0.020421279594302177,\n",
       "  0.017750779166817665,\n",
       "  0.01733609288930893,\n",
       "  0.017422884702682495,\n",
       "  0.018091842532157898,\n",
       "  0.01747754029929638,\n",
       "  0.017569344490766525,\n",
       "  0.01757424883544445,\n",
       "  0.019146379083395004,\n",
       "  0.0178996454924345,\n",
       "  0.017333362251520157,\n",
       "  0.017324965447187424,\n",
       "  0.01733316294848919,\n",
       "  0.017379088327288628,\n",
       "  0.017394280061125755,\n",
       "  0.017337465658783913,\n",
       "  0.01734573021531105,\n",
       "  0.01752086728811264,\n",
       "  0.017347142100334167,\n",
       "  0.01740753836929798,\n",
       "  0.017433837056159973,\n",
       "  0.01861678808927536,\n",
       "  0.017338918522000313,\n",
       "  0.017393816262483597,\n",
       "  0.017329717054963112,\n",
       "  0.017334071919322014,\n",
       "  0.017340732738375664,\n",
       "  0.01733170635998249,\n",
       "  0.017437949776649475,\n",
       "  0.01734527386724949,\n",
       "  0.017439493909478188,\n",
       "  0.017325442284345627,\n",
       "  0.018299954012036324,\n",
       "  0.017787888646125793,\n",
       "  0.017334818840026855,\n",
       "  0.01909620314836502,\n",
       "  0.0173640176653862,\n",
       "  0.017348140478134155,\n",
       "  0.017753560096025467,\n",
       "  0.017436040565371513,\n",
       "  0.017341788858175278,\n",
       "  0.017445409670472145,\n",
       "  0.017330428585410118,\n",
       "  0.017336530610919,\n",
       "  0.01733417809009552,\n",
       "  0.01733388938009739,\n",
       "  0.01735798455774784,\n",
       "  0.017335616052150726,\n",
       "  0.01766359992325306,\n",
       "  0.017346583306789398,\n",
       "  0.0173345934599638,\n",
       "  0.017335016280412674,\n",
       "  0.017334522679448128,\n",
       "  0.01739777997136116,\n",
       "  0.017361408099532127,\n",
       "  0.017329920083284378,\n",
       "  0.017345333471894264,\n",
       "  0.01733068749308586,\n",
       "  0.017332561314105988,\n",
       "  0.017330503091216087,\n",
       "  0.017335308715701103,\n",
       "  0.01737663708627224,\n",
       "  0.017477257177233696,\n",
       "  0.017332211136817932,\n",
       "  0.01733485236763954,\n",
       "  0.017728952690958977,\n",
       "  0.017366118729114532,\n",
       "  0.017333507537841797,\n",
       "  0.0219996590167284,\n",
       "  0.01795516535639763,\n",
       "  0.01732816733419895,\n",
       "  0.01735565811395645,\n",
       "  0.017843572422862053,\n",
       "  0.01733497716486454,\n",
       "  0.01736854575574398,\n",
       "  0.0179850235581398,\n",
       "  0.017462527379393578,\n",
       "  0.021722985431551933,\n",
       "  0.017328985035419464,\n",
       "  0.01732870750129223,\n",
       "  0.017344187945127487,\n",
       "  0.01733441837131977,\n",
       "  0.01733882538974285,\n",
       "  0.017391104251146317,\n",
       "  0.01741150952875614,\n",
       "  0.017361627891659737,\n",
       "  0.01736588031053543,\n",
       "  0.017352359369397163,\n",
       "  0.01764744333922863,\n",
       "  0.017360636964440346,\n",
       "  0.01737532950937748,\n",
       "  0.017329365015029907,\n",
       "  0.017393343150615692,\n",
       "  0.017360424622893333,\n",
       "  0.017591964453458786,\n",
       "  0.017446711659431458,\n",
       "  0.017349544912576675,\n",
       "  0.01732897013425827,\n",
       "  0.019517099484801292,\n",
       "  0.017328867688775063,\n",
       "  0.01772559992969036,\n",
       "  0.01780315861105919,\n",
       "  0.017330395057797432,\n",
       "  0.017367741093039513,\n",
       "  0.017901934683322906,\n",
       "  0.017328312620520592,\n",
       "  0.01742326095700264,\n",
       "  0.017674587666988373,\n",
       "  0.017327111214399338,\n",
       "  0.0203982163220644,\n",
       "  0.01732747256755829,\n",
       "  0.017640676349401474,\n",
       "  0.017326563596725464,\n",
       "  0.0173649862408638,\n",
       "  0.017325682565569878,\n",
       "  0.017322465777397156,\n",
       "  0.01732684299349785,\n",
       "  0.017325490713119507,\n",
       "  0.017326142638921738,\n",
       "  0.017325520515441895,\n",
       "  0.017351191490888596,\n",
       "  0.01824551820755005,\n",
       "  0.017346709966659546,\n",
       "  0.017332173883914948,\n",
       "  0.01741812191903591,\n",
       "  0.017326321452856064,\n",
       "  0.01737353578209877,\n",
       "  0.017345640808343887,\n",
       "  0.017963405698537827,\n",
       "  0.01732821948826313,\n",
       "  0.017328912392258644,\n",
       "  0.017328903079032898,\n",
       "  0.017323141917586327,\n",
       "  0.01734006777405739,\n",
       "  0.01732799783349037,\n",
       "  0.017327576875686646,\n",
       "  0.017326010391116142,\n",
       "  0.017327403649687767,\n",
       "  0.017322033643722534,\n",
       "  0.017332779243588448,\n",
       "  0.017745893448591232,\n",
       "  0.01732539013028145,\n",
       "  0.017324009910225868,\n",
       "  0.017325762659311295,\n",
       "  0.01732454262673855,\n",
       "  0.017325781285762787,\n",
       "  0.01820414513349533,\n",
       "  0.017346814274787903,\n",
       "  0.017324382439255714,\n",
       "  0.017361517995595932,\n",
       "  0.01732715778052807,\n",
       "  0.017325568944215775,\n",
       "  0.017321616411209106,\n",
       "  0.017321905121207237,\n",
       "  0.01732945442199707,\n",
       "  0.017326226457953453,\n",
       "  0.017325665801763535,\n",
       "  0.01732589676976204,\n",
       "  0.017326585948467255,\n",
       "  0.017324049025774002,\n",
       "  0.017325706779956818,\n",
       "  0.017327358946204185,\n",
       "  0.017326395958662033,\n",
       "  0.01733861304819584,\n",
       "  0.017381323501467705,\n",
       "  0.01732252538204193,\n",
       "  0.017323220148682594,\n",
       "  0.01747315749526024,\n",
       "  0.017369979992508888,\n",
       "  0.017319973558187485,\n",
       "  0.020715855062007904,\n",
       "  0.018525833263993263,\n",
       "  0.017904091626405716,\n",
       "  0.017401106655597687,\n",
       "  0.017350036650896072,\n",
       "  0.017453258857131004,\n",
       "  0.017360635101795197,\n",
       "  0.017517725005745888,\n",
       "  0.017373809590935707,\n",
       "  0.017356781288981438,\n",
       "  0.01733337715268135,\n",
       "  0.017316583544015884,\n",
       "  0.017346950247883797,\n",
       "  0.017335034906864166,\n",
       "  0.017326701432466507,\n",
       "  0.017332658171653748,\n",
       "  0.01785860024392605,\n",
       "  0.01732398010790348,\n",
       "  0.017331432551145554,\n",
       "  0.01731930300593376,\n",
       "  0.017414772883057594,\n",
       "  0.017338676378130913,\n",
       "  0.017321119084954262,\n",
       "  0.017329616472125053,\n",
       "  0.01732909306883812,\n",
       "  0.01732777990400791,\n",
       "  0.017336087301373482,\n",
       "  0.017318470403552055,\n",
       "  0.017801597714424133,\n",
       "  0.017345162108540535,\n",
       "  0.01733177900314331,\n",
       "  0.017331676557660103,\n",
       "  0.017330626025795937,\n",
       "  0.017329515889286995,\n",
       "  0.017324592918157578,\n",
       "  0.017363522201776505,\n",
       "  0.017386358231306076,\n",
       "  0.017420047894120216,\n",
       "  0.018065739423036575,\n",
       "  0.017323987558484077,\n",
       "  0.0173319224268198,\n",
       "  0.01733916997909546,\n",
       "  0.017338121309876442,\n",
       "  0.017322421073913574,\n",
       "  0.017376868054270744,\n",
       "  0.01734200119972229,\n",
       "  0.017329420894384384,\n",
       "  0.01735217683017254,\n",
       "  0.01736346073448658,\n",
       "  0.01744317263364792,\n",
       "  0.01736844889819622,\n",
       "  0.017500298097729683,\n",
       "  0.017440523952245712,\n",
       "  0.01732569932937622,\n",
       "  0.017336567863821983,\n",
       "  0.017364779487252235,\n",
       "  0.017323194071650505,\n",
       "  0.017334746196866035,\n",
       "  0.0173269584774971,\n",
       "  0.01732061803340912,\n",
       "  0.01731710322201252,\n",
       "  0.01907326653599739,\n",
       "  0.018686888739466667,\n",
       "  0.01733216643333435,\n",
       "  0.01732955127954483,\n",
       "  0.01732717826962471,\n",
       "  0.0173482708632946,\n",
       "  0.017320966348052025,\n",
       "  0.017322877421975136,\n",
       "  0.017337430268526077,\n",
       "  0.01735115982592106,\n",
       "  0.017455069348216057,\n",
       "  0.017355281859636307,\n",
       "  0.01734025962650776,\n",
       "  0.017354650422930717,\n",
       "  0.017344454303383827,\n",
       "  0.01732725277543068,\n",
       "  0.017329053953289986,\n",
       "  0.01733432151377201,\n",
       "  0.017329489812254906,\n",
       "  0.017333699390292168,\n",
       "  0.017321335151791573,\n",
       "  0.017328590154647827,\n",
       "  0.017353378236293793,\n",
       "  0.017326930537819862,\n",
       "  0.017352823168039322,\n",
       "  0.0173368938267231,\n",
       "  0.017333580181002617,\n",
       "  0.017332477495074272,\n",
       "  0.01733139343559742,\n",
       "  0.017326926812529564,\n",
       "  0.017333393916487694,\n",
       "  0.01738135889172554,\n",
       "  0.017348283901810646,\n",
       "  0.01734648458659649,\n",
       "  0.017329849302768707,\n",
       "  0.01903669908642769],\n",
       " 'mse': [0.017412154003977776,\n",
       "  0.01745527982711792,\n",
       "  0.020502636209130287,\n",
       "  0.01845235377550125,\n",
       "  0.017491541802883148,\n",
       "  0.017568254843354225,\n",
       "  0.017568467184901237,\n",
       "  0.017501942813396454,\n",
       "  0.017383545637130737,\n",
       "  0.018518580123782158,\n",
       "  0.017336400225758553,\n",
       "  0.017528321593999863,\n",
       "  0.017618738114833832,\n",
       "  0.017364483326673508,\n",
       "  0.018170535564422607,\n",
       "  0.01734144240617752,\n",
       "  0.01736733317375183,\n",
       "  0.017332125455141068,\n",
       "  0.018134094774723053,\n",
       "  0.01756437122821808,\n",
       "  0.017333585768938065,\n",
       "  0.01842622086405754,\n",
       "  0.017916390672326088,\n",
       "  0.017631376162171364,\n",
       "  0.018152352422475815,\n",
       "  0.017534272745251656,\n",
       "  0.017458802089095116,\n",
       "  0.028800196945667267,\n",
       "  0.018195824697613716,\n",
       "  0.01751874014735222,\n",
       "  0.020862704142928123,\n",
       "  0.017375728115439415,\n",
       "  0.017332162708044052,\n",
       "  0.020421279594302177,\n",
       "  0.017750779166817665,\n",
       "  0.01733609288930893,\n",
       "  0.017422884702682495,\n",
       "  0.018091842532157898,\n",
       "  0.01747754029929638,\n",
       "  0.017569344490766525,\n",
       "  0.01757424883544445,\n",
       "  0.019146379083395004,\n",
       "  0.0178996454924345,\n",
       "  0.017333362251520157,\n",
       "  0.017324965447187424,\n",
       "  0.01733316294848919,\n",
       "  0.017379088327288628,\n",
       "  0.017394280061125755,\n",
       "  0.017337465658783913,\n",
       "  0.01734573021531105,\n",
       "  0.01752086728811264,\n",
       "  0.017347142100334167,\n",
       "  0.01740753836929798,\n",
       "  0.017433837056159973,\n",
       "  0.01861678808927536,\n",
       "  0.017338918522000313,\n",
       "  0.017393816262483597,\n",
       "  0.017329717054963112,\n",
       "  0.017334071919322014,\n",
       "  0.017340732738375664,\n",
       "  0.01733170635998249,\n",
       "  0.017437949776649475,\n",
       "  0.01734527386724949,\n",
       "  0.017439493909478188,\n",
       "  0.017325442284345627,\n",
       "  0.018299954012036324,\n",
       "  0.017787888646125793,\n",
       "  0.017334818840026855,\n",
       "  0.01909620314836502,\n",
       "  0.0173640176653862,\n",
       "  0.017348140478134155,\n",
       "  0.017753560096025467,\n",
       "  0.017436040565371513,\n",
       "  0.017341788858175278,\n",
       "  0.017445409670472145,\n",
       "  0.017330428585410118,\n",
       "  0.017336530610919,\n",
       "  0.01733417809009552,\n",
       "  0.01733388938009739,\n",
       "  0.01735798455774784,\n",
       "  0.017335616052150726,\n",
       "  0.01766359992325306,\n",
       "  0.017346583306789398,\n",
       "  0.0173345934599638,\n",
       "  0.017335016280412674,\n",
       "  0.017334522679448128,\n",
       "  0.01739777997136116,\n",
       "  0.017361408099532127,\n",
       "  0.017329920083284378,\n",
       "  0.017345333471894264,\n",
       "  0.01733068749308586,\n",
       "  0.017332561314105988,\n",
       "  0.017330503091216087,\n",
       "  0.017335308715701103,\n",
       "  0.01737663708627224,\n",
       "  0.017477257177233696,\n",
       "  0.017332211136817932,\n",
       "  0.01733485236763954,\n",
       "  0.017728952690958977,\n",
       "  0.017366118729114532,\n",
       "  0.017333507537841797,\n",
       "  0.0219996590167284,\n",
       "  0.01795516535639763,\n",
       "  0.01732816733419895,\n",
       "  0.01735565811395645,\n",
       "  0.017843572422862053,\n",
       "  0.01733497716486454,\n",
       "  0.01736854575574398,\n",
       "  0.0179850235581398,\n",
       "  0.017462527379393578,\n",
       "  0.021722985431551933,\n",
       "  0.017328985035419464,\n",
       "  0.01732870750129223,\n",
       "  0.017344187945127487,\n",
       "  0.01733441837131977,\n",
       "  0.01733882538974285,\n",
       "  0.017391104251146317,\n",
       "  0.01741150952875614,\n",
       "  0.017361627891659737,\n",
       "  0.01736588031053543,\n",
       "  0.017352359369397163,\n",
       "  0.01764744333922863,\n",
       "  0.017360636964440346,\n",
       "  0.01737532950937748,\n",
       "  0.017329365015029907,\n",
       "  0.017393343150615692,\n",
       "  0.017360424622893333,\n",
       "  0.017591964453458786,\n",
       "  0.017446711659431458,\n",
       "  0.017349544912576675,\n",
       "  0.01732897013425827,\n",
       "  0.019517099484801292,\n",
       "  0.017328867688775063,\n",
       "  0.01772559992969036,\n",
       "  0.01780315861105919,\n",
       "  0.017330395057797432,\n",
       "  0.017367741093039513,\n",
       "  0.017901934683322906,\n",
       "  0.017328312620520592,\n",
       "  0.01742326095700264,\n",
       "  0.017674587666988373,\n",
       "  0.017327111214399338,\n",
       "  0.0203982163220644,\n",
       "  0.01732747256755829,\n",
       "  0.017640676349401474,\n",
       "  0.017326563596725464,\n",
       "  0.0173649862408638,\n",
       "  0.017325682565569878,\n",
       "  0.017322465777397156,\n",
       "  0.01732684299349785,\n",
       "  0.017325490713119507,\n",
       "  0.017326142638921738,\n",
       "  0.017325520515441895,\n",
       "  0.017351191490888596,\n",
       "  0.01824551820755005,\n",
       "  0.017346709966659546,\n",
       "  0.017332173883914948,\n",
       "  0.01741812191903591,\n",
       "  0.017326321452856064,\n",
       "  0.01737353578209877,\n",
       "  0.017345640808343887,\n",
       "  0.017963405698537827,\n",
       "  0.01732821948826313,\n",
       "  0.017328912392258644,\n",
       "  0.017328903079032898,\n",
       "  0.017323141917586327,\n",
       "  0.01734006777405739,\n",
       "  0.01732799783349037,\n",
       "  0.017327576875686646,\n",
       "  0.017326010391116142,\n",
       "  0.017327403649687767,\n",
       "  0.017322033643722534,\n",
       "  0.017332779243588448,\n",
       "  0.017745893448591232,\n",
       "  0.01732539013028145,\n",
       "  0.017324009910225868,\n",
       "  0.017325762659311295,\n",
       "  0.01732454262673855,\n",
       "  0.017325781285762787,\n",
       "  0.01820414513349533,\n",
       "  0.017346814274787903,\n",
       "  0.017324382439255714,\n",
       "  0.017361517995595932,\n",
       "  0.01732715778052807,\n",
       "  0.017325568944215775,\n",
       "  0.017321616411209106,\n",
       "  0.017321905121207237,\n",
       "  0.01732945442199707,\n",
       "  0.017326226457953453,\n",
       "  0.017325665801763535,\n",
       "  0.01732589676976204,\n",
       "  0.017326585948467255,\n",
       "  0.017324049025774002,\n",
       "  0.017325706779956818,\n",
       "  0.017327358946204185,\n",
       "  0.017326395958662033,\n",
       "  0.01733861304819584,\n",
       "  0.017381323501467705,\n",
       "  0.01732252538204193,\n",
       "  0.017323220148682594,\n",
       "  0.01747315749526024,\n",
       "  0.017369979992508888,\n",
       "  0.017319973558187485,\n",
       "  0.020715855062007904,\n",
       "  0.018525833263993263,\n",
       "  0.017904091626405716,\n",
       "  0.017401106655597687,\n",
       "  0.017350036650896072,\n",
       "  0.017453258857131004,\n",
       "  0.017360635101795197,\n",
       "  0.017517725005745888,\n",
       "  0.017373809590935707,\n",
       "  0.017356781288981438,\n",
       "  0.01733337715268135,\n",
       "  0.017316583544015884,\n",
       "  0.017346950247883797,\n",
       "  0.017335034906864166,\n",
       "  0.017326701432466507,\n",
       "  0.017332658171653748,\n",
       "  0.01785860024392605,\n",
       "  0.01732398010790348,\n",
       "  0.017331432551145554,\n",
       "  0.01731930300593376,\n",
       "  0.017414772883057594,\n",
       "  0.017338676378130913,\n",
       "  0.017321119084954262,\n",
       "  0.017329616472125053,\n",
       "  0.01732909306883812,\n",
       "  0.01732777990400791,\n",
       "  0.017336087301373482,\n",
       "  0.017318470403552055,\n",
       "  0.017801597714424133,\n",
       "  0.017345162108540535,\n",
       "  0.01733177900314331,\n",
       "  0.017331676557660103,\n",
       "  0.017330626025795937,\n",
       "  0.017329515889286995,\n",
       "  0.017324592918157578,\n",
       "  0.017363522201776505,\n",
       "  0.017386358231306076,\n",
       "  0.017420047894120216,\n",
       "  0.018065739423036575,\n",
       "  0.017323987558484077,\n",
       "  0.0173319224268198,\n",
       "  0.01733916997909546,\n",
       "  0.017338121309876442,\n",
       "  0.017322421073913574,\n",
       "  0.017376868054270744,\n",
       "  0.01734200119972229,\n",
       "  0.017329420894384384,\n",
       "  0.01735217683017254,\n",
       "  0.01736346073448658,\n",
       "  0.01744317263364792,\n",
       "  0.01736844889819622,\n",
       "  0.017500298097729683,\n",
       "  0.017440523952245712,\n",
       "  0.01732569932937622,\n",
       "  0.017336567863821983,\n",
       "  0.017364779487252235,\n",
       "  0.017323194071650505,\n",
       "  0.017334746196866035,\n",
       "  0.0173269584774971,\n",
       "  0.01732061803340912,\n",
       "  0.01731710322201252,\n",
       "  0.01907326653599739,\n",
       "  0.018686888739466667,\n",
       "  0.01733216643333435,\n",
       "  0.01732955127954483,\n",
       "  0.01732717826962471,\n",
       "  0.0173482708632946,\n",
       "  0.017320966348052025,\n",
       "  0.017322877421975136,\n",
       "  0.017337430268526077,\n",
       "  0.01735115982592106,\n",
       "  0.017455069348216057,\n",
       "  0.017355281859636307,\n",
       "  0.01734025962650776,\n",
       "  0.017354650422930717,\n",
       "  0.017344454303383827,\n",
       "  0.01732725277543068,\n",
       "  0.017329053953289986,\n",
       "  0.01733432151377201,\n",
       "  0.017329489812254906,\n",
       "  0.017333699390292168,\n",
       "  0.017321335151791573,\n",
       "  0.017328590154647827,\n",
       "  0.017353378236293793,\n",
       "  0.017326930537819862,\n",
       "  0.017352823168039322,\n",
       "  0.0173368938267231,\n",
       "  0.017333580181002617,\n",
       "  0.017332477495074272,\n",
       "  0.01733139343559742,\n",
       "  0.017326926812529564,\n",
       "  0.017333393916487694,\n",
       "  0.01738135889172554,\n",
       "  0.017348283901810646,\n",
       "  0.01734648458659649,\n",
       "  0.017329849302768707,\n",
       "  0.01903669908642769],\n",
       " 'val_loss': [0.017325859516859055,\n",
       "  0.017619330435991287,\n",
       "  0.017343850806355476,\n",
       "  0.017437780275940895,\n",
       "  0.017331354320049286,\n",
       "  0.017318855971097946,\n",
       "  0.017469927668571472,\n",
       "  0.01742277853190899,\n",
       "  0.01750556007027626,\n",
       "  0.017322499305009842,\n",
       "  0.017363032326102257,\n",
       "  0.017423899844288826,\n",
       "  0.017370516434311867,\n",
       "  0.017451303079724312,\n",
       "  0.01735316589474678,\n",
       "  0.01739293523132801,\n",
       "  0.017384253442287445,\n",
       "  0.017481444403529167,\n",
       "  0.017764346674084663,\n",
       "  0.017387274652719498,\n",
       "  0.017735669389367104,\n",
       "  0.01753190904855728,\n",
       "  0.017889216542243958,\n",
       "  0.01854505017399788,\n",
       "  0.0173488836735487,\n",
       "  0.017530910670757294,\n",
       "  0.01856139302253723,\n",
       "  0.02027168683707714,\n",
       "  0.01779683120548725,\n",
       "  0.01803351566195488,\n",
       "  0.017398571595549583,\n",
       "  0.017557624727487564,\n",
       "  0.01914988085627556,\n",
       "  0.017360959202051163,\n",
       "  0.01735568232834339,\n",
       "  0.017629297450184822,\n",
       "  0.017382053658366203,\n",
       "  0.017408695071935654,\n",
       "  0.01776975393295288,\n",
       "  0.017366446554660797,\n",
       "  0.017404012382030487,\n",
       "  0.01747925765812397,\n",
       "  0.01736419089138508,\n",
       "  0.019206155091524124,\n",
       "  0.01732086017727852,\n",
       "  0.01739707961678505,\n",
       "  0.018123533576726913,\n",
       "  0.01734362542629242,\n",
       "  0.01736448146402836,\n",
       "  0.01735040545463562,\n",
       "  0.018684206530451775,\n",
       "  0.017332525923848152,\n",
       "  0.019858166575431824,\n",
       "  0.01936158910393715,\n",
       "  0.017325563356280327,\n",
       "  0.0178526621311903,\n",
       "  0.017334245145320892,\n",
       "  0.017313456162810326,\n",
       "  0.018846018239855766,\n",
       "  0.017321720719337463,\n",
       "  0.01757146790623665,\n",
       "  0.017329147085547447,\n",
       "  0.017348287627100945,\n",
       "  0.017401253804564476,\n",
       "  0.01785871759057045,\n",
       "  0.0302719809114933,\n",
       "  0.020311908796429634,\n",
       "  0.01960684359073639,\n",
       "  0.017404749989509583,\n",
       "  0.01770852692425251,\n",
       "  0.017498459666967392,\n",
       "  0.017476579174399376,\n",
       "  0.017784040421247482,\n",
       "  0.01733570173382759,\n",
       "  0.01859838329255581,\n",
       "  0.017327219247817993,\n",
       "  0.017352821305394173,\n",
       "  0.017327532172203064,\n",
       "  0.017333699390292168,\n",
       "  0.017350269481539726,\n",
       "  0.017332617193460464,\n",
       "  0.01732867956161499,\n",
       "  0.01741507463157177,\n",
       "  0.017331698909401894,\n",
       "  0.017328517511487007,\n",
       "  0.01735050603747368,\n",
       "  0.017327139154076576,\n",
       "  0.017324652522802353,\n",
       "  0.0174944456666708,\n",
       "  0.017858687788248062,\n",
       "  0.017328975722193718,\n",
       "  0.017325060442090034,\n",
       "  0.017348702996969223,\n",
       "  0.017334898933768272,\n",
       "  0.017326433211565018,\n",
       "  0.017465846613049507,\n",
       "  0.01736663095653057,\n",
       "  0.01747441105544567,\n",
       "  0.017328575253486633,\n",
       "  0.017334261909127235,\n",
       "  0.018568094819784164,\n",
       "  0.03129478171467781,\n",
       "  0.017324356362223625,\n",
       "  0.017329176887869835,\n",
       "  0.01731986366212368,\n",
       "  0.01802218332886696,\n",
       "  0.01801721751689911,\n",
       "  0.017362268641591072,\n",
       "  0.017366576939821243,\n",
       "  0.017330622300505638,\n",
       "  0.017322469502687454,\n",
       "  0.017337018623948097,\n",
       "  0.017330383881926537,\n",
       "  0.01731950417160988,\n",
       "  0.017354628071188927,\n",
       "  0.01732475310564041,\n",
       "  0.017321530729532242,\n",
       "  0.017331810668110847,\n",
       "  0.019040260463953018,\n",
       "  0.017679361626505852,\n",
       "  0.017467932775616646,\n",
       "  0.017352210357785225,\n",
       "  0.01739346794784069,\n",
       "  0.021793989464640617,\n",
       "  0.01733037270605564,\n",
       "  0.017337720841169357,\n",
       "  0.017332695424556732,\n",
       "  0.017349034547805786,\n",
       "  0.017350438982248306,\n",
       "  0.01736442558467388,\n",
       "  0.020547669380903244,\n",
       "  0.017318813130259514,\n",
       "  0.022088361904025078,\n",
       "  0.01732517220079899,\n",
       "  0.017408370971679688,\n",
       "  0.017321843653917313,\n",
       "  0.021419789642095566,\n",
       "  0.017780646681785583,\n",
       "  0.01776604726910591,\n",
       "  0.017352569848299026,\n",
       "  0.01733526587486267,\n",
       "  0.0696631371974945,\n",
       "  0.017401745542883873,\n",
       "  0.018993360921740532,\n",
       "  0.017403436824679375,\n",
       "  0.017362968996167183,\n",
       "  0.017354177311062813,\n",
       "  0.017337635159492493,\n",
       "  0.017321500927209854,\n",
       "  0.017333781346678734,\n",
       "  0.01731831766664982,\n",
       "  0.01732185110449791,\n",
       "  0.01732514798641205,\n",
       "  0.017447825521230698,\n",
       "  0.01735636033117771,\n",
       "  0.02847331017255783,\n",
       "  0.017369521781802177,\n",
       "  0.017327027395367622,\n",
       "  0.017329948022961617,\n",
       "  0.017318444326519966,\n",
       "  0.017326656728982925,\n",
       "  0.017349563539028168,\n",
       "  0.0173211470246315,\n",
       "  0.01732868142426014,\n",
       "  0.017333397641777992,\n",
       "  0.018069513142108917,\n",
       "  0.017323534935712814,\n",
       "  0.01732051745057106,\n",
       "  0.017332565039396286,\n",
       "  0.01732562854886055,\n",
       "  0.017321351915597916,\n",
       "  0.017351821064949036,\n",
       "  0.017320703715085983,\n",
       "  0.017319459468126297,\n",
       "  0.017328519374132156,\n",
       "  0.017320165410637856,\n",
       "  0.01736912690103054,\n",
       "  0.017630398273468018,\n",
       "  0.017320314422249794,\n",
       "  0.0173272006213665,\n",
       "  0.017336860299110413,\n",
       "  0.017332209274172783,\n",
       "  0.01804334484040737,\n",
       "  0.01732945814728737,\n",
       "  0.017334695905447006,\n",
       "  0.01732180267572403,\n",
       "  0.017339089885354042,\n",
       "  0.017321676015853882,\n",
       "  0.017323041334748268,\n",
       "  0.017328869551420212,\n",
       "  0.017321402207016945,\n",
       "  0.01732105202972889,\n",
       "  0.017320891842246056,\n",
       "  0.01732359640300274,\n",
       "  0.01732104830443859,\n",
       "  0.017389623448252678,\n",
       "  0.01764080859720707,\n",
       "  0.017325887456536293,\n",
       "  0.017330583184957504,\n",
       "  0.01739680767059326,\n",
       "  0.02121484838426113,\n",
       "  0.017322449013590813,\n",
       "  0.017319973558187485,\n",
       "  0.017393214628100395,\n",
       "  0.017677806317806244,\n",
       "  0.01778561994433403,\n",
       "  0.017328839749097824,\n",
       "  0.017326679080724716,\n",
       "  0.017362114042043686,\n",
       "  0.017346346750855446,\n",
       "  0.017340492457151413,\n",
       "  0.01734030246734619,\n",
       "  0.017329443246126175,\n",
       "  0.017330924049019814,\n",
       "  0.017321385443210602,\n",
       "  0.017335420474410057,\n",
       "  0.01732509210705757,\n",
       "  0.017345421016216278,\n",
       "  0.017325714230537415,\n",
       "  0.01732855848968029,\n",
       "  0.017330557107925415,\n",
       "  0.017322024330496788,\n",
       "  0.01733812503516674,\n",
       "  0.017364999279379845,\n",
       "  0.01732906885445118,\n",
       "  0.017333894968032837,\n",
       "  0.01734626293182373,\n",
       "  0.017351184040308,\n",
       "  0.017341693863272667,\n",
       "  0.017322435975074768,\n",
       "  0.017342688515782356,\n",
       "  0.01731615886092186,\n",
       "  0.017323769629001617,\n",
       "  0.017361115664243698,\n",
       "  0.017338531091809273,\n",
       "  0.017409516498446465,\n",
       "  0.017328035086393356,\n",
       "  0.017322704195976257,\n",
       "  0.01736721210181713,\n",
       "  0.017356539145112038,\n",
       "  0.01732291467487812,\n",
       "  0.021474290639162064,\n",
       "  0.01733582839369774,\n",
       "  0.01734469272196293,\n",
       "  0.01737065799534321,\n",
       "  0.017328815534710884,\n",
       "  0.017372407019138336,\n",
       "  0.017342926934361458,\n",
       "  0.017333999276161194,\n",
       "  0.017345910891890526,\n",
       "  0.017423829063773155,\n",
       "  0.01733960583806038,\n",
       "  0.017365334555506706,\n",
       "  0.017333293333649635,\n",
       "  0.01735595054924488,\n",
       "  0.01735772006213665,\n",
       "  0.017502669245004654,\n",
       "  0.017340881749987602,\n",
       "  0.017354942858219147,\n",
       "  0.017320657148957253,\n",
       "  0.017320509999990463,\n",
       "  0.017327215522527695,\n",
       "  0.017333175987005234,\n",
       "  0.017348628491163254,\n",
       "  0.3339157700538635,\n",
       "  0.017322726547718048,\n",
       "  0.017325449734926224,\n",
       "  0.017330151051282883,\n",
       "  0.017322175204753876,\n",
       "  0.017328443005681038,\n",
       "  0.017332930117845535,\n",
       "  0.017321564257144928,\n",
       "  0.01732666976749897,\n",
       "  0.017342351377010345,\n",
       "  0.017356526106595993,\n",
       "  0.017388300970196724,\n",
       "  0.017344852909445763,\n",
       "  0.017354944720864296,\n",
       "  0.01734026148915291,\n",
       "  0.017373893409967422,\n",
       "  0.017387744039297104,\n",
       "  0.0173308365046978,\n",
       "  0.0173239316791296,\n",
       "  0.017327379435300827,\n",
       "  0.01732838898897171,\n",
       "  0.01732729747891426,\n",
       "  0.017339318990707397,\n",
       "  0.02480156347155571,\n",
       "  0.017347272485494614,\n",
       "  0.017663409933447838,\n",
       "  0.0176067054271698,\n",
       "  0.017324814572930336,\n",
       "  0.01760493591427803,\n",
       "  0.017443237826228142,\n",
       "  0.01732846349477768,\n",
       "  0.017391731962561607,\n",
       "  0.017437070608139038,\n",
       "  0.017376597970724106,\n",
       "  0.017679819837212563,\n",
       "  0.017568785697221756],\n",
       " 'val_mse': [0.017325859516859055,\n",
       "  0.017619330435991287,\n",
       "  0.017343850806355476,\n",
       "  0.017437780275940895,\n",
       "  0.017331354320049286,\n",
       "  0.017318855971097946,\n",
       "  0.017469927668571472,\n",
       "  0.01742277853190899,\n",
       "  0.01750556007027626,\n",
       "  0.017322499305009842,\n",
       "  0.017363032326102257,\n",
       "  0.017423899844288826,\n",
       "  0.017370516434311867,\n",
       "  0.017451303079724312,\n",
       "  0.01735316589474678,\n",
       "  0.01739293523132801,\n",
       "  0.017384253442287445,\n",
       "  0.017481444403529167,\n",
       "  0.017764346674084663,\n",
       "  0.017387274652719498,\n",
       "  0.017735669389367104,\n",
       "  0.01753190904855728,\n",
       "  0.017889216542243958,\n",
       "  0.01854505017399788,\n",
       "  0.0173488836735487,\n",
       "  0.017530910670757294,\n",
       "  0.01856139302253723,\n",
       "  0.02027168683707714,\n",
       "  0.01779683120548725,\n",
       "  0.01803351566195488,\n",
       "  0.017398571595549583,\n",
       "  0.017557624727487564,\n",
       "  0.01914988085627556,\n",
       "  0.017360959202051163,\n",
       "  0.01735568232834339,\n",
       "  0.017629297450184822,\n",
       "  0.017382053658366203,\n",
       "  0.017408695071935654,\n",
       "  0.01776975393295288,\n",
       "  0.017366446554660797,\n",
       "  0.017404012382030487,\n",
       "  0.01747925765812397,\n",
       "  0.01736419089138508,\n",
       "  0.019206155091524124,\n",
       "  0.01732086017727852,\n",
       "  0.01739707961678505,\n",
       "  0.018123533576726913,\n",
       "  0.01734362542629242,\n",
       "  0.01736448146402836,\n",
       "  0.01735040545463562,\n",
       "  0.018684206530451775,\n",
       "  0.017332525923848152,\n",
       "  0.019858166575431824,\n",
       "  0.01936158910393715,\n",
       "  0.017325563356280327,\n",
       "  0.0178526621311903,\n",
       "  0.017334245145320892,\n",
       "  0.017313456162810326,\n",
       "  0.018846018239855766,\n",
       "  0.017321720719337463,\n",
       "  0.01757146790623665,\n",
       "  0.017329147085547447,\n",
       "  0.017348287627100945,\n",
       "  0.017401253804564476,\n",
       "  0.01785871759057045,\n",
       "  0.0302719809114933,\n",
       "  0.020311908796429634,\n",
       "  0.01960684359073639,\n",
       "  0.017404749989509583,\n",
       "  0.01770852692425251,\n",
       "  0.017498459666967392,\n",
       "  0.017476579174399376,\n",
       "  0.017784040421247482,\n",
       "  0.01733570173382759,\n",
       "  0.01859838329255581,\n",
       "  0.017327219247817993,\n",
       "  0.017352821305394173,\n",
       "  0.017327532172203064,\n",
       "  0.017333699390292168,\n",
       "  0.017350269481539726,\n",
       "  0.017332617193460464,\n",
       "  0.01732867956161499,\n",
       "  0.01741507463157177,\n",
       "  0.017331698909401894,\n",
       "  0.017328517511487007,\n",
       "  0.01735050603747368,\n",
       "  0.017327139154076576,\n",
       "  0.017324652522802353,\n",
       "  0.0174944456666708,\n",
       "  0.017858687788248062,\n",
       "  0.017328975722193718,\n",
       "  0.017325060442090034,\n",
       "  0.017348702996969223,\n",
       "  0.017334898933768272,\n",
       "  0.017326433211565018,\n",
       "  0.017465846613049507,\n",
       "  0.01736663095653057,\n",
       "  0.01747441105544567,\n",
       "  0.017328575253486633,\n",
       "  0.017334261909127235,\n",
       "  0.018568094819784164,\n",
       "  0.03129478171467781,\n",
       "  0.017324356362223625,\n",
       "  0.017329176887869835,\n",
       "  0.01731986366212368,\n",
       "  0.01802218332886696,\n",
       "  0.01801721751689911,\n",
       "  0.017362268641591072,\n",
       "  0.017366576939821243,\n",
       "  0.017330622300505638,\n",
       "  0.017322469502687454,\n",
       "  0.017337018623948097,\n",
       "  0.017330383881926537,\n",
       "  0.01731950417160988,\n",
       "  0.017354628071188927,\n",
       "  0.01732475310564041,\n",
       "  0.017321530729532242,\n",
       "  0.017331810668110847,\n",
       "  0.019040260463953018,\n",
       "  0.017679361626505852,\n",
       "  0.017467932775616646,\n",
       "  0.017352210357785225,\n",
       "  0.01739346794784069,\n",
       "  0.021793989464640617,\n",
       "  0.01733037270605564,\n",
       "  0.017337720841169357,\n",
       "  0.017332695424556732,\n",
       "  0.017349034547805786,\n",
       "  0.017350438982248306,\n",
       "  0.01736442558467388,\n",
       "  0.020547669380903244,\n",
       "  0.017318813130259514,\n",
       "  0.022088361904025078,\n",
       "  0.01732517220079899,\n",
       "  0.017408370971679688,\n",
       "  0.017321843653917313,\n",
       "  0.021419789642095566,\n",
       "  0.017780646681785583,\n",
       "  0.01776604726910591,\n",
       "  0.017352569848299026,\n",
       "  0.01733526587486267,\n",
       "  0.0696631371974945,\n",
       "  0.017401745542883873,\n",
       "  0.018993360921740532,\n",
       "  0.017403436824679375,\n",
       "  0.017362968996167183,\n",
       "  0.017354177311062813,\n",
       "  0.017337635159492493,\n",
       "  0.017321500927209854,\n",
       "  0.017333781346678734,\n",
       "  0.01731831766664982,\n",
       "  0.01732185110449791,\n",
       "  0.01732514798641205,\n",
       "  0.017447825521230698,\n",
       "  0.01735636033117771,\n",
       "  0.02847331017255783,\n",
       "  0.017369521781802177,\n",
       "  0.017327027395367622,\n",
       "  0.017329948022961617,\n",
       "  0.017318444326519966,\n",
       "  0.017326656728982925,\n",
       "  0.017349563539028168,\n",
       "  0.0173211470246315,\n",
       "  0.01732868142426014,\n",
       "  0.017333397641777992,\n",
       "  0.018069513142108917,\n",
       "  0.017323534935712814,\n",
       "  0.01732051745057106,\n",
       "  0.017332565039396286,\n",
       "  0.01732562854886055,\n",
       "  0.017321351915597916,\n",
       "  0.017351821064949036,\n",
       "  0.017320703715085983,\n",
       "  0.017319459468126297,\n",
       "  0.017328519374132156,\n",
       "  0.017320165410637856,\n",
       "  0.01736912690103054,\n",
       "  0.017630398273468018,\n",
       "  0.017320314422249794,\n",
       "  0.0173272006213665,\n",
       "  0.017336860299110413,\n",
       "  0.017332209274172783,\n",
       "  0.01804334484040737,\n",
       "  0.01732945814728737,\n",
       "  0.017334695905447006,\n",
       "  0.01732180267572403,\n",
       "  0.017339089885354042,\n",
       "  0.017321676015853882,\n",
       "  0.017323041334748268,\n",
       "  0.017328869551420212,\n",
       "  0.017321402207016945,\n",
       "  0.01732105202972889,\n",
       "  0.017320891842246056,\n",
       "  0.01732359640300274,\n",
       "  0.01732104830443859,\n",
       "  0.017389623448252678,\n",
       "  0.01764080859720707,\n",
       "  0.017325887456536293,\n",
       "  0.017330583184957504,\n",
       "  0.01739680767059326,\n",
       "  0.02121484838426113,\n",
       "  0.017322449013590813,\n",
       "  0.017319973558187485,\n",
       "  0.017393214628100395,\n",
       "  0.017677806317806244,\n",
       "  0.01778561994433403,\n",
       "  0.017328839749097824,\n",
       "  0.017326679080724716,\n",
       "  0.017362114042043686,\n",
       "  0.017346346750855446,\n",
       "  0.017340492457151413,\n",
       "  0.01734030246734619,\n",
       "  0.017329443246126175,\n",
       "  0.017330924049019814,\n",
       "  0.017321385443210602,\n",
       "  0.017335420474410057,\n",
       "  0.01732509210705757,\n",
       "  0.017345421016216278,\n",
       "  0.017325714230537415,\n",
       "  0.01732855848968029,\n",
       "  0.017330557107925415,\n",
       "  0.017322024330496788,\n",
       "  0.01733812503516674,\n",
       "  0.017364999279379845,\n",
       "  0.01732906885445118,\n",
       "  0.017333894968032837,\n",
       "  0.01734626293182373,\n",
       "  0.017351184040308,\n",
       "  0.017341693863272667,\n",
       "  0.017322435975074768,\n",
       "  0.017342688515782356,\n",
       "  0.01731615886092186,\n",
       "  0.017323769629001617,\n",
       "  0.017361115664243698,\n",
       "  0.017338531091809273,\n",
       "  0.017409516498446465,\n",
       "  0.017328035086393356,\n",
       "  0.017322704195976257,\n",
       "  0.01736721210181713,\n",
       "  0.017356539145112038,\n",
       "  0.01732291467487812,\n",
       "  0.021474290639162064,\n",
       "  0.01733582839369774,\n",
       "  0.01734469272196293,\n",
       "  0.01737065799534321,\n",
       "  0.017328815534710884,\n",
       "  0.017372407019138336,\n",
       "  0.017342926934361458,\n",
       "  0.017333999276161194,\n",
       "  0.017345910891890526,\n",
       "  0.017423829063773155,\n",
       "  0.01733960583806038,\n",
       "  0.017365334555506706,\n",
       "  0.017333293333649635,\n",
       "  0.01735595054924488,\n",
       "  0.01735772006213665,\n",
       "  0.017502669245004654,\n",
       "  0.017340881749987602,\n",
       "  0.017354942858219147,\n",
       "  0.017320657148957253,\n",
       "  0.017320509999990463,\n",
       "  0.017327215522527695,\n",
       "  0.017333175987005234,\n",
       "  0.017348628491163254,\n",
       "  0.3339157700538635,\n",
       "  0.017322726547718048,\n",
       "  0.017325449734926224,\n",
       "  0.017330151051282883,\n",
       "  0.017322175204753876,\n",
       "  0.017328443005681038,\n",
       "  0.017332930117845535,\n",
       "  0.017321564257144928,\n",
       "  0.01732666976749897,\n",
       "  0.017342351377010345,\n",
       "  0.017356526106595993,\n",
       "  0.017388300970196724,\n",
       "  0.017344852909445763,\n",
       "  0.017354944720864296,\n",
       "  0.01734026148915291,\n",
       "  0.017373893409967422,\n",
       "  0.017387744039297104,\n",
       "  0.0173308365046978,\n",
       "  0.0173239316791296,\n",
       "  0.017327379435300827,\n",
       "  0.01732838898897171,\n",
       "  0.01732729747891426,\n",
       "  0.017339318990707397,\n",
       "  0.02480156347155571,\n",
       "  0.017347272485494614,\n",
       "  0.017663409933447838,\n",
       "  0.0176067054271698,\n",
       "  0.017324814572930336,\n",
       "  0.01760493591427803,\n",
       "  0.017443237826228142,\n",
       "  0.01732846349477768,\n",
       "  0.017391731962561607,\n",
       "  0.017437070608139038,\n",
       "  0.017376597970724106,\n",
       "  0.017679819837212563,\n",
       "  0.017568785697221756]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(history.history)\n",
    "model_df[['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory vs Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696/1696 [==============================] - 1s 677us/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X4_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 0.01523, st_er: 0.000343\n",
      "y = 0.0012*x + 0.9818\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = y4_pred.flatten()\n",
    "y = test_pred.flatten()\n",
    "m, b, r, p, st_er = stats.linregress(x,y) \n",
    "\n",
    "yfit = [b + m * xi for xi in x]\n",
    "yisx = [0 + 1 * xi for xi in x]\n",
    "plt.plot(x, yfit)\n",
    "plt.plot(x, yisx)\n",
    "\n",
    "plt.scatter(y4_pred, test_pred,  color='black')\n",
    "# plt.axis([0,100, 0, 100])\n",
    "plt.xlabel(\"Theory (nT)\")\n",
    "plt.ylabel(\"Prediction (nT)\")\n",
    "plt.title(\"Neural Network Prediction vs Theory\", fontsize=15)\n",
    "# print(r, st_er)\n",
    "print(\"r: {:.5f}, st_er: {:.6f}\".format(r, st_er))\n",
    "print(\"y = \"+str(round(m,4))+\"*x + \"+str(round(b,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/vkjb_lqj2_30lnyhzhjqtg9w0000gp/T/ipykernel_15935/2364276288.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4_pred['predict'] = test_pred\n"
     ]
    }
   ],
   "source": [
    "df4_pred['predict'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730970</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-943.760986</td>\n",
       "      <td>-940.001038</td>\n",
       "      <td>-946.072876</td>\n",
       "      <td>0.798312</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730972</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-560.983154</td>\n",
       "      <td>-167.993820</td>\n",
       "      <td>-76.056984</td>\n",
       "      <td>0.819827</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731033</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-763.589050</td>\n",
       "      <td>-740.604675</td>\n",
       "      <td>-767.269226</td>\n",
       "      <td>0.786963</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731034</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-993.368958</td>\n",
       "      <td>-994.284729</td>\n",
       "      <td>-993.848022</td>\n",
       "      <td>0.797582</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734749</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-327.346069</td>\n",
       "      <td>-303.126373</td>\n",
       "      <td>-316.548706</td>\n",
       "      <td>0.837726</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066912</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>531.209412</td>\n",
       "      <td>524.617249</td>\n",
       "      <td>528.361572</td>\n",
       "      <td>1.120325</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066965</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-738.791687</td>\n",
       "      <td>-309.708313</td>\n",
       "      <td>248.529739</td>\n",
       "      <td>0.942770</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066966</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-854.326355</td>\n",
       "      <td>-827.500061</td>\n",
       "      <td>-840.415222</td>\n",
       "      <td>0.957650</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067028</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-524.266296</td>\n",
       "      <td>-949.864380</td>\n",
       "      <td>-932.718628</td>\n",
       "      <td>0.927170</td>\n",
       "      <td>0.815558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067029</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-783.382935</td>\n",
       "      <td>-823.340027</td>\n",
       "      <td>-807.056702</td>\n",
       "      <td>0.941901</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54256 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg       mean0       mean1  \\\n",
       "730970    42         3   2  29  26    2    0 -943.760986 -940.001038   \n",
       "730972    42         3   2  29  28    2    0 -560.983154 -167.993820   \n",
       "731033    42         3   2  30  25    2    0 -763.589050 -740.604675   \n",
       "731034    42         3   2  30  26    2    0 -993.368958 -994.284729   \n",
       "734749    42         3   3  24  29    2    0 -327.346069 -303.126373   \n",
       "...      ...       ...  ..  ..  ..  ...  ...         ...         ...   \n",
       "1066912   42         3  40  30  32    2    1  531.209412  524.617249   \n",
       "1066965   42         3  40  31  21    2    1 -738.791687 -309.708313   \n",
       "1066966   42         3  40  31  22    2    1 -854.326355 -827.500061   \n",
       "1067028   42         3  40  32  20    2    1 -524.266296 -949.864380   \n",
       "1067029   42         3  40  32  21    2    1 -783.382935 -823.340027   \n",
       "\n",
       "              mean2    theory   predict  \n",
       "730970  -946.072876  0.798312  0.983463  \n",
       "730972   -76.056984  0.819827  0.983463  \n",
       "731033  -767.269226  0.786963  0.983463  \n",
       "731034  -993.848022  0.797582  0.983463  \n",
       "734749  -316.548706  0.837726  0.983463  \n",
       "...             ...       ...       ...  \n",
       "1066912  528.361572  1.120325  0.983463  \n",
       "1066965  248.529739  0.942770  0.983463  \n",
       "1066966 -840.415222  0.957650  0.983463  \n",
       "1067028 -932.718628  0.927170  0.815558  \n",
       "1067029 -807.056702  0.941901  0.983463  \n",
       "\n",
       "[54256 rows x 12 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720896</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720897</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720898</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720899</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720900</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081339</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.865481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081340</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.910840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081341</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.957465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081342</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.005388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081343</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.054645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360448 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg  mean0  mean1  mean2    theory\n",
       "720896    42         3   0   0   0    2    0    0.0    0.0    0.0  0.578396\n",
       "720897    42         3   0   0   1    2    0    0.0    0.0    0.0  0.584610\n",
       "720898    42         3   0   0   2    2    0    0.0    0.0    0.0  0.590888\n",
       "720899    42         3   0   0   3    2    0    0.0    0.0    0.0  0.597230\n",
       "720900    42         3   0   0   4    2    0    0.0    0.0    0.0  0.603638\n",
       "...      ...       ...  ..  ..  ..  ...  ...    ...    ...    ...       ...\n",
       "1081339   42         3  43  63  59    2    1    0.0    0.0    0.0  1.865481\n",
       "1081340   42         3  43  63  60    2    1    0.0    0.0    0.0  1.910840\n",
       "1081341   42         3  43  63  61    2    1    0.0    0.0    0.0  1.957465\n",
       "1081342   42         3  43  63  62    2    1    0.0    0.0    0.0  2.005388\n",
       "1081343   42         3  43  63  63    2    1    0.0    0.0    0.0  2.054645\n",
       "\n",
       "[360448 rows x 11 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/vkjb_lqj2_30lnyhzhjqtg9w0000gp/T/ipykernel_15935/549603398.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1_test['predict'] = 0.00\n"
     ]
    }
   ],
   "source": [
    "df1_test['predict'] = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df1_test[['i', 'j', 'k', 'neg', 'predict']]\n",
    "df_pre = df4_pred[['i', 'j', 'k', 'neg', 'predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 360448 entries, 720896 to 1081343\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   i        360448 non-null  int64  \n",
      " 1   j        360448 non-null  int64  \n",
      " 2   k        360448 non-null  int64  \n",
      " 3   neg      360448 non-null  int64  \n",
      " 4   predict  360448 non-null  float64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 16.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_out.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_pre)):\n",
    "    i = df_pre.iloc[x, :]['i'].astype(int)\n",
    "    j = df_pre.iloc[x, :]['j'].astype(int)\n",
    "    k = df_pre.iloc[x, :]['k'].astype(int)\n",
    "    neg = df_pre.iloc[x, :]['neg'].astype(int)\n",
    "    pred = df_pre.iloc[x, :]['predict']\n",
    "    idx = df_out[(df_out['i']==i) & (df_out['j']==j) & (df_out['k']==k) & (df_out['neg']==neg)].index\n",
    "    df_out.loc[idx, 'predict']= pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>730970</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730972</th>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731033</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731034</th>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734749</th>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066912</th>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066965</th>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066966</th>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067028</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.815558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067029</th>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54256 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   j   k  neg   predict\n",
       "730970    2  29  26    0  0.983463\n",
       "730972    2  29  28    0  0.983463\n",
       "731033    2  30  25    0  0.983463\n",
       "731034    2  30  26    0  0.983463\n",
       "734749    3  24  29    0  0.983463\n",
       "...      ..  ..  ..  ...       ...\n",
       "1066912  40  30  32    1  0.983463\n",
       "1066965  40  31  21    1  0.983463\n",
       "1066966  40  31  22    1  0.983463\n",
       "1067028  40  32  20    1  0.815558\n",
       "1067029  40  32  21    1  0.983463\n",
       "\n",
       "[54256 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>751656</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931880</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.983463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        i   j   k  neg   predict\n",
       "751656  7  32  40    0  0.983463\n",
       "931880  7  32  40    1  0.983463"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out[(df_out['i']==7) & (df_out['j']==32) & (df_out['k']==40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>720896</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720897</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720898</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720899</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720900</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081339</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081340</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081341</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081342</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081343</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360448 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   j   k  neg  predict\n",
       "720896    0   0   0    0      0.0\n",
       "720897    0   0   1    0      0.0\n",
       "720898    0   0   2    0      0.0\n",
       "720899    0   0   3    0      0.0\n",
       "720900    0   0   4    0      0.0\n",
       "...      ..  ..  ..  ...      ...\n",
       "1081339  43  63  59    1      0.0\n",
       "1081340  43  63  60    1      0.0\n",
       "1081341  43  63  61    1      0.0\n",
       "1081342  43  63  62    1      0.0\n",
       "1081343  43  63  63    1      0.0\n",
       "\n",
       "[360448 rows x 5 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_out[df_out['neg']==0]\n",
    "df_neg = df_out[df_out['neg']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(img_path+\"nn_neg_42_nonzero_pos.txt\", df_pos[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")\n",
    "np.savetxt(img_path+\"nn_neg_42_nonzero_neg.txt\", df_neg[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
