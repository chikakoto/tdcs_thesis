{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pickle\n",
    "# import nibabel as nib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_percentage_error as mape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prepared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"/Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/\"\n",
    "# dir_path = \"/Users/mriworkshop/Documents/TDCS/code/tdcs_thesis/\"\n",
    "save_path = dir_path+\"data/raw/\"\n",
    "img_path =  dir_path+\"data/processed/\"\n",
    "model_path = dir_path+\"models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmap mean all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_cylinder = save_path+\"fmap_mean_neg_amp_32to39.txt\"\n",
    "file_sphere = save_path+\"fmap_mean_neg_amp_42.txt\"\n",
    "\n",
    "columns_mean =['exp', 'mini_exp', 'i', 'j', 'k', 'amp', 'neg', 'mean0', 'mean1', 'mean2', 'theory']\n",
    "data_c = np.loadtxt(file_cylinder);\n",
    "data_s = np.loadtxt(file_sphere);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255227</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255228</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255229</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255230</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12255231</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12255232 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          exp  mini_exp   i   j   k  amp  neg  mean0  mean1  mean2  theory\n",
       "0          32         1   0   0   0    2    0    0.0    0.0    0.0     0.0\n",
       "1          32         1   0   0   1    2    0    0.0    0.0    0.0     0.0\n",
       "2          32         1   0   0   2    2    0    0.0    0.0    0.0     0.0\n",
       "3          32         1   0   0   3    2    0    0.0    0.0    0.0     0.0\n",
       "4          32         1   0   0   4    2    0    0.0    0.0    0.0     0.0\n",
       "...       ...       ...  ..  ..  ..  ...  ...    ...    ...    ...     ...\n",
       "12255227   39         6  43  63  59    1    1    0.0    0.0    0.0     0.0\n",
       "12255228   39         6  43  63  60    1    1    0.0    0.0    0.0     0.0\n",
       "12255229   39         6  43  63  61    1    1    0.0    0.0    0.0     0.0\n",
       "12255230   39         6  43  63  62    1    1    0.0    0.0    0.0     0.0\n",
       "12255231   39         6  43  63  63    1    1    0.0    0.0    0.0     0.0\n",
       "\n",
       "[12255232 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = pd.DataFrame(data_c, columns=columns_mean)\n",
    "df_c = df_c.astype({\"exp\": int, \"i\": int, \"j\": int, \"k\": int, \"mini_exp\": int, \"amp\":int, \"neg\": int})\n",
    "df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.578396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.597230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081339</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.865481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081340</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.910840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081341</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.957465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081342</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.005388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081343</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.054645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1081344 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg  mean0  mean1  mean2    theory\n",
       "0         42         1   0   0   0    2    0    0.0    0.0    0.0  0.578396\n",
       "1         42         1   0   0   1    2    0    0.0    0.0    0.0  0.584610\n",
       "2         42         1   0   0   2    2    0    0.0    0.0    0.0  0.590888\n",
       "3         42         1   0   0   3    2    0    0.0    0.0    0.0  0.597230\n",
       "4         42         1   0   0   4    2    0    0.0    0.0    0.0  0.603638\n",
       "...      ...       ...  ..  ..  ..  ...  ...    ...    ...    ...       ...\n",
       "1081339   42         3  43  63  59    2    1    0.0    0.0    0.0  1.865481\n",
       "1081340   42         3  43  63  60    2    1    0.0    0.0    0.0  1.910840\n",
       "1081341   42         3  43  63  61    2    1    0.0    0.0    0.0  1.957465\n",
       "1081342   42         3  43  63  62    2    1    0.0    0.0    0.0  2.005388\n",
       "1081343   42         3  43  63  63    2    1    0.0    0.0    0.0  2.054645\n",
       "\n",
       "[1081344 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s = pd.DataFrame(data_s, columns=columns_mean)\n",
    "df_s = df_s.astype({\"exp\": int, \"i\": int, \"j\": int, \"k\": int, \"mini_exp\": int, \"amp\":int, \"neg\": int})\n",
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_c, df_s], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13336576 entries, 0 to 13336575\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Dtype  \n",
      "---  ------    -----  \n",
      " 0   exp       int64  \n",
      " 1   mini_exp  int64  \n",
      " 2   i         int64  \n",
      " 3   j         int64  \n",
      " 4   k         int64  \n",
      " 5   amp       int64  \n",
      " 6   neg       int64  \n",
      " 7   mean0     float64\n",
      " 8   mean1     float64\n",
      " 9   mean2     float64\n",
      " 10  theory    float64\n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 1.1 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 1. fmap mean all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train = df[~((df['exp']==42) & ((df['mini_exp']==2) | (df['mini_exp']==3))) & ~((df['exp']==36) & ((df['mini_exp']==5) | (df['mini_exp']==6)))]\n",
    "df1_val =  df[((df['exp']==42) & (df['mini_exp']==2)) | (df['exp']==36) & (df['mini_exp']==5)]\n",
    "df1_test =  df[((df['exp']==42) & (df['mini_exp']==3)) | (df['exp']==36) & (df['mini_exp']==6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11894784, 11)\n",
      "(720896, 11)\n",
      "(720896, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df1_train.shape)\n",
    "print(df1_val.shape)\n",
    "print(df1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615675</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615676</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615677</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615678</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615679</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11894784 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          neg  mean0  mean1  mean2\n",
       "0           0    0.0    0.0    0.0\n",
       "1           0    0.0    0.0    0.0\n",
       "2           0    0.0    0.0    0.0\n",
       "3           0    0.0    0.0    0.0\n",
       "4           0    0.0    0.0    0.0\n",
       "...       ...    ...    ...    ...\n",
       "12615675    1    0.0    0.0    0.0\n",
       "12615676    1    0.0    0.0    0.0\n",
       "12615677    1    0.0    0.0    0.0\n",
       "12615678    1    0.0    0.0    0.0\n",
       "12615679    1    0.0    0.0    0.0\n",
       "\n",
       "[11894784 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_train.iloc[:, 6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = df1_train.iloc[:, 6:-1].values \n",
    "y1_train = df1_train['theory'].values\n",
    "\n",
    "X1_test = df1_val.iloc[:, 6:-1].values \n",
    "y1_test = df1_val['theory'].values\n",
    "\n",
    "X1_pred = df1_test.iloc[:, 6:-1].values \n",
    "y1_pred = df1_val['theory'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11894784, 4)\n",
      "(11894784,)\n",
      "(720896, 4)\n",
      "(720896,)\n",
      "(720896, 4)\n",
      "(720896,)\n"
     ]
    }
   ],
   "source": [
    "print(X1_train.shape)\n",
    "print(y1_train.shape)\n",
    "print(X1_test.shape)\n",
    "print(y1_test.shape)\n",
    "print(X1_pred.shape)\n",
    "print(y1_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data4: None zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonzero = df[(df['mean0']!=0.0) & (df['mean1']!=0.0) & (df['mean2']!=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26572</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-938.799683</td>\n",
       "      <td>-948.025818</td>\n",
       "      <td>-957.294250</td>\n",
       "      <td>2.563399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26573</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-869.659546</td>\n",
       "      <td>-878.644653</td>\n",
       "      <td>-889.680359</td>\n",
       "      <td>2.756330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26574</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-758.812622</td>\n",
       "      <td>-766.461914</td>\n",
       "      <td>-778.177979</td>\n",
       "      <td>2.966980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26575</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-644.983826</td>\n",
       "      <td>-651.566284</td>\n",
       "      <td>-664.997986</td>\n",
       "      <td>3.197531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26576</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-522.547424</td>\n",
       "      <td>-530.130432</td>\n",
       "      <td>-542.555176</td>\n",
       "      <td>3.450627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322144</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>531.209412</td>\n",
       "      <td>524.617249</td>\n",
       "      <td>528.361572</td>\n",
       "      <td>1.120325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322197</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-738.791687</td>\n",
       "      <td>-309.708313</td>\n",
       "      <td>248.529739</td>\n",
       "      <td>0.942770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322198</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-854.326355</td>\n",
       "      <td>-827.500061</td>\n",
       "      <td>-840.415222</td>\n",
       "      <td>0.957650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322260</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-524.266296</td>\n",
       "      <td>-949.864380</td>\n",
       "      <td>-932.718628</td>\n",
       "      <td>0.927170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13322261</th>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-783.382935</td>\n",
       "      <td>-823.340027</td>\n",
       "      <td>-807.056702</td>\n",
       "      <td>0.941901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2463088 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          exp  mini_exp   i   j   k  amp  neg       mean0       mean1  \\\n",
       "26572      32         1   6  31  12    2    0 -938.799683 -948.025818   \n",
       "26573      32         1   6  31  13    2    0 -869.659546 -878.644653   \n",
       "26574      32         1   6  31  14    2    0 -758.812622 -766.461914   \n",
       "26575      32         1   6  31  15    2    0 -644.983826 -651.566284   \n",
       "26576      32         1   6  31  16    2    0 -522.547424 -530.130432   \n",
       "...       ...       ...  ..  ..  ..  ...  ...         ...         ...   \n",
       "13322144   42         3  40  30  32    2    1  531.209412  524.617249   \n",
       "13322197   42         3  40  31  21    2    1 -738.791687 -309.708313   \n",
       "13322198   42         3  40  31  22    2    1 -854.326355 -827.500061   \n",
       "13322260   42         3  40  32  20    2    1 -524.266296 -949.864380   \n",
       "13322261   42         3  40  32  21    2    1 -783.382935 -823.340027   \n",
       "\n",
       "               mean2    theory  \n",
       "26572    -957.294250  2.563399  \n",
       "26573    -889.680359  2.756330  \n",
       "26574    -778.177979  2.966980  \n",
       "26575    -664.997986  3.197531  \n",
       "26576    -542.555176  3.450627  \n",
       "...              ...       ...  \n",
       "13322144  528.361572  1.120325  \n",
       "13322197  248.529739  0.942770  \n",
       "13322198 -840.415222  0.957650  \n",
       "13322260 -932.718628  0.927170  \n",
       "13322261 -807.056702  0.941901  \n",
       "\n",
       "[2463088 rows x 11 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_train = df_nonzero[~((df_nonzero['exp']==42) & ((df_nonzero['mini_exp']==2) | (df_nonzero['mini_exp']==3))) & ~((df_nonzero['exp']==36) & ((df_nonzero['mini_exp']==5) | (df_nonzero['mini_exp']==6)))]\n",
    "df4_test =  df_nonzero[((df_nonzero['exp']==42) & (df_nonzero['mini_exp']==2)) | ((df_nonzero['exp']==36) & (df_nonzero['mini_exp']==5))]\n",
    "df4_pred1 =  df_nonzero[((df_nonzero['exp']==42) & (df_nonzero['mini_exp']==3))]\n",
    "df4_pred2 =  df_nonzero[((df_nonzero['exp']==36) & (df_nonzero['mini_exp']==6))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train = df4_train.iloc[:, 6:-1].values\n",
    "y4_train = df4_train['theory'].values\n",
    "\n",
    "X4_test = df4_test.iloc[:, 6:-1].values\n",
    "y4_test = df4_test['theory'].values\n",
    "\n",
    "X4_pred1 = df4_pred1.iloc[:, 6:-1].values\n",
    "y4_pred1 = df4_pred1['theory'].values\n",
    "\n",
    "X4_pred2 = df4_pred2.iloc[:, 6:-1].values\n",
    "y4_pred2 = df4_pred2['theory'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2218526, 4)\n",
      "(2218526,)\n",
      "(122280, 4)\n",
      "(122280,)\n",
      "(54256, 4)\n",
      "(54256,)\n",
      "(68026, 4)\n",
      "(68026,)\n"
     ]
    }
   ],
   "source": [
    "print(X4_train.shape)\n",
    "print(y4_train.shape)\n",
    "print(X4_test.shape)\n",
    "print(y4_test.shape)\n",
    "print(X4_pred1.shape)\n",
    "print(y4_pred1.shape)\n",
    "print(X4_pred2.shape)\n",
    "print(y4_pred2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X1_train\n",
    "y_train = y1_train\n",
    "X_test = X1_test\n",
    "y_test = y1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (len(X_train[0]),)\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=shape)) \n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='RMSprop', loss='mse', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(model_path+'best_model_nn_neg.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "360439/360448 [============================>.] - ETA: 0s - loss: 2626.2849 - accuracy: 0.7654\n",
      "Epoch 1: val_loss improved from inf to 21.85946, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 370s 1ms/step - loss: 2626.2195 - accuracy: 0.7654 - val_loss: 21.8595 - val_accuracy: 0.8112\n",
      "Epoch 2/300\n",
      "360426/360448 [============================>.] - ETA: 0s - loss: 2621.2356 - accuracy: 0.7302\n",
      "Epoch 2: val_loss improved from 21.85946 to 20.61890, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 387s 1ms/step - loss: 2621.0784 - accuracy: 0.7302 - val_loss: 20.6189 - val_accuracy: 0.8114\n",
      "Epoch 3/300\n",
      "360429/360448 [============================>.] - ETA: 0s - loss: 2621.6799 - accuracy: 0.7387\n",
      "Epoch 3: val_loss improved from 20.61890 to 18.55254, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 369s 1ms/step - loss: 2621.5420 - accuracy: 0.7387 - val_loss: 18.5525 - val_accuracy: 1.9698e-04\n",
      "Epoch 4/300\n",
      "360443/360448 [============================>.] - ETA: 0s - loss: 2619.5203 - accuracy: 0.7236\n",
      "Epoch 4: val_loss improved from 18.55254 to 15.03494, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 394s 1ms/step - loss: 2619.4839 - accuracy: 0.7236 - val_loss: 15.0349 - val_accuracy: 0.8114\n",
      "Epoch 5/300\n",
      "360396/360448 [============================>.] - ETA: 0s - loss: 2621.0249 - accuracy: 0.7441\n",
      "Epoch 5: val_loss did not improve from 15.03494\n",
      "360448/360448 [==============================] - 396s 1ms/step - loss: 2620.6472 - accuracy: 0.7441 - val_loss: 16.4712 - val_accuracy: 0.8114\n",
      "Epoch 6/300\n",
      "360422/360448 [============================>.] - ETA: 0s - loss: 2620.2285 - accuracy: 0.7264\n",
      "Epoch 6: val_loss did not improve from 15.03494\n",
      "360448/360448 [==============================] - 346s 960us/step - loss: 2620.0393 - accuracy: 0.7264 - val_loss: 16.0519 - val_accuracy: 0.8114\n",
      "Epoch 7/300\n",
      "360416/360448 [============================>.] - ETA: 0s - loss: 2620.6765 - accuracy: 0.7083\n",
      "Epoch 7: val_loss did not improve from 15.03494\n",
      "360448/360448 [==============================] - 345s 957us/step - loss: 2620.4453 - accuracy: 0.7083 - val_loss: 21.4826 - val_accuracy: 0.8114\n",
      "Epoch 8/300\n",
      "360401/360448 [============================>.] - ETA: 0s - loss: 2619.2126 - accuracy: 0.7069\n",
      "Epoch 8: val_loss did not improve from 15.03494\n",
      "360448/360448 [==============================] - 345s 956us/step - loss: 2618.8713 - accuracy: 0.7069 - val_loss: 15.9696 - val_accuracy: 2.1640e-04\n",
      "Epoch 9/300\n",
      "360439/360448 [============================>.] - ETA: 0s - loss: 2620.2004 - accuracy: 0.6906\n",
      "Epoch 9: val_loss improved from 15.03494 to 14.84794, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 345s 957us/step - loss: 2620.1350 - accuracy: 0.6906 - val_loss: 14.8479 - val_accuracy: 1.7201e-04\n",
      "Epoch 10/300\n",
      "360447/360448 [============================>.] - ETA: 0s - loss: 2621.9055 - accuracy: 0.7089\n",
      "Epoch 10: val_loss did not improve from 14.84794\n",
      "360448/360448 [==============================] - 347s 962us/step - loss: 2621.8982 - accuracy: 0.7089 - val_loss: 25.4533 - val_accuracy: 0.8114\n",
      "Epoch 11/300\n",
      "360422/360448 [============================>.] - ETA: 0s - loss: 2627.8340 - accuracy: 0.7014\n",
      "Epoch 11: val_loss did not improve from 14.84794\n",
      "360448/360448 [==============================] - 361s 1ms/step - loss: 2627.6448 - accuracy: 0.7014 - val_loss: 20.2374 - val_accuracy: 0.8114\n",
      "Epoch 12/300\n",
      "360442/360448 [============================>.] - ETA: 0s - loss: 2620.4819 - accuracy: 0.6801\n",
      "Epoch 12: val_loss did not improve from 14.84794\n",
      "360448/360448 [==============================] - 366s 1ms/step - loss: 2620.4382 - accuracy: 0.6801 - val_loss: 18.0044 - val_accuracy: 2.1362e-04\n",
      "Epoch 13/300\n",
      "360417/360448 [============================>.] - ETA: 0s - loss: 2620.4651 - accuracy: 0.6794\n",
      "Epoch 13: val_loss did not improve from 14.84794\n",
      "360448/360448 [==============================] - 371s 1ms/step - loss: 2620.2410 - accuracy: 0.6794 - val_loss: 15.5057 - val_accuracy: 0.8114\n",
      "Epoch 14/300\n",
      "360433/360448 [============================>.] - ETA: 0s - loss: 2621.8584 - accuracy: 0.7081\n",
      "Epoch 14: val_loss did not improve from 14.84794\n",
      "360448/360448 [==============================] - 381s 1ms/step - loss: 2621.7493 - accuracy: 0.7081 - val_loss: 16.5437 - val_accuracy: 0.8114\n",
      "Epoch 15/300\n",
      "360433/360448 [============================>.] - ETA: 0s - loss: 2621.8850 - accuracy: 0.7103\n",
      "Epoch 15: val_loss improved from 14.84794 to 14.61182, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 411s 1ms/step - loss: 2621.7761 - accuracy: 0.7103 - val_loss: 14.6118 - val_accuracy: 0.8114\n",
      "Epoch 16/300\n",
      "360446/360448 [============================>.] - ETA: 0s - loss: 2619.6006 - accuracy: 0.7404\n",
      "Epoch 16: val_loss did not improve from 14.61182\n",
      "360448/360448 [==============================] - 355s 986us/step - loss: 2619.5862 - accuracy: 0.7404 - val_loss: 14.7390 - val_accuracy: 0.8114\n",
      "Epoch 17/300\n",
      "360427/360448 [============================>.] - ETA: 0s - loss: 2619.5840 - accuracy: 0.7269\n",
      "Epoch 17: val_loss did not improve from 14.61182\n",
      "360448/360448 [==============================] - 345s 956us/step - loss: 2619.4314 - accuracy: 0.7269 - val_loss: 16.4355 - val_accuracy: 0.8114\n",
      "Epoch 18/300\n",
      "360433/360448 [============================>.] - ETA: 0s - loss: 2619.8174 - accuracy: 0.6910\n",
      "Epoch 18: val_loss did not improve from 14.61182\n",
      "360448/360448 [==============================] - 345s 958us/step - loss: 2619.7495 - accuracy: 0.6910 - val_loss: 16.3081 - val_accuracy: 0.8114\n",
      "Epoch 19/300\n",
      "360424/360448 [============================>.] - ETA: 0s - loss: 2619.8105 - accuracy: 0.6563\n",
      "Epoch 19: val_loss did not improve from 14.61182\n",
      "360448/360448 [==============================] - 349s 969us/step - loss: 2619.6365 - accuracy: 0.6563 - val_loss: 14.9790 - val_accuracy: 0.8114\n",
      "Epoch 20/300\n",
      "360430/360448 [============================>.] - ETA: 0s - loss: 2620.4150 - accuracy: 0.6642\n",
      "Epoch 20: val_loss improved from 14.61182 to 14.12867, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_norm.h5\n",
      "360448/360448 [==============================] - 376s 1ms/step - loss: 2620.2842 - accuracy: 0.6642 - val_loss: 14.1287 - val_accuracy: 0.8114\n",
      "Epoch 21/300\n",
      "360419/360448 [============================>.] - ETA: 0s - loss: 2624.1150 - accuracy: 0.6502\n",
      "Epoch 21: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 347s 963us/step - loss: 2623.9055 - accuracy: 0.6502 - val_loss: 24.4345 - val_accuracy: 0.8113\n",
      "Epoch 22/300\n",
      "360444/360448 [============================>.] - ETA: 0s - loss: 2627.0898 - accuracy: 0.6354\n",
      "Epoch 22: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 347s 963us/step - loss: 2627.0608 - accuracy: 0.6354 - val_loss: 24.1419 - val_accuracy: 2.1085e-04\n",
      "Epoch 23/300\n",
      "360403/360448 [============================>.] - ETA: 0s - loss: 2621.1357 - accuracy: 0.7156\n",
      "Epoch 23: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 348s 966us/step - loss: 2620.8154 - accuracy: 0.7156 - val_loss: 18.7798 - val_accuracy: 0.8112\n",
      "Epoch 24/300\n",
      "360438/360448 [============================>.] - ETA: 0s - loss: 2619.5916 - accuracy: 0.7301\n",
      "Epoch 24: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 953us/step - loss: 2619.5188 - accuracy: 0.7301 - val_loss: 19.7253 - val_accuracy: 0.8114\n",
      "Epoch 25/300\n",
      "360405/360448 [============================>.] - ETA: 0s - loss: 2621.8657 - accuracy: 0.7145\n",
      "Epoch 25: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 952us/step - loss: 2621.5530 - accuracy: 0.7145 - val_loss: 20.4040 - val_accuracy: 1.9420e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/300\n",
      "360439/360448 [============================>.] - ETA: 0s - loss: 2620.0681 - accuracy: 0.7048\n",
      "Epoch 26: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 953us/step - loss: 2620.0027 - accuracy: 0.7048 - val_loss: 21.7032 - val_accuracy: 0.8112\n",
      "Epoch 27/300\n",
      "360403/360448 [============================>.] - ETA: 0s - loss: 2620.7729 - accuracy: 0.7177\n",
      "Epoch 27: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 952us/step - loss: 2620.4470 - accuracy: 0.7176 - val_loss: 20.4090 - val_accuracy: 0.8114\n",
      "Epoch 28/300\n",
      "360401/360448 [============================>.] - ETA: 0s - loss: 2620.2585 - accuracy: 0.7103\n",
      "Epoch 28: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 344s 953us/step - loss: 2619.9180 - accuracy: 0.7103 - val_loss: 16.9250 - val_accuracy: 0.8114\n",
      "Epoch 29/300\n",
      "360400/360448 [============================>.] - ETA: 0s - loss: 2620.1257 - accuracy: 0.7119\n",
      "Epoch 29: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 344s 955us/step - loss: 2619.7810 - accuracy: 0.7119 - val_loss: 19.6273 - val_accuracy: 0.8114\n",
      "Epoch 30/300\n",
      "360437/360448 [============================>.] - ETA: 0s - loss: 2619.9409 - accuracy: 0.7248\n",
      "Epoch 30: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 952us/step - loss: 2619.8613 - accuracy: 0.7248 - val_loss: 18.6042 - val_accuracy: 0.8114\n",
      "Epoch 31/300\n",
      "360431/360448 [============================>.] - ETA: 0s - loss: 2620.1846 - accuracy: 0.7345\n",
      "Epoch 31: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 381s 1ms/step - loss: 2620.0613 - accuracy: 0.7345 - val_loss: 18.0641 - val_accuracy: 0.8114\n",
      "Epoch 32/300\n",
      "360398/360448 [============================>.] - ETA: 0s - loss: 2619.9297 - accuracy: 0.7381\n",
      "Epoch 32: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 380s 1ms/step - loss: 2619.5667 - accuracy: 0.7381 - val_loss: 18.4952 - val_accuracy: 0.8114\n",
      "Epoch 33/300\n",
      "360435/360448 [============================>.] - ETA: 0s - loss: 2619.3423 - accuracy: 0.7312\n",
      "Epoch 33: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 348s 966us/step - loss: 2619.2478 - accuracy: 0.7312 - val_loss: 16.1863 - val_accuracy: 0.8114\n",
      "Epoch 34/300\n",
      "360448/360448 [==============================] - ETA: 0s - loss: 2619.6025 - accuracy: 0.7306\n",
      "Epoch 34: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 343s 953us/step - loss: 2619.6025 - accuracy: 0.7306 - val_loss: 19.2194 - val_accuracy: 2.1640e-04\n",
      "Epoch 35/300\n",
      "360395/360448 [============================>.] - ETA: 0s - loss: 2621.9373 - accuracy: 0.7267\n",
      "Epoch 35: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 344s 954us/step - loss: 2621.5522 - accuracy: 0.7267 - val_loss: 17.9452 - val_accuracy: 0.8112\n",
      "Epoch 36/300\n",
      "360403/360448 [============================>.] - ETA: 0s - loss: 2620.9937 - accuracy: 0.7068\n",
      "Epoch 36: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 344s 955us/step - loss: 2620.6697 - accuracy: 0.7068 - val_loss: 17.6544 - val_accuracy: 0.8114\n",
      "Epoch 37/300\n",
      "360403/360448 [============================>.] - ETA: 0s - loss: 2621.0032 - accuracy: 0.7322\n",
      "Epoch 37: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 344s 954us/step - loss: 2620.6768 - accuracy: 0.7322 - val_loss: 19.8101 - val_accuracy: 0.8114\n",
      "Epoch 38/300\n",
      "360434/360448 [============================>.] - ETA: 0s - loss: 2620.7056 - accuracy: 0.7381\n",
      "Epoch 38: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 346s 959us/step - loss: 2620.6040 - accuracy: 0.7381 - val_loss: 20.3099 - val_accuracy: 0.8114\n",
      "Epoch 39/300\n",
      "360442/360448 [============================>.] - ETA: 0s - loss: 2622.2981 - accuracy: 0.7361\n",
      "Epoch 39: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 353s 978us/step - loss: 2622.2546 - accuracy: 0.7361 - val_loss: 20.4575 - val_accuracy: 0.8114\n",
      "Epoch 40/300\n",
      "360427/360448 [============================>.] - ETA: 0s - loss: 2620.9407 - accuracy: 0.6382\n",
      "Epoch 40: val_loss did not improve from 14.12867\n",
      "360448/360448 [==============================] - 370s 1ms/step - loss: 2620.7881 - accuracy: 0.6383 - val_loss: 21.1190 - val_accuracy: 0.8114\n",
      "Epoch 41/300\n",
      "144318/360448 [===========>..................] - ETA: 3:43 - loss: 3325.8752 - accuracy: 0.5817"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py:1401\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m   1400\u001b[0m   data_handler\u001b[38;5;241m.\u001b[39m_initial_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_load_initial_step_from_ckpt()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1401\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[1;32m   1402\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m         epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m         step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m         _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m       callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/data_adapter.py:1248\u001b[0m, in \u001b[0;36mDataHandler.steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_insufficient_data:  \u001b[38;5;66;03m# Set by `catch_stop_iteration`.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m original_spe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1249\u001b[0m can_run_full_execution \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1250\u001b[0m     original_spe \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inferred_steps \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\n\u001b[1;32m   1253\u001b[0m     original_spe)\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_run_full_execution:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    638\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    639\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:712\u001b[0m, in \u001b[0;36mBaseResourceVariable.read_value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m\"\"\"Constructs an op which reads the value of this variable.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03mShould be used when there are multiple reads, or when it is desirable to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m the read operation.\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 712\u001b[0m   value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_variable_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Return an identity so it can get placed on whatever device the context\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# specifies instead of the device where the variable is.\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39midentity(value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:691\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m       result \u001b[38;5;241m=\u001b[39m read_and_set_handle()\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 691\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mread_and_set_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;66;03m# Note that if a control flow context is active the input of the read op\u001b[39;00m\n\u001b[1;32m    695\u001b[0m   \u001b[38;5;66;03m# might not actually be the handle. This line bypasses it.\u001b[39;00m\n\u001b[1;32m    696\u001b[0m   tape\u001b[38;5;241m.\u001b[39mrecord_operation(\n\u001b[1;32m    697\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReadVariableOp\u001b[39m\u001b[38;5;124m\"\u001b[39m, [result], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle],\n\u001b[1;32m    698\u001b[0m       backward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x],\n\u001b[1;32m    699\u001b[0m       forward_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: [x])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:681\u001b[0m, in \u001b[0;36mBaseResourceVariable._read_variable_op.<locals>.read_and_set_handle\u001b[0;34m()\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_and_set_handle\u001b[39m():\n\u001b[0;32m--> 681\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mgen_resource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_variable_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m   _maybe_set_handle_data(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, result)\n\u001b[1;32m    684\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py:479\u001b[0m, in \u001b[0;36mread_variable_op\u001b[0;34m(resource, dtype, name)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    478\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 479\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReadVariableOp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    482\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300, batch_size=32, validation_data=[X_test, y_test], callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "print(mse(train_pred, y_train))\n",
    "print(mape(train_pred, y_train))\n",
    "test_pred = model.predict(X_test)\n",
    "print(mse(test_pred, y_test))\n",
    "print(mape(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(test_pred.flatten(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_df = pd.DataFrame(history.history)\n",
    "model_df[['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_df[['accuracy', 'val_accuracy']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Training and Validation Accuray Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run NN for NonZero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X4_train\n",
    "y_train = y4_train\n",
    "X_test = X4_test\n",
    "y_test = y4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 16:14:30.156752: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "shape = (len(X_train[0]),)\n",
    "model = keras.Sequential()\n",
    "model.add(Dense(5, activation='relu', input_shape=shape)) \n",
    "model.add(Dense(5, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(optimizer='RMSprop', loss='mse', metrics=[\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(model_path+'best_model_nn_neg_both_nonzero2.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "34661/34665 [============================>.] - ETA: 0s - loss: 13586.1553 - mse: 13586.1553\n",
      "Epoch 1: val_loss improved from inf to 59.46264, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 33s 947us/step - loss: 13584.7998 - mse: 13584.7998 - val_loss: 59.4626 - val_mse: 59.4626\n",
      "Epoch 2/300\n",
      "34618/34665 [============================>.] - ETA: 0s - loss: 13602.9141 - mse: 13602.9141\n",
      "Epoch 2: val_loss improved from 59.46264 to 57.20226, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 33s 955us/step - loss: 13584.9287 - mse: 13584.9287 - val_loss: 57.2023 - val_mse: 57.2023\n",
      "Epoch 3/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13602.8701 - mse: 13602.8701\n",
      "Epoch 3: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 34s 968us/step - loss: 13582.6445 - mse: 13582.6445 - val_loss: 57.9853 - val_mse: 57.9853\n",
      "Epoch 4/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13584.7383 - mse: 13584.7383\n",
      "Epoch 4: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 947us/step - loss: 13584.1650 - mse: 13584.1650 - val_loss: 60.2782 - val_mse: 60.2782\n",
      "Epoch 5/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13598.6377 - mse: 13598.6377\n",
      "Epoch 5: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 948us/step - loss: 13582.4805 - mse: 13582.4805 - val_loss: 58.9107 - val_mse: 58.9107\n",
      "Epoch 6/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13585.6016 - mse: 13585.6016\n",
      "Epoch 6: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 944us/step - loss: 13583.0830 - mse: 13583.0830 - val_loss: 61.1163 - val_mse: 61.1163\n",
      "Epoch 7/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13598.9854 - mse: 13598.9854\n",
      "Epoch 7: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 946us/step - loss: 13581.4648 - mse: 13581.4648 - val_loss: 59.4936 - val_mse: 59.4936\n",
      "Epoch 8/300\n",
      "34636/34665 [============================>.] - ETA: 0s - loss: 13588.0293 - mse: 13588.0293\n",
      "Epoch 8: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 947us/step - loss: 13581.0488 - mse: 13581.0488 - val_loss: 62.2807 - val_mse: 62.2807\n",
      "Epoch 9/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13599.6094 - mse: 13599.6094\n",
      "Epoch 9: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 950us/step - loss: 13583.0635 - mse: 13583.0635 - val_loss: 57.4273 - val_mse: 57.4273\n",
      "Epoch 10/300\n",
      "34630/34665 [============================>.] - ETA: 0s - loss: 13595.9707 - mse: 13595.9707\n",
      "Epoch 10: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 955us/step - loss: 13582.6523 - mse: 13582.6523 - val_loss: 65.0070 - val_mse: 65.0070\n",
      "Epoch 11/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13582.1064 - mse: 13582.1064\n",
      "Epoch 11: val_loss did not improve from 57.20226\n",
      "34665/34665 [==============================] - 33s 954us/step - loss: 13582.1064 - mse: 13582.1064 - val_loss: 68.5709 - val_mse: 68.5709\n",
      "Epoch 12/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13581.6123 - mse: 13581.6123\n",
      "Epoch 12: val_loss improved from 57.20226 to 56.19434, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 33s 947us/step - loss: 13581.6123 - mse: 13581.6123 - val_loss: 56.1943 - val_mse: 56.1943\n",
      "Epoch 13/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13589.7383 - mse: 13589.7383\n",
      "Epoch 13: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 946us/step - loss: 13582.6592 - mse: 13582.6592 - val_loss: 58.3327 - val_mse: 58.3327\n",
      "Epoch 14/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13593.5811 - mse: 13593.5811\n",
      "Epoch 14: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 945us/step - loss: 13581.1621 - mse: 13581.1621 - val_loss: 56.8705 - val_mse: 56.8705\n",
      "Epoch 15/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13587.5352 - mse: 13587.5352\n",
      "Epoch 15: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 955us/step - loss: 13582.6748 - mse: 13582.6748 - val_loss: 60.2830 - val_mse: 60.2830\n",
      "Epoch 16/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13589.8525 - mse: 13589.8525\n",
      "Epoch 16: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 944us/step - loss: 13580.2812 - mse: 13580.2812 - val_loss: 71.0588 - val_mse: 71.0588\n",
      "Epoch 17/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13583.2383 - mse: 13583.2383\n",
      "Epoch 17: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 947us/step - loss: 13578.4238 - mse: 13578.4238 - val_loss: 58.1582 - val_mse: 58.1582\n",
      "Epoch 18/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13588.2949 - mse: 13588.2949\n",
      "Epoch 18: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 949us/step - loss: 13582.1436 - mse: 13582.1436 - val_loss: 58.8482 - val_mse: 58.8482\n",
      "Epoch 19/300\n",
      "34662/34665 [============================>.] - ETA: 0s - loss: 13582.2480 - mse: 13582.2480\n",
      "Epoch 19: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 946us/step - loss: 13581.2900 - mse: 13581.2900 - val_loss: 59.1719 - val_mse: 59.1719\n",
      "Epoch 20/300\n",
      "34633/34665 [============================>.] - ETA: 0s - loss: 13593.4102 - mse: 13593.4102\n",
      "Epoch 20: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 954us/step - loss: 13581.2188 - mse: 13581.2188 - val_loss: 57.5872 - val_mse: 57.5872\n",
      "Epoch 21/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13587.5293 - mse: 13587.5293\n",
      "Epoch 21: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 948us/step - loss: 13583.4736 - mse: 13583.4736 - val_loss: 58.9031 - val_mse: 58.9031\n",
      "Epoch 22/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13593.6562 - mse: 13593.6562\n",
      "Epoch 22: val_loss did not improve from 56.19434\n",
      "34665/34665 [==============================] - 33s 949us/step - loss: 13581.2666 - mse: 13581.2666 - val_loss: 57.0100 - val_mse: 57.0100\n",
      "Epoch 23/300\n",
      "34646/34665 [============================>.] - ETA: 0s - loss: 13590.9873 - mse: 13590.9873\n",
      "Epoch 23: val_loss improved from 56.19434 to 55.82032, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 33s 948us/step - loss: 13583.8330 - mse: 13583.8330 - val_loss: 55.8203 - val_mse: 55.8203\n",
      "Epoch 24/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13580.1045 - mse: 13580.1045\n",
      "Epoch 24: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 33s 945us/step - loss: 13579.9209 - mse: 13579.9209 - val_loss: 57.5366 - val_mse: 57.5366\n",
      "Epoch 25/300\n",
      "34646/34665 [============================>.] - ETA: 0s - loss: 13588.1045 - mse: 13588.1045\n",
      "Epoch 25: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 33s 951us/step - loss: 13580.8926 - mse: 13580.8926 - val_loss: 56.4884 - val_mse: 56.4884\n",
      "Epoch 26/300\n",
      "34638/34665 [============================>.] - ETA: 0s - loss: 13591.3877 - mse: 13591.3877\n",
      "Epoch 26: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 33s 949us/step - loss: 13581.0674 - mse: 13581.0674 - val_loss: 56.6596 - val_mse: 56.6596\n",
      "Epoch 27/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13597.3057 - mse: 13597.3057\n",
      "Epoch 27: val_loss did not improve from 55.82032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34665/34665 [==============================] - 33s 945us/step - loss: 13582.6680 - mse: 13582.6680 - val_loss: 57.2203 - val_mse: 57.2203\n",
      "Epoch 28/300\n",
      "34637/34665 [============================>.] - ETA: 0s - loss: 13591.4258 - mse: 13591.4258\n",
      "Epoch 28: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 33s 938us/step - loss: 13580.7188 - mse: 13580.7188 - val_loss: 58.2697 - val_mse: 58.2697\n",
      "Epoch 29/300\n",
      "34647/34665 [============================>.] - ETA: 0s - loss: 13589.8584 - mse: 13589.8584\n",
      "Epoch 29: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 33s 943us/step - loss: 13583.0410 - mse: 13583.0410 - val_loss: 56.8333 - val_mse: 56.8333\n",
      "Epoch 30/300\n",
      "34620/34665 [============================>.] - ETA: 0s - loss: 13599.8027 - mse: 13599.8027\n",
      "Epoch 30: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 32s 934us/step - loss: 13582.9707 - mse: 13582.9707 - val_loss: 57.1114 - val_mse: 57.1114\n",
      "Epoch 31/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13597.9775 - mse: 13597.9775\n",
      "Epoch 31: val_loss did not improve from 55.82032\n",
      "34665/34665 [==============================] - 32s 937us/step - loss: 13583.4492 - mse: 13583.4492 - val_loss: 61.8055 - val_mse: 61.8055\n",
      "Epoch 32/300\n",
      "34644/34665 [============================>.] - ETA: 0s - loss: 13590.2783 - mse: 13590.2783\n",
      "Epoch 32: val_loss improved from 55.82032 to 55.76436, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 33s 942us/step - loss: 13582.3271 - mse: 13582.3271 - val_loss: 55.7644 - val_mse: 55.7644\n",
      "Epoch 33/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13596.1289 - mse: 13596.1289\n",
      "Epoch 33: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 32s 937us/step - loss: 13579.6494 - mse: 13579.6494 - val_loss: 55.8916 - val_mse: 55.8916\n",
      "Epoch 34/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13592.1826 - mse: 13592.1826\n",
      "Epoch 34: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 33s 938us/step - loss: 13580.6846 - mse: 13580.6846 - val_loss: 55.8661 - val_mse: 55.8661\n",
      "Epoch 35/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13568.1846 - mse: 13568.1846\n",
      "Epoch 35: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 32s 936us/step - loss: 13580.5342 - mse: 13580.5342 - val_loss: 70.7586 - val_mse: 70.7586\n",
      "Epoch 36/300\n",
      "34618/34665 [============================>.] - ETA: 0s - loss: 13600.4248 - mse: 13600.4248\n",
      "Epoch 36: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 33s 938us/step - loss: 13582.3135 - mse: 13582.3135 - val_loss: 61.7924 - val_mse: 61.7924\n",
      "Epoch 37/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13583.9941 - mse: 13583.9941\n",
      "Epoch 37: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 34s 993us/step - loss: 13578.9463 - mse: 13578.9463 - val_loss: 65.4446 - val_mse: 65.4446\n",
      "Epoch 38/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13587.4932 - mse: 13587.4932\n",
      "Epoch 38: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 37s 1ms/step - loss: 13582.4082 - mse: 13582.4082 - val_loss: 58.7517 - val_mse: 58.7517\n",
      "Epoch 39/300\n",
      "34631/34665 [============================>.] - ETA: 0s - loss: 13594.3242 - mse: 13594.3242\n",
      "Epoch 39: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13581.4609 - mse: 13581.4609 - val_loss: 58.6302 - val_mse: 58.6302\n",
      "Epoch 40/300\n",
      "34660/34665 [============================>.] - ETA: 0s - loss: 13583.3037 - mse: 13583.3037\n",
      "Epoch 40: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 37s 1ms/step - loss: 13581.6523 - mse: 13581.6523 - val_loss: 62.9362 - val_mse: 62.9362\n",
      "Epoch 41/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13593.6582 - mse: 13593.6582\n",
      "Epoch 41: val_loss did not improve from 55.76436\n",
      "34665/34665 [==============================] - 37s 1ms/step - loss: 13579.9111 - mse: 13579.9111 - val_loss: 59.1958 - val_mse: 59.1958\n",
      "Epoch 42/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13593.9160 - mse: 13593.9160\n",
      "Epoch 42: val_loss improved from 55.76436 to 55.29700, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13580.0938 - mse: 13580.0938 - val_loss: 55.2970 - val_mse: 55.2970\n",
      "Epoch 43/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13584.2881 - mse: 13584.2881\n",
      "Epoch 43: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 37s 1ms/step - loss: 13580.5898 - mse: 13580.5898 - val_loss: 60.4755 - val_mse: 60.4755\n",
      "Epoch 44/300\n",
      "34660/34665 [============================>.] - ETA: 0s - loss: 13579.3691 - mse: 13579.3691\n",
      "Epoch 44: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13577.6221 - mse: 13577.6221 - val_loss: 57.1835 - val_mse: 57.1835\n",
      "Epoch 45/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 12551.4160 - mse: 12551.4160\n",
      "Epoch 45: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13581.3311 - mse: 13581.3311 - val_loss: 60.6776 - val_mse: 60.6776\n",
      "Epoch 46/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13594.3906 - mse: 13594.3906\n",
      "Epoch 46: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13581.7920 - mse: 13581.7920 - val_loss: 56.5008 - val_mse: 56.5008\n",
      "Epoch 47/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13592.9951 - mse: 13592.9951\n",
      "Epoch 47: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13581.0967 - mse: 13581.0967 - val_loss: 58.2123 - val_mse: 58.2123\n",
      "Epoch 48/300\n",
      "34636/34665 [============================>.] - ETA: 0s - loss: 13590.9023 - mse: 13590.9023\n",
      "Epoch 48: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13579.8408 - mse: 13579.8408 - val_loss: 57.6827 - val_mse: 57.6827\n",
      "Epoch 49/300\n",
      "34644/34665 [============================>.] - ETA: 0s - loss: 13588.8271 - mse: 13588.8271\n",
      "Epoch 49: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13580.8867 - mse: 13580.8867 - val_loss: 60.9232 - val_mse: 60.9232\n",
      "Epoch 50/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13593.4570 - mse: 13593.4570\n",
      "Epoch 50: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 37s 1ms/step - loss: 13579.8564 - mse: 13579.8564 - val_loss: 56.5674 - val_mse: 56.5674\n",
      "Epoch 51/300\n",
      "34662/34665 [============================>.] - ETA: 0s - loss: 13582.1895 - mse: 13582.1895\n",
      "Epoch 51: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13581.2441 - mse: 13581.2441 - val_loss: 71.4598 - val_mse: 71.4598\n",
      "Epoch 52/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13589.6299 - mse: 13589.6299\n",
      "Epoch 52: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13577.8887 - mse: 13577.8887 - val_loss: 71.1605 - val_mse: 71.1605\n",
      "Epoch 53/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13594.7412 - mse: 13594.7412\n",
      "Epoch 53: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 36s 1ms/step - loss: 13580.9873 - mse: 13580.9873 - val_loss: 57.6998 - val_mse: 57.6998\n",
      "Epoch 54/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13596.9150 - mse: 13596.9150\n",
      "Epoch 54: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 33s 964us/step - loss: 13579.3164 - mse: 13579.3164 - val_loss: 56.1067 - val_mse: 56.1067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13592.2432 - mse: 13592.2432\n",
      "Epoch 55: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 908us/step - loss: 13579.6035 - mse: 13579.6035 - val_loss: 59.6073 - val_mse: 59.6073\n",
      "Epoch 56/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13586.7363 - mse: 13586.7363\n",
      "Epoch 56: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 906us/step - loss: 13578.9727 - mse: 13578.9727 - val_loss: 57.5890 - val_mse: 57.5890\n",
      "Epoch 57/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13584.4004 - mse: 13584.4004\n",
      "Epoch 57: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 900us/step - loss: 13580.3154 - mse: 13580.3154 - val_loss: 59.6397 - val_mse: 59.6397\n",
      "Epoch 58/300\n",
      "34611/34665 [============================>.] - ETA: 0s - loss: 13601.4365 - mse: 13601.4365\n",
      "Epoch 58: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13580.9141 - mse: 13580.9141 - val_loss: 57.0980 - val_mse: 57.0980\n",
      "Epoch 59/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13595.6943 - mse: 13595.6943\n",
      "Epoch 59: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13581.0801 - mse: 13581.0801 - val_loss: 59.0942 - val_mse: 59.0942\n",
      "Epoch 60/300\n",
      "34607/34665 [============================>.] - ETA: 0s - loss: 13603.7676 - mse: 13603.7676\n",
      "Epoch 60: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13581.3594 - mse: 13581.3594 - val_loss: 55.5742 - val_mse: 55.5742\n",
      "Epoch 61/300\n",
      "34649/34665 [============================>.] - ETA: 0s - loss: 13586.9131 - mse: 13586.9131\n",
      "Epoch 61: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13581.0049 - mse: 13581.0049 - val_loss: 60.0768 - val_mse: 60.0768\n",
      "Epoch 62/300\n",
      "34647/34665 [============================>.] - ETA: 0s - loss: 13587.5107 - mse: 13587.5107\n",
      "Epoch 62: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13580.7012 - mse: 13580.7012 - val_loss: 59.5827 - val_mse: 59.5827\n",
      "Epoch 63/300\n",
      "34611/34665 [============================>.] - ETA: 0s - loss: 13496.9590 - mse: 13496.9590\n",
      "Epoch 63: val_loss did not improve from 55.29700\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13580.6582 - mse: 13580.6582 - val_loss: 59.8123 - val_mse: 59.8123\n",
      "Epoch 64/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13592.2021 - mse: 13592.2021\n",
      "Epoch 64: val_loss improved from 55.29700 to 54.44996, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13578.4482 - mse: 13578.4482 - val_loss: 54.4500 - val_mse: 54.4500\n",
      "Epoch 65/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13593.8174 - mse: 13593.8174\n",
      "Epoch 65: val_loss did not improve from 54.44996\n",
      "34665/34665 [==============================] - 32s 911us/step - loss: 13581.1494 - mse: 13581.1494 - val_loss: 60.0697 - val_mse: 60.0697\n",
      "Epoch 66/300\n",
      "34610/34665 [============================>.] - ETA: 0s - loss: 13601.6152 - mse: 13601.6152\n",
      "Epoch 66: val_loss did not improve from 54.44996\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13580.4414 - mse: 13580.4414 - val_loss: 57.0896 - val_mse: 57.0896\n",
      "Epoch 67/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13591.6953 - mse: 13591.6953\n",
      "Epoch 67: val_loss improved from 54.44996 to 54.16714, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.5205 - mse: 13576.5205 - val_loss: 54.1671 - val_mse: 54.1671\n",
      "Epoch 68/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13597.3975 - mse: 13597.3975\n",
      "Epoch 68: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13579.6797 - mse: 13579.6797 - val_loss: 54.9151 - val_mse: 54.9151\n",
      "Epoch 69/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13592.9072 - mse: 13592.9072\n",
      "Epoch 69: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13580.8750 - mse: 13580.8750 - val_loss: 57.7327 - val_mse: 57.7327\n",
      "Epoch 70/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13582.1982 - mse: 13582.1982\n",
      "Epoch 70: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13579.6719 - mse: 13579.6719 - val_loss: 55.5884 - val_mse: 55.5884\n",
      "Epoch 71/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13579.8848 - mse: 13579.8848\n",
      "Epoch 71: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13579.8848 - mse: 13579.8848 - val_loss: 58.3542 - val_mse: 58.3542\n",
      "Epoch 72/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13576.1494 - mse: 13576.1494\n",
      "Epoch 72: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 906us/step - loss: 13580.2295 - mse: 13580.2295 - val_loss: 120.0842 - val_mse: 120.0842\n",
      "Epoch 73/300\n",
      "34615/34665 [============================>.] - ETA: 0s - loss: 13585.6523 - mse: 13585.6523\n",
      "Epoch 73: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13579.7461 - mse: 13579.7461 - val_loss: 69.2428 - val_mse: 69.2428\n",
      "Epoch 74/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13585.5537 - mse: 13585.5537\n",
      "Epoch 74: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 32s 909us/step - loss: 13579.1670 - mse: 13579.1670 - val_loss: 58.3336 - val_mse: 58.3336\n",
      "Epoch 75/300\n",
      "34609/34665 [============================>.] - ETA: 0s - loss: 13599.6338 - mse: 13599.6338\n",
      "Epoch 75: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13582.1699 - mse: 13582.1699 - val_loss: 57.7114 - val_mse: 57.7114\n",
      "Epoch 76/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13599.2666 - mse: 13599.2666\n",
      "Epoch 76: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13581.0049 - mse: 13581.0049 - val_loss: 56.8214 - val_mse: 56.8214\n",
      "Epoch 77/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13583.7363 - mse: 13583.7363\n",
      "Epoch 77: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13580.0488 - mse: 13580.0488 - val_loss: 54.6484 - val_mse: 54.6484\n",
      "Epoch 78/300\n",
      "34610/34665 [============================>.] - ETA: 0s - loss: 13602.3887 - mse: 13602.3887\n",
      "Epoch 78: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13581.3672 - mse: 13581.3672 - val_loss: 59.0625 - val_mse: 59.0625\n",
      "Epoch 79/300\n",
      "34618/34665 [============================>.] - ETA: 0s - loss: 13597.2910 - mse: 13597.2910\n",
      "Epoch 79: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13579.2852 - mse: 13579.2852 - val_loss: 55.8105 - val_mse: 55.8105\n",
      "Epoch 80/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13590.1826 - mse: 13590.1826\n",
      "Epoch 80: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.2676 - mse: 13580.2676 - val_loss: 66.7254 - val_mse: 66.7254\n",
      "Epoch 81/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13586.8789 - mse: 13586.8789\n",
      "Epoch 81: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13582.0049 - mse: 13582.0049 - val_loss: 56.6406 - val_mse: 56.6406\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34621/34665 [============================>.] - ETA: 0s - loss: 13598.3301 - mse: 13598.3301\n",
      "Epoch 82: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13581.8584 - mse: 13581.8584 - val_loss: 65.2830 - val_mse: 65.2830\n",
      "Epoch 83/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13585.6328 - mse: 13585.6328\n",
      "Epoch 83: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 903us/step - loss: 13579.2744 - mse: 13579.2744 - val_loss: 56.8596 - val_mse: 56.8596\n",
      "Epoch 84/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13596.9443 - mse: 13596.9443\n",
      "Epoch 84: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13579.5713 - mse: 13579.5713 - val_loss: 57.8565 - val_mse: 57.8565\n",
      "Epoch 85/300\n",
      "34621/34665 [============================>.] - ETA: 0s - loss: 13597.3379 - mse: 13597.3379\n",
      "Epoch 85: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 905us/step - loss: 13580.4658 - mse: 13580.4658 - val_loss: 56.7936 - val_mse: 56.7936\n",
      "Epoch 86/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13591.0869 - mse: 13591.0869\n",
      "Epoch 86: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13580.3291 - mse: 13580.3291 - val_loss: 61.6798 - val_mse: 61.6798\n",
      "Epoch 87/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13598.1963 - mse: 13598.1963\n",
      "Epoch 87: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 32s 912us/step - loss: 13580.5879 - mse: 13580.5879 - val_loss: 56.7493 - val_mse: 56.7493\n",
      "Epoch 88/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13581.9707 - mse: 13581.9707\n",
      "Epoch 88: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13581.3955 - mse: 13581.3955 - val_loss: 60.8158 - val_mse: 60.8158\n",
      "Epoch 89/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13585.5586 - mse: 13585.5586\n",
      "Epoch 89: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.6875 - mse: 13580.6875 - val_loss: 56.1539 - val_mse: 56.1539\n",
      "Epoch 90/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13602.1104 - mse: 13602.1104\n",
      "Epoch 90: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13581.6934 - mse: 13581.6934 - val_loss: 59.5667 - val_mse: 59.5667\n",
      "Epoch 91/300\n",
      "34607/34665 [============================>.] - ETA: 0s - loss: 13600.0830 - mse: 13600.0830\n",
      "Epoch 91: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 903us/step - loss: 13577.7764 - mse: 13577.7764 - val_loss: 55.4925 - val_mse: 55.4925\n",
      "Epoch 92/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13586.9111 - mse: 13586.9111\n",
      "Epoch 92: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13582.0439 - mse: 13582.0439 - val_loss: 60.0273 - val_mse: 60.0273\n",
      "Epoch 93/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13585.9707 - mse: 13585.9707\n",
      "Epoch 93: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13581.1016 - mse: 13581.1016 - val_loss: 62.2662 - val_mse: 62.2662\n",
      "Epoch 94/300\n",
      "34633/34665 [============================>.] - ETA: 0s - loss: 13591.7998 - mse: 13591.7998\n",
      "Epoch 94: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 903us/step - loss: 13579.5576 - mse: 13579.5576 - val_loss: 57.1030 - val_mse: 57.1030\n",
      "Epoch 95/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13578.8066 - mse: 13578.8066\n",
      "Epoch 95: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13578.2334 - mse: 13578.2334 - val_loss: 57.8583 - val_mse: 57.8583\n",
      "Epoch 96/300\n",
      "34661/34665 [============================>.] - ETA: 0s - loss: 13581.3711 - mse: 13581.3711\n",
      "Epoch 96: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.0215 - mse: 13580.0215 - val_loss: 75.1706 - val_mse: 75.1706\n",
      "Epoch 97/300\n",
      "34653/34665 [============================>.] - ETA: 0s - loss: 13584.5352 - mse: 13584.5352\n",
      "Epoch 97: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13580.0713 - mse: 13580.0713 - val_loss: 58.3785 - val_mse: 58.3785\n",
      "Epoch 98/300\n",
      "34638/34665 [============================>.] - ETA: 0s - loss: 13589.9736 - mse: 13589.9736\n",
      "Epoch 98: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 904us/step - loss: 13579.6729 - mse: 13579.6729 - val_loss: 55.0167 - val_mse: 55.0167\n",
      "Epoch 99/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13596.5830 - mse: 13596.5830\n",
      "Epoch 99: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13580.4775 - mse: 13580.4775 - val_loss: 55.6818 - val_mse: 55.6818\n",
      "Epoch 100/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13595.0098 - mse: 13595.0098\n",
      "Epoch 100: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13579.6699 - mse: 13579.6699 - val_loss: 62.7832 - val_mse: 62.7832\n",
      "Epoch 101/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13594.1318 - mse: 13594.1318\n",
      "Epoch 101: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 31s 904us/step - loss: 13581.5684 - mse: 13581.5684 - val_loss: 55.9610 - val_mse: 55.9610\n",
      "Epoch 102/300\n",
      "34635/34665 [============================>.] - ETA: 0s - loss: 13592.7930 - mse: 13592.7930\n",
      "Epoch 102: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 32s 910us/step - loss: 13581.3652 - mse: 13581.3652 - val_loss: 57.9907 - val_mse: 57.9907\n",
      "Epoch 103/300\n",
      "34616/34665 [============================>.] - ETA: 0s - loss: 13597.7549 - mse: 13597.7549\n",
      "Epoch 103: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 32s 910us/step - loss: 13580.0059 - mse: 13580.0059 - val_loss: 55.6344 - val_mse: 55.6344\n",
      "Epoch 104/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13594.2520 - mse: 13594.2520\n",
      "Epoch 104: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 33s 941us/step - loss: 13580.6963 - mse: 13580.6963 - val_loss: 60.9765 - val_mse: 60.9765\n",
      "Epoch 105/300\n",
      "34636/34665 [============================>.] - ETA: 0s - loss: 13590.8105 - mse: 13590.8105\n",
      "Epoch 105: val_loss did not improve from 54.16714\n",
      "34665/34665 [==============================] - 34s 969us/step - loss: 13579.7783 - mse: 13579.7783 - val_loss: 55.7433 - val_mse: 55.7433\n",
      "Epoch 106/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13580.9902 - mse: 13580.9902\n",
      "Epoch 106: val_loss improved from 54.16714 to 53.95003, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 34s 973us/step - loss: 13580.4150 - mse: 13580.4150 - val_loss: 53.9500 - val_mse: 53.9500\n",
      "Epoch 107/300\n",
      "34621/34665 [============================>.] - ETA: 0s - loss: 13596.2520 - mse: 13596.2520\n",
      "Epoch 107: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 33s 953us/step - loss: 13579.5850 - mse: 13579.5850 - val_loss: 59.0572 - val_mse: 59.0572\n",
      "Epoch 108/300\n",
      "34614/34665 [============================>.] - ETA: 0s - loss: 13599.2773 - mse: 13599.2773\n",
      "Epoch 108: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 33s 959us/step - loss: 13580.2041 - mse: 13580.2041 - val_loss: 62.0186 - val_mse: 62.0186\n",
      "Epoch 109/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13592.4092 - mse: 13592.4092\n",
      "Epoch 109: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 35s 1ms/step - loss: 13577.1650 - mse: 13577.1650 - val_loss: 57.3497 - val_mse: 57.3497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300\n",
      "34644/34665 [============================>.] - ETA: 0s - loss: 13588.7773 - mse: 13588.7773\n",
      "Epoch 110: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 34s 992us/step - loss: 13581.1299 - mse: 13581.1299 - val_loss: 56.8352 - val_mse: 56.8352\n",
      "Epoch 111/300\n",
      "34628/34665 [============================>.] - ETA: 0s - loss: 13594.7539 - mse: 13594.7539\n",
      "Epoch 111: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 35s 999us/step - loss: 13580.5781 - mse: 13580.5781 - val_loss: 65.4792 - val_mse: 65.4792\n",
      "Epoch 112/300\n",
      "34620/34665 [============================>.] - ETA: 0s - loss: 13594.8779 - mse: 13594.8779\n",
      "Epoch 112: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 35s 1ms/step - loss: 13577.6289 - mse: 13577.6289 - val_loss: 57.3643 - val_mse: 57.3643\n",
      "Epoch 113/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13582.4639 - mse: 13582.4639\n",
      "Epoch 113: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 930us/step - loss: 13579.9629 - mse: 13579.9629 - val_loss: 57.7262 - val_mse: 57.7262\n",
      "Epoch 114/300\n",
      "34621/34665 [============================>.] - ETA: 0s - loss: 13597.4863 - mse: 13597.4863\n",
      "Epoch 114: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 915us/step - loss: 13580.7021 - mse: 13580.7021 - val_loss: 58.8478 - val_mse: 58.8478\n",
      "Epoch 115/300\n",
      "34624/34665 [============================>.] - ETA: 0s - loss: 13595.6641 - mse: 13595.6641\n",
      "Epoch 115: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13580.1084 - mse: 13580.1084 - val_loss: 56.6215 - val_mse: 56.6215\n",
      "Epoch 116/300\n",
      "34635/34665 [============================>.] - ETA: 0s - loss: 13592.6445 - mse: 13592.6445\n",
      "Epoch 116: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13581.2373 - mse: 13581.2373 - val_loss: 57.0893 - val_mse: 57.0893\n",
      "Epoch 117/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13595.2129 - mse: 13595.2129\n",
      "Epoch 117: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 903us/step - loss: 13580.2891 - mse: 13580.2891 - val_loss: 60.4700 - val_mse: 60.4700\n",
      "Epoch 118/300\n",
      "34630/34665 [============================>.] - ETA: 0s - loss: 13592.7949 - mse: 13592.7949\n",
      "Epoch 118: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13579.4434 - mse: 13579.4434 - val_loss: 57.9055 - val_mse: 57.9055\n",
      "Epoch 119/300\n",
      "34645/34665 [============================>.] - ETA: 0s - loss: 13587.3516 - mse: 13587.3516\n",
      "Epoch 119: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13580.0459 - mse: 13580.0459 - val_loss: 62.5070 - val_mse: 62.5070\n",
      "Epoch 120/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13582.6230 - mse: 13582.6230\n",
      "Epoch 120: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 900us/step - loss: 13580.0986 - mse: 13580.0986 - val_loss: 55.8244 - val_mse: 55.8244\n",
      "Epoch 121/300\n",
      "34615/34665 [============================>.] - ETA: 0s - loss: 13600.0381 - mse: 13600.0381\n",
      "Epoch 121: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 899us/step - loss: 13580.8037 - mse: 13580.8037 - val_loss: 59.5917 - val_mse: 59.5917\n",
      "Epoch 122/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13585.3389 - mse: 13585.3389\n",
      "Epoch 122: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.4795 - mse: 13580.4795 - val_loss: 56.7372 - val_mse: 56.7372\n",
      "Epoch 123/300\n",
      "34640/34665 [============================>.] - ETA: 0s - loss: 13589.7070 - mse: 13589.7070\n",
      "Epoch 123: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 907us/step - loss: 13580.1836 - mse: 13580.1836 - val_loss: 57.3841 - val_mse: 57.3841\n",
      "Epoch 124/300\n",
      "34628/34665 [============================>.] - ETA: 0s - loss: 13592.7070 - mse: 13592.7070\n",
      "Epoch 124: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13578.5078 - mse: 13578.5078 - val_loss: 58.0901 - val_mse: 58.0901\n",
      "Epoch 125/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13585.2236 - mse: 13585.2236\n",
      "Epoch 125: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13579.1221 - mse: 13579.1221 - val_loss: 59.3122 - val_mse: 59.3122\n",
      "Epoch 126/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13594.5879 - mse: 13594.5879\n",
      "Epoch 126: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 928us/step - loss: 13579.1738 - mse: 13579.1738 - val_loss: 62.1016 - val_mse: 62.1016\n",
      "Epoch 127/300\n",
      "34627/34665 [============================>.] - ETA: 0s - loss: 13593.7646 - mse: 13593.7646\n",
      "Epoch 127: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 918us/step - loss: 13579.3486 - mse: 13579.3486 - val_loss: 59.5075 - val_mse: 59.5075\n",
      "Epoch 128/300\n",
      "34653/34665 [============================>.] - ETA: 0s - loss: 13583.3467 - mse: 13583.3467\n",
      "Epoch 128: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 923us/step - loss: 13578.8770 - mse: 13578.8770 - val_loss: 57.7100 - val_mse: 57.7100\n",
      "Epoch 129/300\n",
      "34626/34665 [============================>.] - ETA: 0s - loss: 13596.9170 - mse: 13596.9170\n",
      "Epoch 129: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 33s 938us/step - loss: 13582.0176 - mse: 13582.0176 - val_loss: 60.4054 - val_mse: 60.4054\n",
      "Epoch 130/300\n",
      "34629/34665 [============================>.] - ETA: 0s - loss: 13591.9336 - mse: 13591.9336\n",
      "Epoch 130: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13578.1572 - mse: 13578.1572 - val_loss: 56.2635 - val_mse: 56.2635\n",
      "Epoch 131/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13586.4033 - mse: 13586.4033\n",
      "Epoch 131: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13581.1680 - mse: 13581.1680 - val_loss: 58.3877 - val_mse: 58.3877\n",
      "Epoch 132/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13581.0557 - mse: 13581.0557\n",
      "Epoch 132: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13581.0557 - mse: 13581.0557 - val_loss: 62.0295 - val_mse: 62.0295\n",
      "Epoch 133/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13586.3887 - mse: 13586.3887\n",
      "Epoch 133: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 906us/step - loss: 13582.3721 - mse: 13582.3721 - val_loss: 57.7891 - val_mse: 57.7891\n",
      "Epoch 134/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13594.5605 - mse: 13594.5605\n",
      "Epoch 134: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 905us/step - loss: 13576.7979 - mse: 13576.7979 - val_loss: 57.5004 - val_mse: 57.5004\n",
      "Epoch 135/300\n",
      "34650/34665 [============================>.] - ETA: 0s - loss: 13586.5869 - mse: 13586.5869\n",
      "Epoch 135: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13581.4561 - mse: 13581.4561 - val_loss: 66.1521 - val_mse: 66.1521\n",
      "Epoch 136/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13599.9404 - mse: 13599.9404\n",
      "Epoch 136: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13579.8525 - mse: 13579.8525 - val_loss: 55.2696 - val_mse: 55.2696\n",
      "Epoch 137/300\n",
      "34638/34665 [============================>.] - ETA: 0s - loss: 13590.6426 - mse: 13590.6426\n",
      "Epoch 137: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 900us/step - loss: 13580.3223 - mse: 13580.3223 - val_loss: 60.0940 - val_mse: 60.0940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13591.2129 - mse: 13591.2129\n",
      "Epoch 138: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13579.1367 - mse: 13579.1367 - val_loss: 57.1493 - val_mse: 57.1493\n",
      "Epoch 139/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13582.8096 - mse: 13582.8096\n",
      "Epoch 139: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 891us/step - loss: 13578.8633 - mse: 13578.8633 - val_loss: 61.6878 - val_mse: 61.6878\n",
      "Epoch 140/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13597.8916 - mse: 13597.8916\n",
      "Epoch 140: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 913us/step - loss: 13577.8701 - mse: 13577.8701 - val_loss: 56.6331 - val_mse: 56.6331\n",
      "Epoch 141/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13585.0879 - mse: 13585.0879\n",
      "Epoch 141: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13580.2207 - mse: 13580.2207 - val_loss: 55.6584 - val_mse: 55.6584\n",
      "Epoch 142/300\n",
      "34647/34665 [============================>.] - ETA: 0s - loss: 13586.2422 - mse: 13586.2422\n",
      "Epoch 142: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13579.4170 - mse: 13579.4170 - val_loss: 58.5109 - val_mse: 58.5109\n",
      "Epoch 143/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13581.7686 - mse: 13581.7686\n",
      "Epoch 143: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 904us/step - loss: 13581.1943 - mse: 13581.1943 - val_loss: 59.6129 - val_mse: 59.6129\n",
      "Epoch 144/300\n",
      "34618/34665 [============================>.] - ETA: 0s - loss: 13597.7256 - mse: 13597.7256\n",
      "Epoch 144: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13579.9199 - mse: 13579.9199 - val_loss: 55.7389 - val_mse: 55.7389\n",
      "Epoch 145/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13578.8984 - mse: 13578.8984\n",
      "Epoch 145: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13578.8984 - mse: 13578.8984 - val_loss: 57.7055 - val_mse: 57.7055\n",
      "Epoch 146/300\n",
      "34628/34665 [============================>.] - ETA: 0s - loss: 13590.2754 - mse: 13590.2754\n",
      "Epoch 146: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 901us/step - loss: 13577.2139 - mse: 13577.2139 - val_loss: 58.2948 - val_mse: 58.2948\n",
      "Epoch 147/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13584.1104 - mse: 13584.1104\n",
      "Epoch 147: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13580.4092 - mse: 13580.4092 - val_loss: 56.9310 - val_mse: 56.9310\n",
      "Epoch 148/300\n",
      "34630/34665 [============================>.] - ETA: 0s - loss: 13594.7852 - mse: 13594.7852\n",
      "Epoch 148: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 889us/step - loss: 13581.7949 - mse: 13581.7949 - val_loss: 60.1479 - val_mse: 60.1479\n",
      "Epoch 149/300\n",
      "34641/34665 [============================>.] - ETA: 0s - loss: 13587.5361 - mse: 13587.5361\n",
      "Epoch 149: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13578.4238 - mse: 13578.4238 - val_loss: 57.4997 - val_mse: 57.4997\n",
      "Epoch 150/300\n",
      "34631/34665 [============================>.] - ETA: 0s - loss: 13591.5322 - mse: 13591.5322\n",
      "Epoch 150: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 909us/step - loss: 13578.9697 - mse: 13578.9697 - val_loss: 60.3062 - val_mse: 60.3062\n",
      "Epoch 151/300\n",
      "34660/34665 [============================>.] - ETA: 0s - loss: 13582.1377 - mse: 13582.1377\n",
      "Epoch 151: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 891us/step - loss: 13580.3945 - mse: 13580.3945 - val_loss: 62.6880 - val_mse: 62.6880\n",
      "Epoch 152/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13598.3076 - mse: 13598.3076\n",
      "Epoch 152: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13578.1748 - mse: 13578.1748 - val_loss: 61.0934 - val_mse: 61.0934\n",
      "Epoch 153/300\n",
      "34653/34665 [============================>.] - ETA: 0s - loss: 13584.0156 - mse: 13584.0156\n",
      "Epoch 153: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 903us/step - loss: 13579.5771 - mse: 13579.5771 - val_loss: 60.0189 - val_mse: 60.0189\n",
      "Epoch 154/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13577.4463 - mse: 13577.4463\n",
      "Epoch 154: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.4463 - mse: 13577.4463 - val_loss: 56.3744 - val_mse: 56.3744\n",
      "Epoch 155/300\n",
      "34643/34665 [============================>.] - ETA: 0s - loss: 13586.8330 - mse: 13586.8330\n",
      "Epoch 155: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13578.5391 - mse: 13578.5391 - val_loss: 57.9519 - val_mse: 57.9519\n",
      "Epoch 156/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13599.5957 - mse: 13599.5957\n",
      "Epoch 156: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 891us/step - loss: 13578.6377 - mse: 13578.6377 - val_loss: 59.4162 - val_mse: 59.4162\n",
      "Epoch 157/300\n",
      "34641/34665 [============================>.] - ETA: 0s - loss: 13586.0381 - mse: 13586.0381\n",
      "Epoch 157: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 910us/step - loss: 13578.6963 - mse: 13578.6963 - val_loss: 61.7120 - val_mse: 61.7120\n",
      "Epoch 158/300\n",
      "34606/34665 [============================>.] - ETA: 0s - loss: 13596.1084 - mse: 13596.1084\n",
      "Epoch 158: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.4600 - mse: 13577.4600 - val_loss: 59.5835 - val_mse: 59.5835\n",
      "Epoch 159/300\n",
      "34636/34665 [============================>.] - ETA: 0s - loss: 13589.7676 - mse: 13589.7676\n",
      "Epoch 159: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13578.7920 - mse: 13578.7920 - val_loss: 59.9962 - val_mse: 59.9962\n",
      "Epoch 160/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13584.0117 - mse: 13584.0117\n",
      "Epoch 160: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13577.6221 - mse: 13577.6221 - val_loss: 57.6962 - val_mse: 57.6962\n",
      "Epoch 161/300\n",
      "34628/34665 [============================>.] - ETA: 0s - loss: 13592.4902 - mse: 13592.4902\n",
      "Epoch 161: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13578.4824 - mse: 13578.4824 - val_loss: 65.0240 - val_mse: 65.0240\n",
      "Epoch 162/300\n",
      "34646/34665 [============================>.] - ETA: 0s - loss: 13585.9287 - mse: 13585.9287\n",
      "Epoch 162: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 905us/step - loss: 13578.7754 - mse: 13578.7754 - val_loss: 65.1802 - val_mse: 65.1802\n",
      "Epoch 163/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13579.6611 - mse: 13579.6611\n",
      "Epoch 163: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.1768 - mse: 13577.1768 - val_loss: 59.6014 - val_mse: 59.6014\n",
      "Epoch 164/300\n",
      "34638/34665 [============================>.] - ETA: 0s - loss: 13589.2979 - mse: 13589.2979\n",
      "Epoch 164: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13579.0225 - mse: 13579.0225 - val_loss: 59.4992 - val_mse: 59.4992\n",
      "Epoch 165/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13583.7812 - mse: 13583.7812\n",
      "Epoch 165: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.0918 - mse: 13580.0918 - val_loss: 57.1238 - val_mse: 57.1238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13591.7207 - mse: 13591.7207\n",
      "Epoch 166: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.5928 - mse: 13576.5928 - val_loss: 56.0911 - val_mse: 56.0911\n",
      "Epoch 167/300\n",
      "34641/34665 [============================>.] - ETA: 0s - loss: 13588.6416 - mse: 13588.6416\n",
      "Epoch 167: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 905us/step - loss: 13579.5371 - mse: 13579.5371 - val_loss: 62.0115 - val_mse: 62.0115\n",
      "Epoch 168/300\n",
      "34624/34665 [============================>.] - ETA: 0s - loss: 13593.3193 - mse: 13593.3193\n",
      "Epoch 168: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.6689 - mse: 13577.6689 - val_loss: 56.7862 - val_mse: 56.7862\n",
      "Epoch 169/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13589.1035 - mse: 13589.1035\n",
      "Epoch 169: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13579.1621 - mse: 13579.1621 - val_loss: 56.6341 - val_mse: 56.6341\n",
      "Epoch 170/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13583.5781 - mse: 13583.5781\n",
      "Epoch 170: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 906us/step - loss: 13578.7383 - mse: 13578.7383 - val_loss: 57.7582 - val_mse: 57.7582\n",
      "Epoch 171/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13576.9033 - mse: 13576.9033\n",
      "Epoch 171: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.7207 - mse: 13576.7207 - val_loss: 55.9849 - val_mse: 55.9849\n",
      "Epoch 172/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13586.9512 - mse: 13586.9512\n",
      "Epoch 172: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 908us/step - loss: 13577.0332 - mse: 13577.0332 - val_loss: 57.1266 - val_mse: 57.1266\n",
      "Epoch 173/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13594.7402 - mse: 13594.7402\n",
      "Epoch 173: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 904us/step - loss: 13578.5352 - mse: 13578.5352 - val_loss: 57.5119 - val_mse: 57.5119\n",
      "Epoch 174/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13576.9648 - mse: 13576.9648\n",
      "Epoch 174: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.3936 - mse: 13576.3936 - val_loss: 69.0510 - val_mse: 69.0510\n",
      "Epoch 175/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13577.4375 - mse: 13577.4375\n",
      "Epoch 175: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 891us/step - loss: 13577.2549 - mse: 13577.2549 - val_loss: 58.7113 - val_mse: 58.7113\n",
      "Epoch 176/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13582.6162 - mse: 13582.6162\n",
      "Epoch 176: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13577.3691 - mse: 13577.3691 - val_loss: 55.2051 - val_mse: 55.2051\n",
      "Epoch 177/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13599.0527 - mse: 13599.0527\n",
      "Epoch 177: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.0244 - mse: 13577.0244 - val_loss: 56.8829 - val_mse: 56.8829\n",
      "Epoch 178/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13596.3408 - mse: 13596.3408\n",
      "Epoch 178: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13578.1260 - mse: 13578.1260 - val_loss: 60.7812 - val_mse: 60.7812\n",
      "Epoch 179/300\n",
      "34646/34665 [============================>.] - ETA: 0s - loss: 13584.0244 - mse: 13584.0244\n",
      "Epoch 179: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 32s 909us/step - loss: 13577.9141 - mse: 13577.9141 - val_loss: 55.6130 - val_mse: 55.6130\n",
      "Epoch 180/300\n",
      "34633/34665 [============================>.] - ETA: 0s - loss: 13590.7510 - mse: 13590.7510\n",
      "Epoch 180: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13578.4521 - mse: 13578.4521 - val_loss: 58.3870 - val_mse: 58.3870\n",
      "Epoch 181/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13576.6689 - mse: 13576.6689\n",
      "Epoch 181: val_loss did not improve from 53.95003\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.6689 - mse: 13576.6689 - val_loss: 58.3379 - val_mse: 58.3379\n",
      "Epoch 182/300\n",
      "34631/34665 [============================>.] - ETA: 0s - loss: 13591.1016 - mse: 13591.1016\n",
      "Epoch 182: val_loss improved from 53.95003 to 53.75972, saving model to /Users/chikakoolsen/opt/python/thesis/code/tdcs_thesis/models/best_model_nn_neg_both_nonzero2.h5\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13578.3018 - mse: 13578.3018 - val_loss: 53.7597 - val_mse: 53.7597\n",
      "Epoch 183/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13595.4629 - mse: 13595.4629\n",
      "Epoch 183: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.7646 - mse: 13577.7646 - val_loss: 54.4282 - val_mse: 54.4282\n",
      "Epoch 184/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13598.4062 - mse: 13598.4062\n",
      "Epoch 184: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.3896 - mse: 13576.3896 - val_loss: 60.2384 - val_mse: 60.2384\n",
      "Epoch 185/300\n",
      "34650/34665 [============================>.] - ETA: 0s - loss: 13584.5791 - mse: 13584.5791\n",
      "Epoch 185: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13578.9629 - mse: 13578.9629 - val_loss: 59.7068 - val_mse: 59.7068\n",
      "Epoch 186/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13595.9053 - mse: 13595.9053\n",
      "Epoch 186: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13575.5977 - mse: 13575.5977 - val_loss: 56.5804 - val_mse: 56.5804\n",
      "Epoch 187/300\n",
      "34610/34665 [============================>.] - ETA: 0s - loss: 13592.2832 - mse: 13592.2832\n",
      "Epoch 187: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13575.1230 - mse: 13575.1230 - val_loss: 57.4867 - val_mse: 57.4867\n",
      "Epoch 188/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13574.5264 - mse: 13574.5264\n",
      "Epoch 188: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13573.9521 - mse: 13573.9521 - val_loss: 58.3805 - val_mse: 58.3805\n",
      "Epoch 189/300\n",
      "34643/34665 [============================>.] - ETA: 0s - loss: 13585.0352 - mse: 13585.0352\n",
      "Epoch 189: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13576.7490 - mse: 13576.7490 - val_loss: 56.1568 - val_mse: 56.1568\n",
      "Epoch 190/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13578.0654 - mse: 13578.0654\n",
      "Epoch 190: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13575.5430 - mse: 13575.5430 - val_loss: 56.5589 - val_mse: 56.5589\n",
      "Epoch 191/300\n",
      "34661/34665 [============================>.] - ETA: 0s - loss: 13577.7090 - mse: 13577.7090\n",
      "Epoch 191: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.6475 - mse: 13576.6475 - val_loss: 63.0682 - val_mse: 63.0682\n",
      "Epoch 192/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13592.3037 - mse: 13592.3037\n",
      "Epoch 192: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13576.8916 - mse: 13576.8916 - val_loss: 59.0335 - val_mse: 59.0335\n",
      "Epoch 193/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13597.0078 - mse: 13597.0078\n",
      "Epoch 193: val_loss did not improve from 53.75972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.9453 - mse: 13576.9453 - val_loss: 59.4115 - val_mse: 59.4115\n",
      "Epoch 194/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13591.7754 - mse: 13591.7754\n",
      "Epoch 194: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13575.2207 - mse: 13575.2207 - val_loss: 58.2065 - val_mse: 58.2065\n",
      "Epoch 195/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 12540.4766 - mse: 12540.4766\n",
      "Epoch 195: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13575.9727 - mse: 13575.9727 - val_loss: 62.1673 - val_mse: 62.1673\n",
      "Epoch 196/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13599.4795 - mse: 13599.4795\n",
      "Epoch 196: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.5811 - mse: 13577.5811 - val_loss: 54.6978 - val_mse: 54.6978\n",
      "Epoch 197/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13591.0400 - mse: 13591.0400\n",
      "Epoch 197: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13576.8701 - mse: 13576.8701 - val_loss: 57.8585 - val_mse: 57.8585\n",
      "Epoch 198/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13580.0908 - mse: 13580.0908\n",
      "Epoch 198: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13577.6553 - mse: 13577.6553 - val_loss: 59.4603 - val_mse: 59.4603\n",
      "Epoch 199/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13578.4385 - mse: 13578.4385\n",
      "Epoch 199: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13575.9453 - mse: 13575.9453 - val_loss: 61.0204 - val_mse: 61.0204\n",
      "Epoch 200/300\n",
      "34640/34665 [============================>.] - ETA: 0s - loss: 13588.2617 - mse: 13588.2617\n",
      "Epoch 200: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13578.9521 - mse: 13578.9521 - val_loss: 57.3110 - val_mse: 57.3110\n",
      "Epoch 201/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13595.6455 - mse: 13595.6455\n",
      "Epoch 201: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13575.4531 - mse: 13575.4531 - val_loss: 56.2695 - val_mse: 56.2695\n",
      "Epoch 202/300\n",
      "34657/34665 [============================>.] - ETA: 0s - loss: 13578.8340 - mse: 13578.8340\n",
      "Epoch 202: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13575.9297 - mse: 13575.9297 - val_loss: 58.6476 - val_mse: 58.6476\n",
      "Epoch 203/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13577.3184 - mse: 13577.3184\n",
      "Epoch 203: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.3184 - mse: 13577.3184 - val_loss: 59.5933 - val_mse: 59.5933\n",
      "Epoch 204/300\n",
      "34615/34665 [============================>.] - ETA: 0s - loss: 13595.2441 - mse: 13595.2441\n",
      "Epoch 204: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.2627 - mse: 13576.2627 - val_loss: 58.1905 - val_mse: 58.1905\n",
      "Epoch 205/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13578.8662 - mse: 13578.8662\n",
      "Epoch 205: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13578.6836 - mse: 13578.6836 - val_loss: 59.2670 - val_mse: 59.2670\n",
      "Epoch 206/300\n",
      "34636/34665 [============================>.] - ETA: 0s - loss: 13587.8252 - mse: 13587.8252\n",
      "Epoch 206: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13576.7539 - mse: 13576.7539 - val_loss: 65.8426 - val_mse: 65.8426\n",
      "Epoch 207/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13583.7539 - mse: 13583.7539\n",
      "Epoch 207: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.3350 - mse: 13577.3350 - val_loss: 57.4620 - val_mse: 57.4620\n",
      "Epoch 208/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13580.9434 - mse: 13580.9434\n",
      "Epoch 208: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13577.2510 - mse: 13577.2510 - val_loss: 62.7633 - val_mse: 62.7633\n",
      "Epoch 209/300\n",
      "34650/34665 [============================>.] - ETA: 0s - loss: 13576.6816 - mse: 13576.6816\n",
      "Epoch 209: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13575.5059 - mse: 13575.5059 - val_loss: 56.1268 - val_mse: 56.1268\n",
      "Epoch 210/300\n",
      "34611/34665 [============================>.] - ETA: 0s - loss: 13597.2725 - mse: 13597.2725\n",
      "Epoch 210: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.2578 - mse: 13577.2578 - val_loss: 65.8698 - val_mse: 65.8698\n",
      "Epoch 211/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13576.6230 - mse: 13576.6230\n",
      "Epoch 211: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13576.0479 - mse: 13576.0479 - val_loss: 56.8604 - val_mse: 56.8604\n",
      "Epoch 212/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13596.8789 - mse: 13596.8789\n",
      "Epoch 212: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.3789 - mse: 13576.3789 - val_loss: 55.6834 - val_mse: 55.6834\n",
      "Epoch 213/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13579.6445 - mse: 13579.6445\n",
      "Epoch 213: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13574.4238 - mse: 13574.4238 - val_loss: 58.4449 - val_mse: 58.4449\n",
      "Epoch 214/300\n",
      "34635/34665 [============================>.] - ETA: 0s - loss: 13589.5723 - mse: 13589.5723\n",
      "Epoch 214: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13578.0742 - mse: 13578.0742 - val_loss: 58.2790 - val_mse: 58.2790\n",
      "Epoch 215/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13584.9434 - mse: 13584.9434\n",
      "Epoch 215: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13575.1963 - mse: 13575.1963 - val_loss: 59.4921 - val_mse: 59.4921\n",
      "Epoch 216/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13585.1738 - mse: 13585.1738\n",
      "Epoch 216: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13575.2432 - mse: 13575.2432 - val_loss: 56.0957 - val_mse: 56.0957\n",
      "Epoch 217/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13594.4209 - mse: 13594.4209\n",
      "Epoch 217: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13578.3672 - mse: 13578.3672 - val_loss: 59.3451 - val_mse: 59.3451\n",
      "Epoch 218/300\n",
      "34640/34665 [============================>.] - ETA: 0s - loss: 13585.6494 - mse: 13585.6494\n",
      "Epoch 218: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 32s 910us/step - loss: 13576.0889 - mse: 13576.0889 - val_loss: 61.3911 - val_mse: 61.3911\n",
      "Epoch 219/300\n",
      "34619/34665 [============================>.] - ETA: 0s - loss: 13595.6182 - mse: 13595.6182\n",
      "Epoch 219: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13577.9316 - mse: 13577.9316 - val_loss: 57.4150 - val_mse: 57.4150\n",
      "Epoch 220/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13586.4834 - mse: 13586.4834\n",
      "Epoch 220: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13574.7598 - mse: 13574.7598 - val_loss: 59.5712 - val_mse: 59.5712\n",
      "Epoch 221/300\n",
      "34622/34665 [============================>.] - ETA: 0s - loss: 13596.2012 - mse: 13596.2012\n",
      "Epoch 221: val_loss did not improve from 53.75972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34665/34665 [==============================] - 31s 898us/step - loss: 13579.7568 - mse: 13579.7568 - val_loss: 56.4185 - val_mse: 56.4185\n",
      "Epoch 222/300\n",
      "34659/34665 [============================>.] - ETA: 0s - loss: 13581.1230 - mse: 13581.1230\n",
      "Epoch 222: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13579.2334 - mse: 13579.2334 - val_loss: 54.3425 - val_mse: 54.3425\n",
      "Epoch 223/300\n",
      "34661/34665 [============================>.] - ETA: 0s - loss: 13578.5703 - mse: 13578.5703\n",
      "Epoch 223: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 901us/step - loss: 13577.2197 - mse: 13577.2197 - val_loss: 60.9812 - val_mse: 60.9812\n",
      "Epoch 224/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13576.9746 - mse: 13576.9746\n",
      "Epoch 224: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.7910 - mse: 13576.7910 - val_loss: 65.4646 - val_mse: 65.4646\n",
      "Epoch 225/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13584.8867 - mse: 13584.8867\n",
      "Epoch 225: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13578.4766 - mse: 13578.4766 - val_loss: 55.6107 - val_mse: 55.6107\n",
      "Epoch 226/300\n",
      "34663/34665 [============================>.] - ETA: 0s - loss: 13577.0469 - mse: 13577.0469\n",
      "Epoch 226: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13576.4736 - mse: 13576.4736 - val_loss: 57.7830 - val_mse: 57.7830\n",
      "Epoch 227/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13595.9570 - mse: 13595.9570\n",
      "Epoch 227: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 902us/step - loss: 13575.5938 - mse: 13575.5938 - val_loss: 59.9207 - val_mse: 59.9207\n",
      "Epoch 228/300\n",
      "34637/34665 [============================>.] - ETA: 0s - loss: 13586.2285 - mse: 13586.2285\n",
      "Epoch 228: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13575.7266 - mse: 13575.7266 - val_loss: 58.4095 - val_mse: 58.4095\n",
      "Epoch 229/300\n",
      "34662/34665 [============================>.] - ETA: 0s - loss: 13578.4238 - mse: 13578.4238\n",
      "Epoch 229: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13577.4609 - mse: 13577.4609 - val_loss: 61.5894 - val_mse: 61.5894\n",
      "Epoch 230/300\n",
      "34660/34665 [============================>.] - ETA: 0s - loss: 13577.5225 - mse: 13577.5225\n",
      "Epoch 230: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 899us/step - loss: 13575.7764 - mse: 13575.7764 - val_loss: 58.1835 - val_mse: 58.1835\n",
      "Epoch 231/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13577.7656 - mse: 13577.7656\n",
      "Epoch 231: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.5830 - mse: 13577.5830 - val_loss: 55.1665 - val_mse: 55.1665\n",
      "Epoch 232/300\n",
      "34640/34665 [============================>.] - ETA: 0s - loss: 13586.9658 - mse: 13586.9658\n",
      "Epoch 232: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13577.4561 - mse: 13577.4561 - val_loss: 57.0649 - val_mse: 57.0649\n",
      "Epoch 233/300\n",
      "34650/34665 [============================>.] - ETA: 0s - loss: 13580.9492 - mse: 13580.9492\n",
      "Epoch 233: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13575.3018 - mse: 13575.3018 - val_loss: 57.9853 - val_mse: 57.9853\n",
      "Epoch 234/300\n",
      "34626/34665 [============================>.] - ETA: 0s - loss: 13591.4531 - mse: 13591.4531\n",
      "Epoch 234: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13576.4668 - mse: 13576.4668 - val_loss: 84.2719 - val_mse: 84.2719\n",
      "Epoch 235/300\n",
      "34616/34665 [============================>.] - ETA: 0s - loss: 13595.3330 - mse: 13595.3330\n",
      "Epoch 235: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.7227 - mse: 13576.7227 - val_loss: 59.1175 - val_mse: 59.1175\n",
      "Epoch 236/300\n",
      "34633/34665 [============================>.] - ETA: 0s - loss: 13588.7119 - mse: 13588.7119\n",
      "Epoch 236: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.4727 - mse: 13576.4727 - val_loss: 61.2843 - val_mse: 61.2843\n",
      "Epoch 237/300\n",
      "34651/34665 [============================>.] - ETA: 0s - loss: 13580.4619 - mse: 13580.4619\n",
      "Epoch 237: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 899us/step - loss: 13575.2100 - mse: 13575.2100 - val_loss: 59.7615 - val_mse: 59.7615\n",
      "Epoch 238/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13576.2998 - mse: 13576.2998\n",
      "Epoch 238: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.1172 - mse: 13576.1172 - val_loss: 61.1712 - val_mse: 61.1712\n",
      "Epoch 239/300\n",
      "34614/34665 [============================>.] - ETA: 0s - loss: 13596.4150 - mse: 13596.4150\n",
      "Epoch 239: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.8389 - mse: 13576.8389 - val_loss: 59.6730 - val_mse: 59.6730\n",
      "Epoch 240/300\n",
      "34614/34665 [============================>.] - ETA: 0s - loss: 13595.6631 - mse: 13595.6631\n",
      "Epoch 240: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13575.9600 - mse: 13575.9600 - val_loss: 56.9643 - val_mse: 56.9643\n",
      "Epoch 241/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 12548.8652 - mse: 12548.8652\n",
      "Epoch 241: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13573.4131 - mse: 13573.4131 - val_loss: 68.3440 - val_mse: 68.3440\n",
      "Epoch 242/300\n",
      "34647/34665 [============================>.] - ETA: 0s - loss: 13583.0801 - mse: 13583.0801\n",
      "Epoch 242: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.9561 - mse: 13576.9561 - val_loss: 61.1979 - val_mse: 61.1979\n",
      "Epoch 243/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13597.8613 - mse: 13597.8613\n",
      "Epoch 243: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.0127 - mse: 13576.0127 - val_loss: 72.2005 - val_mse: 72.2005\n",
      "Epoch 244/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13594.8457 - mse: 13594.8457\n",
      "Epoch 244: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13573.6680 - mse: 13573.6680 - val_loss: 56.2933 - val_mse: 56.2933\n",
      "Epoch 245/300\n",
      "34661/34665 [============================>.] - ETA: 0s - loss: 13576.0986 - mse: 13576.0986\n",
      "Epoch 245: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13574.7607 - mse: 13574.7607 - val_loss: 65.4268 - val_mse: 65.4268\n",
      "Epoch 246/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13594.8096 - mse: 13594.8096\n",
      "Epoch 246: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13576.6641 - mse: 13576.6641 - val_loss: 63.9221 - val_mse: 63.9221\n",
      "Epoch 247/300\n",
      "34652/34665 [============================>.] - ETA: 0s - loss: 13580.5088 - mse: 13580.5088\n",
      "Epoch 247: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13575.8311 - mse: 13575.8311 - val_loss: 79.0836 - val_mse: 79.0836\n",
      "Epoch 248/300\n",
      "34616/34665 [============================>.] - ETA: 0s - loss: 13597.3242 - mse: 13597.3242\n",
      "Epoch 248: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13579.3223 - mse: 13579.3223 - val_loss: 63.0119 - val_mse: 63.0119\n",
      "Epoch 249/300\n",
      "34608/34665 [============================>.] - ETA: 0s - loss: 13492.1182 - mse: 13492.1182\n",
      "Epoch 249: val_loss did not improve from 53.75972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34665/34665 [==============================] - 31s 898us/step - loss: 13574.6113 - mse: 13574.6113 - val_loss: 67.3373 - val_mse: 67.3373\n",
      "Epoch 250/300\n",
      "34640/34665 [============================>.] - ETA: 0s - loss: 13585.9287 - mse: 13585.9287\n",
      "Epoch 250: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.3760 - mse: 13576.3760 - val_loss: 58.0030 - val_mse: 58.0030\n",
      "Epoch 251/300\n",
      "34620/34665 [============================>.] - ETA: 0s - loss: 13592.4346 - mse: 13592.4346\n",
      "Epoch 251: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13575.9170 - mse: 13575.9170 - val_loss: 59.0430 - val_mse: 59.0430\n",
      "Epoch 252/300\n",
      "34631/34665 [============================>.] - ETA: 0s - loss: 13588.3975 - mse: 13588.3975\n",
      "Epoch 252: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.0195 - mse: 13576.0195 - val_loss: 58.0864 - val_mse: 58.0864\n",
      "Epoch 253/300\n",
      "34628/34665 [============================>.] - ETA: 0s - loss: 13591.7295 - mse: 13591.7295\n",
      "Epoch 253: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.4893 - mse: 13577.4893 - val_loss: 66.1622 - val_mse: 66.1622\n",
      "Epoch 254/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13588.7754 - mse: 13588.7754\n",
      "Epoch 254: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.1045 - mse: 13576.1045 - val_loss: 57.7183 - val_mse: 57.7183\n",
      "Epoch 255/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13594.0010 - mse: 13594.0010\n",
      "Epoch 255: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.1895 - mse: 13576.1895 - val_loss: 66.3275 - val_mse: 66.3275\n",
      "Epoch 256/300\n",
      "34637/34665 [============================>.] - ETA: 0s - loss: 13586.8359 - mse: 13586.8359\n",
      "Epoch 256: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 32s 914us/step - loss: 13576.1396 - mse: 13576.1396 - val_loss: 58.7887 - val_mse: 58.7887\n",
      "Epoch 257/300\n",
      "34648/34665 [============================>.] - ETA: 0s - loss: 13583.2227 - mse: 13583.2227\n",
      "Epoch 257: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.9023 - mse: 13576.9023 - val_loss: 58.1698 - val_mse: 58.1698\n",
      "Epoch 258/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13592.9385 - mse: 13592.9385\n",
      "Epoch 258: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13574.5059 - mse: 13574.5059 - val_loss: 59.5440 - val_mse: 59.5440\n",
      "Epoch 259/300\n",
      "34633/34665 [============================>.] - ETA: 0s - loss: 13587.8984 - mse: 13587.8984\n",
      "Epoch 259: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.7080 - mse: 13576.7080 - val_loss: 57.4464 - val_mse: 57.4464\n",
      "Epoch 260/300\n",
      "34611/34665 [============================>.] - ETA: 0s - loss: 13598.2363 - mse: 13598.2363\n",
      "Epoch 260: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 901us/step - loss: 13577.3887 - mse: 13577.3887 - val_loss: 63.4514 - val_mse: 63.4514\n",
      "Epoch 261/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13575.5332 - mse: 13575.5332\n",
      "Epoch 261: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13576.3486 - mse: 13576.3486 - val_loss: 59.8292 - val_mse: 59.8292\n",
      "Epoch 262/300\n",
      "34655/34665 [============================>.] - ETA: 0s - loss: 13580.8730 - mse: 13580.8730\n",
      "Epoch 262: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13577.1748 - mse: 13577.1748 - val_loss: 58.7425 - val_mse: 58.7425\n",
      "Epoch 263/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13593.4307 - mse: 13593.4307\n",
      "Epoch 263: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 900us/step - loss: 13577.3574 - mse: 13577.3574 - val_loss: 62.8980 - val_mse: 62.8980\n",
      "Epoch 264/300\n",
      "34609/34665 [============================>.] - ETA: 0s - loss: 13597.0498 - mse: 13597.0498\n",
      "Epoch 264: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13577.1963 - mse: 13577.1963 - val_loss: 58.0865 - val_mse: 58.0865\n",
      "Epoch 265/300\n",
      "34635/34665 [============================>.] - ETA: 0s - loss: 13587.8506 - mse: 13587.8506\n",
      "Epoch 265: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13576.3818 - mse: 13576.3818 - val_loss: 58.2534 - val_mse: 58.2534\n",
      "Epoch 266/300\n",
      "34631/34665 [============================>.] - ETA: 0s - loss: 13590.8447 - mse: 13590.8447\n",
      "Epoch 266: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 898us/step - loss: 13578.0469 - mse: 13578.0469 - val_loss: 58.6563 - val_mse: 58.6563\n",
      "Epoch 267/300\n",
      "34660/34665 [============================>.] - ETA: 0s - loss: 13575.8252 - mse: 13575.8252\n",
      "Epoch 267: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13578.1953 - mse: 13578.1953 - val_loss: 61.5894 - val_mse: 61.5894\n",
      "Epoch 268/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13583.0449 - mse: 13583.0449\n",
      "Epoch 268: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 891us/step - loss: 13578.9570 - mse: 13578.9570 - val_loss: 63.8279 - val_mse: 63.8279\n",
      "Epoch 269/300\n",
      "34609/34665 [============================>.] - ETA: 0s - loss: 13493.7412 - mse: 13493.7412\n",
      "Epoch 269: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.4121 - mse: 13576.4121 - val_loss: 57.5770 - val_mse: 57.5770\n",
      "Epoch 270/300\n",
      "34625/34665 [============================>.] - ETA: 0s - loss: 13593.1318 - mse: 13593.1318\n",
      "Epoch 270: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 889us/step - loss: 13577.8242 - mse: 13577.8242 - val_loss: 56.7069 - val_mse: 56.7069\n",
      "Epoch 271/300\n",
      "34634/34665 [============================>.] - ETA: 0s - loss: 13588.2090 - mse: 13588.2090\n",
      "Epoch 271: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 904us/step - loss: 13576.3506 - mse: 13576.3506 - val_loss: 57.4150 - val_mse: 57.4150\n",
      "Epoch 272/300\n",
      "34616/34665 [============================>.] - ETA: 0s - loss: 13596.8848 - mse: 13596.8848\n",
      "Epoch 272: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 889us/step - loss: 13578.2578 - mse: 13578.2578 - val_loss: 55.7511 - val_mse: 55.7511\n",
      "Epoch 273/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13590.6758 - mse: 13590.6758\n",
      "Epoch 273: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 895us/step - loss: 13575.0283 - mse: 13575.0283 - val_loss: 56.5470 - val_mse: 56.5470\n",
      "Epoch 274/300\n",
      "34624/34665 [============================>.] - ETA: 0s - loss: 13590.8857 - mse: 13590.8857\n",
      "Epoch 274: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13575.1152 - mse: 13575.1152 - val_loss: 55.6245 - val_mse: 55.6245\n",
      "Epoch 275/300\n",
      "34623/34665 [============================>.] - ETA: 0s - loss: 13592.7881 - mse: 13592.7881\n",
      "Epoch 275: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.6387 - mse: 13576.6387 - val_loss: 57.1373 - val_mse: 57.1373\n",
      "Epoch 276/300\n",
      "34654/34665 [============================>.] - ETA: 0s - loss: 13581.2344 - mse: 13581.2344\n",
      "Epoch 276: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13577.1465 - mse: 13577.1465 - val_loss: 62.0644 - val_mse: 62.0644\n",
      "Epoch 277/300\n",
      "34659/34665 [============================>.] - ETA: 0s - loss: 13579.3193 - mse: 13579.3193\n",
      "Epoch 277: val_loss did not improve from 53.75972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.1826 - mse: 13577.1826 - val_loss: 57.1092 - val_mse: 57.1092\n",
      "Epoch 278/300\n",
      "34632/34665 [============================>.] - ETA: 0s - loss: 13588.6963 - mse: 13588.6963\n",
      "Epoch 278: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13576.1064 - mse: 13576.1064 - val_loss: 55.5960 - val_mse: 55.5960\n",
      "Epoch 279/300\n",
      "34653/34665 [============================>.] - ETA: 0s - loss: 13581.8154 - mse: 13581.8154\n",
      "Epoch 279: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.3340 - mse: 13577.3340 - val_loss: 61.8568 - val_mse: 61.8568\n",
      "Epoch 280/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13540.8750 - mse: 13540.8750\n",
      "Epoch 280: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.4844 - mse: 13576.4844 - val_loss: 57.5083 - val_mse: 57.5083\n",
      "Epoch 281/300\n",
      "34653/34665 [============================>.] - ETA: 0s - loss: 13580.8662 - mse: 13580.8662\n",
      "Epoch 281: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.3857 - mse: 13576.3857 - val_loss: 55.7115 - val_mse: 55.7115\n",
      "Epoch 282/300\n",
      "34612/34665 [============================>.] - ETA: 0s - loss: 13597.6045 - mse: 13597.6045\n",
      "Epoch 282: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 889us/step - loss: 13577.1934 - mse: 13577.1934 - val_loss: 66.8689 - val_mse: 66.8689\n",
      "Epoch 283/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13600.1162 - mse: 13600.1162\n",
      "Epoch 283: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13580.0488 - mse: 13580.0488 - val_loss: 55.5087 - val_mse: 55.5087\n",
      "Epoch 284/300\n",
      "34646/34665 [============================>.] - ETA: 0s - loss: 13582.1416 - mse: 13582.1416\n",
      "Epoch 284: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 890us/step - loss: 13574.9912 - mse: 13574.9912 - val_loss: 64.5620 - val_mse: 64.5620\n",
      "Epoch 285/300\n",
      "34615/34665 [============================>.] - ETA: 0s - loss: 13595.4443 - mse: 13595.4443\n",
      "Epoch 285: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13576.1553 - mse: 13576.1553 - val_loss: 58.7683 - val_mse: 58.7683\n",
      "Epoch 286/300\n",
      "34644/34665 [============================>.] - ETA: 0s - loss: 13582.5049 - mse: 13582.5049\n",
      "Epoch 286: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 32s 927us/step - loss: 13574.7939 - mse: 13574.7939 - val_loss: 56.9923 - val_mse: 56.9923\n",
      "Epoch 287/300\n",
      "34664/34665 [============================>.] - ETA: 0s - loss: 13575.1875 - mse: 13575.1875\n",
      "Epoch 287: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 32s 910us/step - loss: 13575.0049 - mse: 13575.0049 - val_loss: 58.5370 - val_mse: 58.5370\n",
      "Epoch 288/300\n",
      "34609/34665 [============================>.] - ETA: 0s - loss: 13598.2881 - mse: 13598.2881\n",
      "Epoch 288: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 32s 912us/step - loss: 13576.6562 - mse: 13576.6562 - val_loss: 57.8315 - val_mse: 57.8315\n",
      "Epoch 289/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13595.2666 - mse: 13595.2666\n",
      "Epoch 289: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13575.1836 - mse: 13575.1836 - val_loss: 61.3092 - val_mse: 61.3092\n",
      "Epoch 290/300\n",
      "34617/34665 [============================>.] - ETA: 0s - loss: 13596.1445 - mse: 13596.1445\n",
      "Epoch 290: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.8184 - mse: 13577.8184 - val_loss: 57.2509 - val_mse: 57.2509\n",
      "Epoch 291/300\n",
      "34639/34665 [============================>.] - ETA: 0s - loss: 13587.3506 - mse: 13587.3506\n",
      "Epoch 291: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13577.4326 - mse: 13577.4326 - val_loss: 59.6139 - val_mse: 59.6139\n",
      "Epoch 292/300\n",
      "34626/34665 [============================>.] - ETA: 0s - loss: 13593.1650 - mse: 13593.1650\n",
      "Epoch 292: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13578.2656 - mse: 13578.2656 - val_loss: 55.6067 - val_mse: 55.6067\n",
      "Epoch 293/300\n",
      "34645/34665 [============================>.] - ETA: 0s - loss: 13585.4502 - mse: 13585.4502\n",
      "Epoch 293: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.8525 - mse: 13577.8525 - val_loss: 61.1502 - val_mse: 61.1502\n",
      "Epoch 294/300\n",
      "34613/34665 [============================>.] - ETA: 0s - loss: 13595.4863 - mse: 13595.4863\n",
      "Epoch 294: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13575.4600 - mse: 13575.4600 - val_loss: 57.4376 - val_mse: 57.4376\n",
      "Epoch 295/300\n",
      "34665/34665 [==============================] - ETA: 0s - loss: 13575.5156 - mse: 13575.5156\n",
      "Epoch 295: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 897us/step - loss: 13575.5156 - mse: 13575.5156 - val_loss: 57.2888 - val_mse: 57.2888\n",
      "Epoch 296/300\n",
      "34645/34665 [============================>.] - ETA: 0s - loss: 13584.1289 - mse: 13584.1289\n",
      "Epoch 296: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 892us/step - loss: 13576.5674 - mse: 13576.5674 - val_loss: 56.6675 - val_mse: 56.6675\n",
      "Epoch 297/300\n",
      "34607/34665 [============================>.] - ETA: 0s - loss: 13597.5898 - mse: 13597.5898\n",
      "Epoch 297: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13576.9912 - mse: 13576.9912 - val_loss: 56.0447 - val_mse: 56.0447\n",
      "Epoch 298/300\n",
      "34658/34665 [============================>.] - ETA: 0s - loss: 13578.6465 - mse: 13578.6465\n",
      "Epoch 298: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 896us/step - loss: 13576.2012 - mse: 13576.2012 - val_loss: 54.6416 - val_mse: 54.6416\n",
      "Epoch 299/300\n",
      "34624/34665 [============================>.] - ETA: 0s - loss: 13593.4229 - mse: 13593.4229\n",
      "Epoch 299: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 894us/step - loss: 13577.6680 - mse: 13577.6680 - val_loss: 57.6950 - val_mse: 57.6950\n",
      "Epoch 300/300\n",
      "34638/34665 [============================>.] - ETA: 0s - loss: 13587.6035 - mse: 13587.6035\n",
      "Epoch 300: val_loss did not improve from 53.75972\n",
      "34665/34665 [==============================] - 31s 893us/step - loss: 13577.4570 - mse: 13577.4570 - val_loss: 56.7188 - val_mse: 56.7188\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=300, batch_size=64, validation_data=[X_test, y_test], callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69329/69329 [==============================] - 51s 732us/step\n",
      "13570.304087545786\n",
      "4243025614.7816515\n",
      "3822/3822 [==============================] - 3s 732us/step\n",
      "56.7188035192588\n",
      "2.6219890237431023\n"
     ]
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "print(mse(train_pred, y_train))\n",
    "print(mape(train_pred, y_train))\n",
    "test_pred = model.predict(X_test)\n",
    "print(mse(test_pred, y_test))\n",
    "print(mape(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3822/3822 [==============================] - 3s 870us/step - loss: 56.7189 - mse: 56.7189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[56.7188606262207, 56.7188606262207]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82952596],\n",
       "       [0.82952596, 1.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(test_pred.flatten(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 25        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61\n",
      "Trainable params: 61\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [13584.7998046875,\n",
       "  13584.9287109375,\n",
       "  13582.64453125,\n",
       "  13584.1650390625,\n",
       "  13582.48046875,\n",
       "  13583.0830078125,\n",
       "  13581.46484375,\n",
       "  13581.048828125,\n",
       "  13583.0634765625,\n",
       "  13582.65234375,\n",
       "  13582.1064453125,\n",
       "  13581.6123046875,\n",
       "  13582.6591796875,\n",
       "  13581.162109375,\n",
       "  13582.6748046875,\n",
       "  13580.28125,\n",
       "  13578.423828125,\n",
       "  13582.1435546875,\n",
       "  13581.2900390625,\n",
       "  13581.21875,\n",
       "  13583.4736328125,\n",
       "  13581.2666015625,\n",
       "  13583.8330078125,\n",
       "  13579.9208984375,\n",
       "  13580.892578125,\n",
       "  13581.0673828125,\n",
       "  13582.66796875,\n",
       "  13580.71875,\n",
       "  13583.041015625,\n",
       "  13582.970703125,\n",
       "  13583.44921875,\n",
       "  13582.3271484375,\n",
       "  13579.6494140625,\n",
       "  13580.6845703125,\n",
       "  13580.5341796875,\n",
       "  13582.3134765625,\n",
       "  13578.9462890625,\n",
       "  13582.408203125,\n",
       "  13581.4609375,\n",
       "  13581.65234375,\n",
       "  13579.9111328125,\n",
       "  13580.09375,\n",
       "  13580.58984375,\n",
       "  13577.6220703125,\n",
       "  13581.3310546875,\n",
       "  13581.7919921875,\n",
       "  13581.0966796875,\n",
       "  13579.8408203125,\n",
       "  13580.88671875,\n",
       "  13579.8564453125,\n",
       "  13581.244140625,\n",
       "  13577.888671875,\n",
       "  13580.9873046875,\n",
       "  13579.31640625,\n",
       "  13579.603515625,\n",
       "  13578.97265625,\n",
       "  13580.3154296875,\n",
       "  13580.9140625,\n",
       "  13581.080078125,\n",
       "  13581.359375,\n",
       "  13581.0048828125,\n",
       "  13580.701171875,\n",
       "  13580.658203125,\n",
       "  13578.4482421875,\n",
       "  13581.1494140625,\n",
       "  13580.44140625,\n",
       "  13576.5205078125,\n",
       "  13579.6796875,\n",
       "  13580.875,\n",
       "  13579.671875,\n",
       "  13579.884765625,\n",
       "  13580.2294921875,\n",
       "  13579.74609375,\n",
       "  13579.1669921875,\n",
       "  13582.169921875,\n",
       "  13581.0048828125,\n",
       "  13580.048828125,\n",
       "  13581.3671875,\n",
       "  13579.28515625,\n",
       "  13580.267578125,\n",
       "  13582.0048828125,\n",
       "  13581.8583984375,\n",
       "  13579.2744140625,\n",
       "  13579.5712890625,\n",
       "  13580.4658203125,\n",
       "  13580.3291015625,\n",
       "  13580.587890625,\n",
       "  13581.3955078125,\n",
       "  13580.6875,\n",
       "  13581.693359375,\n",
       "  13577.7763671875,\n",
       "  13582.0439453125,\n",
       "  13581.1015625,\n",
       "  13579.5576171875,\n",
       "  13578.2333984375,\n",
       "  13580.021484375,\n",
       "  13580.0712890625,\n",
       "  13579.6728515625,\n",
       "  13580.4775390625,\n",
       "  13579.669921875,\n",
       "  13581.568359375,\n",
       "  13581.365234375,\n",
       "  13580.005859375,\n",
       "  13580.6962890625,\n",
       "  13579.7783203125,\n",
       "  13580.4150390625,\n",
       "  13579.5849609375,\n",
       "  13580.2041015625,\n",
       "  13577.1650390625,\n",
       "  13581.1298828125,\n",
       "  13580.578125,\n",
       "  13577.62890625,\n",
       "  13579.962890625,\n",
       "  13580.7021484375,\n",
       "  13580.1083984375,\n",
       "  13581.2373046875,\n",
       "  13580.2890625,\n",
       "  13579.443359375,\n",
       "  13580.0458984375,\n",
       "  13580.0986328125,\n",
       "  13580.8037109375,\n",
       "  13580.4794921875,\n",
       "  13580.18359375,\n",
       "  13578.5078125,\n",
       "  13579.1220703125,\n",
       "  13579.173828125,\n",
       "  13579.3486328125,\n",
       "  13578.876953125,\n",
       "  13582.017578125,\n",
       "  13578.1572265625,\n",
       "  13581.16796875,\n",
       "  13581.0556640625,\n",
       "  13582.3720703125,\n",
       "  13576.7978515625,\n",
       "  13581.4560546875,\n",
       "  13579.8525390625,\n",
       "  13580.322265625,\n",
       "  13579.13671875,\n",
       "  13578.86328125,\n",
       "  13577.8701171875,\n",
       "  13580.220703125,\n",
       "  13579.4169921875,\n",
       "  13581.1943359375,\n",
       "  13579.919921875,\n",
       "  13578.8984375,\n",
       "  13577.2138671875,\n",
       "  13580.4091796875,\n",
       "  13581.794921875,\n",
       "  13578.423828125,\n",
       "  13578.9697265625,\n",
       "  13580.39453125,\n",
       "  13578.1748046875,\n",
       "  13579.5771484375,\n",
       "  13577.4462890625,\n",
       "  13578.5390625,\n",
       "  13578.6376953125,\n",
       "  13578.6962890625,\n",
       "  13577.4599609375,\n",
       "  13578.7919921875,\n",
       "  13577.6220703125,\n",
       "  13578.482421875,\n",
       "  13578.775390625,\n",
       "  13577.1767578125,\n",
       "  13579.0224609375,\n",
       "  13580.091796875,\n",
       "  13576.5927734375,\n",
       "  13579.537109375,\n",
       "  13577.6689453125,\n",
       "  13579.162109375,\n",
       "  13578.73828125,\n",
       "  13576.720703125,\n",
       "  13577.033203125,\n",
       "  13578.53515625,\n",
       "  13576.3935546875,\n",
       "  13577.2548828125,\n",
       "  13577.369140625,\n",
       "  13577.0244140625,\n",
       "  13578.1259765625,\n",
       "  13577.9140625,\n",
       "  13578.4521484375,\n",
       "  13576.6689453125,\n",
       "  13578.3017578125,\n",
       "  13577.7646484375,\n",
       "  13576.3896484375,\n",
       "  13578.962890625,\n",
       "  13575.59765625,\n",
       "  13575.123046875,\n",
       "  13573.9521484375,\n",
       "  13576.7490234375,\n",
       "  13575.54296875,\n",
       "  13576.6474609375,\n",
       "  13576.8916015625,\n",
       "  13576.9453125,\n",
       "  13575.220703125,\n",
       "  13575.97265625,\n",
       "  13577.5810546875,\n",
       "  13576.8701171875,\n",
       "  13577.6552734375,\n",
       "  13575.9453125,\n",
       "  13578.9521484375,\n",
       "  13575.453125,\n",
       "  13575.9296875,\n",
       "  13577.318359375,\n",
       "  13576.2626953125,\n",
       "  13578.68359375,\n",
       "  13576.75390625,\n",
       "  13577.3349609375,\n",
       "  13577.2509765625,\n",
       "  13575.505859375,\n",
       "  13577.2578125,\n",
       "  13576.0478515625,\n",
       "  13576.37890625,\n",
       "  13574.423828125,\n",
       "  13578.07421875,\n",
       "  13575.1962890625,\n",
       "  13575.2431640625,\n",
       "  13578.3671875,\n",
       "  13576.0888671875,\n",
       "  13577.931640625,\n",
       "  13574.759765625,\n",
       "  13579.7568359375,\n",
       "  13579.2333984375,\n",
       "  13577.2197265625,\n",
       "  13576.791015625,\n",
       "  13578.4765625,\n",
       "  13576.4736328125,\n",
       "  13575.59375,\n",
       "  13575.7265625,\n",
       "  13577.4609375,\n",
       "  13575.7763671875,\n",
       "  13577.5830078125,\n",
       "  13577.4560546875,\n",
       "  13575.3017578125,\n",
       "  13576.466796875,\n",
       "  13576.72265625,\n",
       "  13576.47265625,\n",
       "  13575.2099609375,\n",
       "  13576.1171875,\n",
       "  13576.8388671875,\n",
       "  13575.9599609375,\n",
       "  13573.4130859375,\n",
       "  13576.9560546875,\n",
       "  13576.0126953125,\n",
       "  13573.66796875,\n",
       "  13574.7607421875,\n",
       "  13576.6640625,\n",
       "  13575.8310546875,\n",
       "  13579.322265625,\n",
       "  13574.611328125,\n",
       "  13576.3759765625,\n",
       "  13575.9169921875,\n",
       "  13576.01953125,\n",
       "  13577.4892578125,\n",
       "  13576.1044921875,\n",
       "  13576.189453125,\n",
       "  13576.1396484375,\n",
       "  13576.90234375,\n",
       "  13574.505859375,\n",
       "  13576.7080078125,\n",
       "  13577.388671875,\n",
       "  13576.3486328125,\n",
       "  13577.1748046875,\n",
       "  13577.357421875,\n",
       "  13577.1962890625,\n",
       "  13576.3818359375,\n",
       "  13578.046875,\n",
       "  13578.1953125,\n",
       "  13578.95703125,\n",
       "  13576.412109375,\n",
       "  13577.82421875,\n",
       "  13576.3505859375,\n",
       "  13578.2578125,\n",
       "  13575.0283203125,\n",
       "  13575.115234375,\n",
       "  13576.638671875,\n",
       "  13577.146484375,\n",
       "  13577.1826171875,\n",
       "  13576.1064453125,\n",
       "  13577.333984375,\n",
       "  13576.484375,\n",
       "  13576.3857421875,\n",
       "  13577.193359375,\n",
       "  13580.048828125,\n",
       "  13574.9912109375,\n",
       "  13576.1552734375,\n",
       "  13574.7939453125,\n",
       "  13575.0048828125,\n",
       "  13576.65625,\n",
       "  13575.18359375,\n",
       "  13577.818359375,\n",
       "  13577.4326171875,\n",
       "  13578.265625,\n",
       "  13577.8525390625,\n",
       "  13575.4599609375,\n",
       "  13575.515625,\n",
       "  13576.5673828125,\n",
       "  13576.9912109375,\n",
       "  13576.201171875,\n",
       "  13577.66796875,\n",
       "  13577.45703125],\n",
       " 'mse': [13584.7998046875,\n",
       "  13584.9287109375,\n",
       "  13582.64453125,\n",
       "  13584.1650390625,\n",
       "  13582.48046875,\n",
       "  13583.0830078125,\n",
       "  13581.46484375,\n",
       "  13581.048828125,\n",
       "  13583.0634765625,\n",
       "  13582.65234375,\n",
       "  13582.1064453125,\n",
       "  13581.6123046875,\n",
       "  13582.6591796875,\n",
       "  13581.162109375,\n",
       "  13582.6748046875,\n",
       "  13580.28125,\n",
       "  13578.423828125,\n",
       "  13582.1435546875,\n",
       "  13581.2900390625,\n",
       "  13581.21875,\n",
       "  13583.4736328125,\n",
       "  13581.2666015625,\n",
       "  13583.8330078125,\n",
       "  13579.9208984375,\n",
       "  13580.892578125,\n",
       "  13581.0673828125,\n",
       "  13582.66796875,\n",
       "  13580.71875,\n",
       "  13583.041015625,\n",
       "  13582.970703125,\n",
       "  13583.44921875,\n",
       "  13582.3271484375,\n",
       "  13579.6494140625,\n",
       "  13580.6845703125,\n",
       "  13580.5341796875,\n",
       "  13582.3134765625,\n",
       "  13578.9462890625,\n",
       "  13582.408203125,\n",
       "  13581.4609375,\n",
       "  13581.65234375,\n",
       "  13579.9111328125,\n",
       "  13580.09375,\n",
       "  13580.58984375,\n",
       "  13577.6220703125,\n",
       "  13581.3310546875,\n",
       "  13581.7919921875,\n",
       "  13581.0966796875,\n",
       "  13579.8408203125,\n",
       "  13580.88671875,\n",
       "  13579.8564453125,\n",
       "  13581.244140625,\n",
       "  13577.888671875,\n",
       "  13580.9873046875,\n",
       "  13579.31640625,\n",
       "  13579.603515625,\n",
       "  13578.97265625,\n",
       "  13580.3154296875,\n",
       "  13580.9140625,\n",
       "  13581.080078125,\n",
       "  13581.359375,\n",
       "  13581.0048828125,\n",
       "  13580.701171875,\n",
       "  13580.658203125,\n",
       "  13578.4482421875,\n",
       "  13581.1494140625,\n",
       "  13580.44140625,\n",
       "  13576.5205078125,\n",
       "  13579.6796875,\n",
       "  13580.875,\n",
       "  13579.671875,\n",
       "  13579.884765625,\n",
       "  13580.2294921875,\n",
       "  13579.74609375,\n",
       "  13579.1669921875,\n",
       "  13582.169921875,\n",
       "  13581.0048828125,\n",
       "  13580.048828125,\n",
       "  13581.3671875,\n",
       "  13579.28515625,\n",
       "  13580.267578125,\n",
       "  13582.0048828125,\n",
       "  13581.8583984375,\n",
       "  13579.2744140625,\n",
       "  13579.5712890625,\n",
       "  13580.4658203125,\n",
       "  13580.3291015625,\n",
       "  13580.587890625,\n",
       "  13581.3955078125,\n",
       "  13580.6875,\n",
       "  13581.693359375,\n",
       "  13577.7763671875,\n",
       "  13582.0439453125,\n",
       "  13581.1015625,\n",
       "  13579.5576171875,\n",
       "  13578.2333984375,\n",
       "  13580.021484375,\n",
       "  13580.0712890625,\n",
       "  13579.6728515625,\n",
       "  13580.4775390625,\n",
       "  13579.669921875,\n",
       "  13581.568359375,\n",
       "  13581.365234375,\n",
       "  13580.005859375,\n",
       "  13580.6962890625,\n",
       "  13579.7783203125,\n",
       "  13580.4150390625,\n",
       "  13579.5849609375,\n",
       "  13580.2041015625,\n",
       "  13577.1650390625,\n",
       "  13581.1298828125,\n",
       "  13580.578125,\n",
       "  13577.62890625,\n",
       "  13579.962890625,\n",
       "  13580.7021484375,\n",
       "  13580.1083984375,\n",
       "  13581.2373046875,\n",
       "  13580.2890625,\n",
       "  13579.443359375,\n",
       "  13580.0458984375,\n",
       "  13580.0986328125,\n",
       "  13580.8037109375,\n",
       "  13580.4794921875,\n",
       "  13580.18359375,\n",
       "  13578.5078125,\n",
       "  13579.1220703125,\n",
       "  13579.173828125,\n",
       "  13579.3486328125,\n",
       "  13578.876953125,\n",
       "  13582.017578125,\n",
       "  13578.1572265625,\n",
       "  13581.16796875,\n",
       "  13581.0556640625,\n",
       "  13582.3720703125,\n",
       "  13576.7978515625,\n",
       "  13581.4560546875,\n",
       "  13579.8525390625,\n",
       "  13580.322265625,\n",
       "  13579.13671875,\n",
       "  13578.86328125,\n",
       "  13577.8701171875,\n",
       "  13580.220703125,\n",
       "  13579.4169921875,\n",
       "  13581.1943359375,\n",
       "  13579.919921875,\n",
       "  13578.8984375,\n",
       "  13577.2138671875,\n",
       "  13580.4091796875,\n",
       "  13581.794921875,\n",
       "  13578.423828125,\n",
       "  13578.9697265625,\n",
       "  13580.39453125,\n",
       "  13578.1748046875,\n",
       "  13579.5771484375,\n",
       "  13577.4462890625,\n",
       "  13578.5390625,\n",
       "  13578.6376953125,\n",
       "  13578.6962890625,\n",
       "  13577.4599609375,\n",
       "  13578.7919921875,\n",
       "  13577.6220703125,\n",
       "  13578.482421875,\n",
       "  13578.775390625,\n",
       "  13577.1767578125,\n",
       "  13579.0224609375,\n",
       "  13580.091796875,\n",
       "  13576.5927734375,\n",
       "  13579.537109375,\n",
       "  13577.6689453125,\n",
       "  13579.162109375,\n",
       "  13578.73828125,\n",
       "  13576.720703125,\n",
       "  13577.033203125,\n",
       "  13578.53515625,\n",
       "  13576.3935546875,\n",
       "  13577.2548828125,\n",
       "  13577.369140625,\n",
       "  13577.0244140625,\n",
       "  13578.1259765625,\n",
       "  13577.9140625,\n",
       "  13578.4521484375,\n",
       "  13576.6689453125,\n",
       "  13578.3017578125,\n",
       "  13577.7646484375,\n",
       "  13576.3896484375,\n",
       "  13578.962890625,\n",
       "  13575.59765625,\n",
       "  13575.123046875,\n",
       "  13573.9521484375,\n",
       "  13576.7490234375,\n",
       "  13575.54296875,\n",
       "  13576.6474609375,\n",
       "  13576.8916015625,\n",
       "  13576.9453125,\n",
       "  13575.220703125,\n",
       "  13575.97265625,\n",
       "  13577.5810546875,\n",
       "  13576.8701171875,\n",
       "  13577.6552734375,\n",
       "  13575.9453125,\n",
       "  13578.9521484375,\n",
       "  13575.453125,\n",
       "  13575.9296875,\n",
       "  13577.318359375,\n",
       "  13576.2626953125,\n",
       "  13578.68359375,\n",
       "  13576.75390625,\n",
       "  13577.3349609375,\n",
       "  13577.2509765625,\n",
       "  13575.505859375,\n",
       "  13577.2578125,\n",
       "  13576.0478515625,\n",
       "  13576.37890625,\n",
       "  13574.423828125,\n",
       "  13578.07421875,\n",
       "  13575.1962890625,\n",
       "  13575.2431640625,\n",
       "  13578.3671875,\n",
       "  13576.0888671875,\n",
       "  13577.931640625,\n",
       "  13574.759765625,\n",
       "  13579.7568359375,\n",
       "  13579.2333984375,\n",
       "  13577.2197265625,\n",
       "  13576.791015625,\n",
       "  13578.4765625,\n",
       "  13576.4736328125,\n",
       "  13575.59375,\n",
       "  13575.7265625,\n",
       "  13577.4609375,\n",
       "  13575.7763671875,\n",
       "  13577.5830078125,\n",
       "  13577.4560546875,\n",
       "  13575.3017578125,\n",
       "  13576.466796875,\n",
       "  13576.72265625,\n",
       "  13576.47265625,\n",
       "  13575.2099609375,\n",
       "  13576.1171875,\n",
       "  13576.8388671875,\n",
       "  13575.9599609375,\n",
       "  13573.4130859375,\n",
       "  13576.9560546875,\n",
       "  13576.0126953125,\n",
       "  13573.66796875,\n",
       "  13574.7607421875,\n",
       "  13576.6640625,\n",
       "  13575.8310546875,\n",
       "  13579.322265625,\n",
       "  13574.611328125,\n",
       "  13576.3759765625,\n",
       "  13575.9169921875,\n",
       "  13576.01953125,\n",
       "  13577.4892578125,\n",
       "  13576.1044921875,\n",
       "  13576.189453125,\n",
       "  13576.1396484375,\n",
       "  13576.90234375,\n",
       "  13574.505859375,\n",
       "  13576.7080078125,\n",
       "  13577.388671875,\n",
       "  13576.3486328125,\n",
       "  13577.1748046875,\n",
       "  13577.357421875,\n",
       "  13577.1962890625,\n",
       "  13576.3818359375,\n",
       "  13578.046875,\n",
       "  13578.1953125,\n",
       "  13578.95703125,\n",
       "  13576.412109375,\n",
       "  13577.82421875,\n",
       "  13576.3505859375,\n",
       "  13578.2578125,\n",
       "  13575.0283203125,\n",
       "  13575.115234375,\n",
       "  13576.638671875,\n",
       "  13577.146484375,\n",
       "  13577.1826171875,\n",
       "  13576.1064453125,\n",
       "  13577.333984375,\n",
       "  13576.484375,\n",
       "  13576.3857421875,\n",
       "  13577.193359375,\n",
       "  13580.048828125,\n",
       "  13574.9912109375,\n",
       "  13576.1552734375,\n",
       "  13574.7939453125,\n",
       "  13575.0048828125,\n",
       "  13576.65625,\n",
       "  13575.18359375,\n",
       "  13577.818359375,\n",
       "  13577.4326171875,\n",
       "  13578.265625,\n",
       "  13577.8525390625,\n",
       "  13575.4599609375,\n",
       "  13575.515625,\n",
       "  13576.5673828125,\n",
       "  13576.9912109375,\n",
       "  13576.201171875,\n",
       "  13577.66796875,\n",
       "  13577.45703125],\n",
       " 'val_loss': [59.4626350402832,\n",
       "  57.20225524902344,\n",
       "  57.985252380371094,\n",
       "  60.27815246582031,\n",
       "  58.91069793701172,\n",
       "  61.116268157958984,\n",
       "  59.493614196777344,\n",
       "  62.28074264526367,\n",
       "  57.42725372314453,\n",
       "  65.00704956054688,\n",
       "  68.57085418701172,\n",
       "  56.194339752197266,\n",
       "  58.33272171020508,\n",
       "  56.87054443359375,\n",
       "  60.282989501953125,\n",
       "  71.05884552001953,\n",
       "  58.15818786621094,\n",
       "  58.848175048828125,\n",
       "  59.17185974121094,\n",
       "  57.58717346191406,\n",
       "  58.90313720703125,\n",
       "  57.0100212097168,\n",
       "  55.820316314697266,\n",
       "  57.53662872314453,\n",
       "  56.48838806152344,\n",
       "  56.65956497192383,\n",
       "  57.22030258178711,\n",
       "  58.26968002319336,\n",
       "  56.833335876464844,\n",
       "  57.11135482788086,\n",
       "  61.80549240112305,\n",
       "  55.76436233520508,\n",
       "  55.89164352416992,\n",
       "  55.86610412597656,\n",
       "  70.75863647460938,\n",
       "  61.79241943359375,\n",
       "  65.4446029663086,\n",
       "  58.751739501953125,\n",
       "  58.630191802978516,\n",
       "  62.93621826171875,\n",
       "  59.195777893066406,\n",
       "  55.297000885009766,\n",
       "  60.47549057006836,\n",
       "  57.183467864990234,\n",
       "  60.677581787109375,\n",
       "  56.50075912475586,\n",
       "  58.21234130859375,\n",
       "  57.6827278137207,\n",
       "  60.923221588134766,\n",
       "  56.567378997802734,\n",
       "  71.45983123779297,\n",
       "  71.16053009033203,\n",
       "  57.6998405456543,\n",
       "  56.106651306152344,\n",
       "  59.60729217529297,\n",
       "  57.58902359008789,\n",
       "  59.63968276977539,\n",
       "  57.09803009033203,\n",
       "  59.09424591064453,\n",
       "  55.574195861816406,\n",
       "  60.076786041259766,\n",
       "  59.58266830444336,\n",
       "  59.812259674072266,\n",
       "  54.44995880126953,\n",
       "  60.069725036621094,\n",
       "  57.08955001831055,\n",
       "  54.16714096069336,\n",
       "  54.915103912353516,\n",
       "  57.732749938964844,\n",
       "  55.58842086791992,\n",
       "  58.35415267944336,\n",
       "  120.0842056274414,\n",
       "  69.24282836914062,\n",
       "  58.33360290527344,\n",
       "  57.71142578125,\n",
       "  56.82143783569336,\n",
       "  54.64841842651367,\n",
       "  59.06248092651367,\n",
       "  55.81050109863281,\n",
       "  66.72540283203125,\n",
       "  56.64060592651367,\n",
       "  65.28297424316406,\n",
       "  56.859596252441406,\n",
       "  57.85646057128906,\n",
       "  56.79362487792969,\n",
       "  61.67978286743164,\n",
       "  56.74927520751953,\n",
       "  60.815757751464844,\n",
       "  56.15385437011719,\n",
       "  59.56669235229492,\n",
       "  55.49250030517578,\n",
       "  60.02726364135742,\n",
       "  62.26618194580078,\n",
       "  57.102970123291016,\n",
       "  57.85829162597656,\n",
       "  75.17060852050781,\n",
       "  58.378509521484375,\n",
       "  55.01668930053711,\n",
       "  55.68181610107422,\n",
       "  62.78316879272461,\n",
       "  55.96099853515625,\n",
       "  57.990684509277344,\n",
       "  55.63442611694336,\n",
       "  60.97645568847656,\n",
       "  55.743282318115234,\n",
       "  53.95002746582031,\n",
       "  59.05720901489258,\n",
       "  62.018646240234375,\n",
       "  57.349700927734375,\n",
       "  56.835227966308594,\n",
       "  65.479248046875,\n",
       "  57.364288330078125,\n",
       "  57.72624969482422,\n",
       "  58.84775161743164,\n",
       "  56.62154006958008,\n",
       "  57.089317321777344,\n",
       "  60.46999740600586,\n",
       "  57.905540466308594,\n",
       "  62.50702667236328,\n",
       "  55.824424743652344,\n",
       "  59.59170913696289,\n",
       "  56.7371940612793,\n",
       "  57.38412094116211,\n",
       "  58.09010314941406,\n",
       "  59.312156677246094,\n",
       "  62.101646423339844,\n",
       "  59.50745391845703,\n",
       "  57.7099723815918,\n",
       "  60.405357360839844,\n",
       "  56.26348114013672,\n",
       "  58.387699127197266,\n",
       "  62.02947998046875,\n",
       "  57.789058685302734,\n",
       "  57.5003662109375,\n",
       "  66.15214538574219,\n",
       "  55.26958465576172,\n",
       "  60.09402847290039,\n",
       "  57.14929962158203,\n",
       "  61.687801361083984,\n",
       "  56.63309097290039,\n",
       "  55.658390045166016,\n",
       "  58.51089859008789,\n",
       "  59.61292266845703,\n",
       "  55.7388916015625,\n",
       "  57.70545196533203,\n",
       "  58.294803619384766,\n",
       "  56.930973052978516,\n",
       "  60.14788055419922,\n",
       "  57.49966049194336,\n",
       "  60.30616760253906,\n",
       "  62.68798828125,\n",
       "  61.09339141845703,\n",
       "  60.018890380859375,\n",
       "  56.37442398071289,\n",
       "  57.951904296875,\n",
       "  59.41619873046875,\n",
       "  61.71195983886719,\n",
       "  59.583518981933594,\n",
       "  59.99618148803711,\n",
       "  57.69624328613281,\n",
       "  65.02397155761719,\n",
       "  65.18022918701172,\n",
       "  59.601383209228516,\n",
       "  59.49917984008789,\n",
       "  57.123779296875,\n",
       "  56.09114456176758,\n",
       "  62.011497497558594,\n",
       "  56.78619384765625,\n",
       "  56.63408660888672,\n",
       "  57.75817108154297,\n",
       "  55.98493576049805,\n",
       "  57.126625061035156,\n",
       "  57.5118522644043,\n",
       "  69.05097198486328,\n",
       "  58.711326599121094,\n",
       "  55.20512390136719,\n",
       "  56.882930755615234,\n",
       "  60.781158447265625,\n",
       "  55.61295700073242,\n",
       "  58.38697814941406,\n",
       "  58.33791732788086,\n",
       "  53.75972366333008,\n",
       "  54.42823791503906,\n",
       "  60.23839569091797,\n",
       "  59.706790924072266,\n",
       "  56.58042526245117,\n",
       "  57.486671447753906,\n",
       "  58.38054275512695,\n",
       "  56.156829833984375,\n",
       "  56.55885314941406,\n",
       "  63.068233489990234,\n",
       "  59.03350830078125,\n",
       "  59.411460876464844,\n",
       "  58.20648956298828,\n",
       "  62.16732406616211,\n",
       "  54.69783020019531,\n",
       "  57.85846710205078,\n",
       "  59.46026611328125,\n",
       "  61.020362854003906,\n",
       "  57.31101989746094,\n",
       "  56.26946258544922,\n",
       "  58.6475830078125,\n",
       "  59.59334945678711,\n",
       "  58.19049835205078,\n",
       "  59.26700210571289,\n",
       "  65.84259033203125,\n",
       "  57.46204376220703,\n",
       "  62.76329803466797,\n",
       "  56.12675094604492,\n",
       "  65.86981964111328,\n",
       "  56.86040115356445,\n",
       "  55.68335723876953,\n",
       "  58.44485092163086,\n",
       "  58.27901840209961,\n",
       "  59.49209213256836,\n",
       "  56.09565734863281,\n",
       "  59.34514617919922,\n",
       "  61.391090393066406,\n",
       "  57.4150390625,\n",
       "  59.57123947143555,\n",
       "  56.41851043701172,\n",
       "  54.34245681762695,\n",
       "  60.981170654296875,\n",
       "  65.46458435058594,\n",
       "  55.6107177734375,\n",
       "  57.782958984375,\n",
       "  59.92074966430664,\n",
       "  58.40951919555664,\n",
       "  61.58942413330078,\n",
       "  58.183467864990234,\n",
       "  55.16652297973633,\n",
       "  57.06492233276367,\n",
       "  57.9853401184082,\n",
       "  84.2719497680664,\n",
       "  59.117515563964844,\n",
       "  61.284297943115234,\n",
       "  59.761478424072266,\n",
       "  61.171241760253906,\n",
       "  59.673030853271484,\n",
       "  56.964263916015625,\n",
       "  68.34400939941406,\n",
       "  61.197872161865234,\n",
       "  72.20045471191406,\n",
       "  56.29326248168945,\n",
       "  65.4267807006836,\n",
       "  63.922080993652344,\n",
       "  79.08361053466797,\n",
       "  63.01191711425781,\n",
       "  67.33727264404297,\n",
       "  58.00303268432617,\n",
       "  59.04295349121094,\n",
       "  58.08644485473633,\n",
       "  66.1622314453125,\n",
       "  57.71830368041992,\n",
       "  66.32754516601562,\n",
       "  58.788692474365234,\n",
       "  58.16975784301758,\n",
       "  59.54404830932617,\n",
       "  57.44635009765625,\n",
       "  63.45139694213867,\n",
       "  59.829158782958984,\n",
       "  58.74254608154297,\n",
       "  62.89799880981445,\n",
       "  58.086517333984375,\n",
       "  58.253353118896484,\n",
       "  58.6562614440918,\n",
       "  61.58940505981445,\n",
       "  63.82793426513672,\n",
       "  57.57704544067383,\n",
       "  56.706886291503906,\n",
       "  57.41503143310547,\n",
       "  55.751094818115234,\n",
       "  56.546966552734375,\n",
       "  55.624473571777344,\n",
       "  57.13728332519531,\n",
       "  62.06437683105469,\n",
       "  57.10920715332031,\n",
       "  55.59600830078125,\n",
       "  61.856842041015625,\n",
       "  57.50826644897461,\n",
       "  55.71148681640625,\n",
       "  66.86888885498047,\n",
       "  55.50871658325195,\n",
       "  64.56204986572266,\n",
       "  58.76827621459961,\n",
       "  56.99229049682617,\n",
       "  58.53696060180664,\n",
       "  57.83147430419922,\n",
       "  61.309242248535156,\n",
       "  57.25086212158203,\n",
       "  59.61390686035156,\n",
       "  55.60673522949219,\n",
       "  61.15017318725586,\n",
       "  57.43759536743164,\n",
       "  57.28881072998047,\n",
       "  56.66751480102539,\n",
       "  56.044673919677734,\n",
       "  54.641605377197266,\n",
       "  57.694969177246094,\n",
       "  56.71882247924805],\n",
       " 'val_mse': [59.4626350402832,\n",
       "  57.20225524902344,\n",
       "  57.985252380371094,\n",
       "  60.27815246582031,\n",
       "  58.91069793701172,\n",
       "  61.116268157958984,\n",
       "  59.493614196777344,\n",
       "  62.28074264526367,\n",
       "  57.42725372314453,\n",
       "  65.00704956054688,\n",
       "  68.57085418701172,\n",
       "  56.194339752197266,\n",
       "  58.33272171020508,\n",
       "  56.87054443359375,\n",
       "  60.282989501953125,\n",
       "  71.05884552001953,\n",
       "  58.15818786621094,\n",
       "  58.848175048828125,\n",
       "  59.17185974121094,\n",
       "  57.58717346191406,\n",
       "  58.90313720703125,\n",
       "  57.0100212097168,\n",
       "  55.820316314697266,\n",
       "  57.53662872314453,\n",
       "  56.48838806152344,\n",
       "  56.65956497192383,\n",
       "  57.22030258178711,\n",
       "  58.26968002319336,\n",
       "  56.833335876464844,\n",
       "  57.11135482788086,\n",
       "  61.80549240112305,\n",
       "  55.76436233520508,\n",
       "  55.89164352416992,\n",
       "  55.86610412597656,\n",
       "  70.75863647460938,\n",
       "  61.79241943359375,\n",
       "  65.4446029663086,\n",
       "  58.751739501953125,\n",
       "  58.630191802978516,\n",
       "  62.93621826171875,\n",
       "  59.195777893066406,\n",
       "  55.297000885009766,\n",
       "  60.47549057006836,\n",
       "  57.183467864990234,\n",
       "  60.677581787109375,\n",
       "  56.50075912475586,\n",
       "  58.21234130859375,\n",
       "  57.6827278137207,\n",
       "  60.923221588134766,\n",
       "  56.567378997802734,\n",
       "  71.45983123779297,\n",
       "  71.16053009033203,\n",
       "  57.6998405456543,\n",
       "  56.106651306152344,\n",
       "  59.60729217529297,\n",
       "  57.58902359008789,\n",
       "  59.63968276977539,\n",
       "  57.09803009033203,\n",
       "  59.09424591064453,\n",
       "  55.574195861816406,\n",
       "  60.076786041259766,\n",
       "  59.58266830444336,\n",
       "  59.812259674072266,\n",
       "  54.44995880126953,\n",
       "  60.069725036621094,\n",
       "  57.08955001831055,\n",
       "  54.16714096069336,\n",
       "  54.915103912353516,\n",
       "  57.732749938964844,\n",
       "  55.58842086791992,\n",
       "  58.35415267944336,\n",
       "  120.0842056274414,\n",
       "  69.24282836914062,\n",
       "  58.33360290527344,\n",
       "  57.71142578125,\n",
       "  56.82143783569336,\n",
       "  54.64841842651367,\n",
       "  59.06248092651367,\n",
       "  55.81050109863281,\n",
       "  66.72540283203125,\n",
       "  56.64060592651367,\n",
       "  65.28297424316406,\n",
       "  56.859596252441406,\n",
       "  57.85646057128906,\n",
       "  56.79362487792969,\n",
       "  61.67978286743164,\n",
       "  56.74927520751953,\n",
       "  60.815757751464844,\n",
       "  56.15385437011719,\n",
       "  59.56669235229492,\n",
       "  55.49250030517578,\n",
       "  60.02726364135742,\n",
       "  62.26618194580078,\n",
       "  57.102970123291016,\n",
       "  57.85829162597656,\n",
       "  75.17060852050781,\n",
       "  58.378509521484375,\n",
       "  55.01668930053711,\n",
       "  55.68181610107422,\n",
       "  62.78316879272461,\n",
       "  55.96099853515625,\n",
       "  57.990684509277344,\n",
       "  55.63442611694336,\n",
       "  60.97645568847656,\n",
       "  55.743282318115234,\n",
       "  53.95002746582031,\n",
       "  59.05720901489258,\n",
       "  62.018646240234375,\n",
       "  57.349700927734375,\n",
       "  56.835227966308594,\n",
       "  65.479248046875,\n",
       "  57.364288330078125,\n",
       "  57.72624969482422,\n",
       "  58.84775161743164,\n",
       "  56.62154006958008,\n",
       "  57.089317321777344,\n",
       "  60.46999740600586,\n",
       "  57.905540466308594,\n",
       "  62.50702667236328,\n",
       "  55.824424743652344,\n",
       "  59.59170913696289,\n",
       "  56.7371940612793,\n",
       "  57.38412094116211,\n",
       "  58.09010314941406,\n",
       "  59.312156677246094,\n",
       "  62.101646423339844,\n",
       "  59.50745391845703,\n",
       "  57.7099723815918,\n",
       "  60.405357360839844,\n",
       "  56.26348114013672,\n",
       "  58.387699127197266,\n",
       "  62.02947998046875,\n",
       "  57.789058685302734,\n",
       "  57.5003662109375,\n",
       "  66.15214538574219,\n",
       "  55.26958465576172,\n",
       "  60.09402847290039,\n",
       "  57.14929962158203,\n",
       "  61.687801361083984,\n",
       "  56.63309097290039,\n",
       "  55.658390045166016,\n",
       "  58.51089859008789,\n",
       "  59.61292266845703,\n",
       "  55.7388916015625,\n",
       "  57.70545196533203,\n",
       "  58.294803619384766,\n",
       "  56.930973052978516,\n",
       "  60.14788055419922,\n",
       "  57.49966049194336,\n",
       "  60.30616760253906,\n",
       "  62.68798828125,\n",
       "  61.09339141845703,\n",
       "  60.018890380859375,\n",
       "  56.37442398071289,\n",
       "  57.951904296875,\n",
       "  59.41619873046875,\n",
       "  61.71195983886719,\n",
       "  59.583518981933594,\n",
       "  59.99618148803711,\n",
       "  57.69624328613281,\n",
       "  65.02397155761719,\n",
       "  65.18022918701172,\n",
       "  59.601383209228516,\n",
       "  59.49917984008789,\n",
       "  57.123779296875,\n",
       "  56.09114456176758,\n",
       "  62.011497497558594,\n",
       "  56.78619384765625,\n",
       "  56.63408660888672,\n",
       "  57.75817108154297,\n",
       "  55.98493576049805,\n",
       "  57.126625061035156,\n",
       "  57.5118522644043,\n",
       "  69.05097198486328,\n",
       "  58.711326599121094,\n",
       "  55.20512390136719,\n",
       "  56.882930755615234,\n",
       "  60.781158447265625,\n",
       "  55.61295700073242,\n",
       "  58.38697814941406,\n",
       "  58.33791732788086,\n",
       "  53.75972366333008,\n",
       "  54.42823791503906,\n",
       "  60.23839569091797,\n",
       "  59.706790924072266,\n",
       "  56.58042526245117,\n",
       "  57.486671447753906,\n",
       "  58.38054275512695,\n",
       "  56.156829833984375,\n",
       "  56.55885314941406,\n",
       "  63.068233489990234,\n",
       "  59.03350830078125,\n",
       "  59.411460876464844,\n",
       "  58.20648956298828,\n",
       "  62.16732406616211,\n",
       "  54.69783020019531,\n",
       "  57.85846710205078,\n",
       "  59.46026611328125,\n",
       "  61.020362854003906,\n",
       "  57.31101989746094,\n",
       "  56.26946258544922,\n",
       "  58.6475830078125,\n",
       "  59.59334945678711,\n",
       "  58.19049835205078,\n",
       "  59.26700210571289,\n",
       "  65.84259033203125,\n",
       "  57.46204376220703,\n",
       "  62.76329803466797,\n",
       "  56.12675094604492,\n",
       "  65.86981964111328,\n",
       "  56.86040115356445,\n",
       "  55.68335723876953,\n",
       "  58.44485092163086,\n",
       "  58.27901840209961,\n",
       "  59.49209213256836,\n",
       "  56.09565734863281,\n",
       "  59.34514617919922,\n",
       "  61.391090393066406,\n",
       "  57.4150390625,\n",
       "  59.57123947143555,\n",
       "  56.41851043701172,\n",
       "  54.34245681762695,\n",
       "  60.981170654296875,\n",
       "  65.46458435058594,\n",
       "  55.6107177734375,\n",
       "  57.782958984375,\n",
       "  59.92074966430664,\n",
       "  58.40951919555664,\n",
       "  61.58942413330078,\n",
       "  58.183467864990234,\n",
       "  55.16652297973633,\n",
       "  57.06492233276367,\n",
       "  57.9853401184082,\n",
       "  84.2719497680664,\n",
       "  59.117515563964844,\n",
       "  61.284297943115234,\n",
       "  59.761478424072266,\n",
       "  61.171241760253906,\n",
       "  59.673030853271484,\n",
       "  56.964263916015625,\n",
       "  68.34400939941406,\n",
       "  61.197872161865234,\n",
       "  72.20045471191406,\n",
       "  56.29326248168945,\n",
       "  65.4267807006836,\n",
       "  63.922080993652344,\n",
       "  79.08361053466797,\n",
       "  63.01191711425781,\n",
       "  67.33727264404297,\n",
       "  58.00303268432617,\n",
       "  59.04295349121094,\n",
       "  58.08644485473633,\n",
       "  66.1622314453125,\n",
       "  57.71830368041992,\n",
       "  66.32754516601562,\n",
       "  58.788692474365234,\n",
       "  58.16975784301758,\n",
       "  59.54404830932617,\n",
       "  57.44635009765625,\n",
       "  63.45139694213867,\n",
       "  59.829158782958984,\n",
       "  58.74254608154297,\n",
       "  62.89799880981445,\n",
       "  58.086517333984375,\n",
       "  58.253353118896484,\n",
       "  58.6562614440918,\n",
       "  61.58940505981445,\n",
       "  63.82793426513672,\n",
       "  57.57704544067383,\n",
       "  56.706886291503906,\n",
       "  57.41503143310547,\n",
       "  55.751094818115234,\n",
       "  56.546966552734375,\n",
       "  55.624473571777344,\n",
       "  57.13728332519531,\n",
       "  62.06437683105469,\n",
       "  57.10920715332031,\n",
       "  55.59600830078125,\n",
       "  61.856842041015625,\n",
       "  57.50826644897461,\n",
       "  55.71148681640625,\n",
       "  66.86888885498047,\n",
       "  55.50871658325195,\n",
       "  64.56204986572266,\n",
       "  58.76827621459961,\n",
       "  56.99229049682617,\n",
       "  58.53696060180664,\n",
       "  57.83147430419922,\n",
       "  61.309242248535156,\n",
       "  57.25086212158203,\n",
       "  59.61390686035156,\n",
       "  55.60673522949219,\n",
       "  61.15017318725586,\n",
       "  57.43759536743164,\n",
       "  57.28881072998047,\n",
       "  56.66751480102539,\n",
       "  56.044673919677734,\n",
       "  54.641605377197266,\n",
       "  57.694969177246094,\n",
       "  56.71882247924805]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(history.history)\n",
    "model_df[['loss', 'val_loss']].plot()\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Training Period\", pad=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory vs Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_pred = X4_pred2\n",
    "y4_pred = y4_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2126/2126 [==============================] - 1s 675us/step\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict(X4_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: 0.86658, st_er: 0.001315\n",
      "y = 0.5956*x + 0.0381\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = y4_pred.flatten()\n",
    "y = test_pred.flatten()\n",
    "m, b, r, p, st_er = stats.linregress(x,y) \n",
    "\n",
    "yfit = [b + m * xi for xi in x]\n",
    "yisx = [0 + 1 * xi for xi in x]\n",
    "plt.plot(x, yfit)\n",
    "plt.plot(x, yisx)\n",
    "\n",
    "plt.scatter(y4_pred, test_pred,  color='black')\n",
    "plt.axis([0,100, 0, 100])\n",
    "plt.xlabel(\"Theory (nT)\")\n",
    "plt.ylabel(\"Prediction (nT)\")\n",
    "plt.title(\"Neural Network Prediction vs Theory\", fontsize=15)\n",
    "# print(r, st_er)\n",
    "print(\"r: {:.5f}, st_er: {:.6f}\".format(r, st_er))\n",
    "print(\"y = \"+str(round(m,4))+\"*x + \"+str(round(b,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df1_test.loc[df1_test['exp']==36]\n",
    "df4_pred = df4_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/vkjb_lqj2_30lnyhzhjqtg9w0000gp/T/ipykernel_15963/2364276288.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df4_pred['predict'] = test_pred\n"
     ]
    }
   ],
   "source": [
    "df4_pred['predict'] = test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exp</th>\n",
       "      <th>mini_exp</th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>amp</th>\n",
       "      <th>neg</th>\n",
       "      <th>mean0</th>\n",
       "      <th>mean1</th>\n",
       "      <th>mean2</th>\n",
       "      <th>theory</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5437480</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-183.593781</td>\n",
       "      <td>-184.268631</td>\n",
       "      <td>-199.538742</td>\n",
       "      <td>4.988159</td>\n",
       "      <td>7.116714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437481</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-383.761566</td>\n",
       "      <td>-383.066467</td>\n",
       "      <td>-395.916077</td>\n",
       "      <td>4.591202</td>\n",
       "      <td>5.983032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437482</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-779.052185</td>\n",
       "      <td>-785.270813</td>\n",
       "      <td>-803.943787</td>\n",
       "      <td>4.237324</td>\n",
       "      <td>5.576271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437483</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1017.557312</td>\n",
       "      <td>-1026.294189</td>\n",
       "      <td>-1037.133667</td>\n",
       "      <td>3.918999</td>\n",
       "      <td>5.165688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437519</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-1508.948120</td>\n",
       "      <td>-1513.032471</td>\n",
       "      <td>-1527.041992</td>\n",
       "      <td>3.371496</td>\n",
       "      <td>4.489059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736792</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.617607</td>\n",
       "      <td>-59.281792</td>\n",
       "      <td>-58.657875</td>\n",
       "      <td>-7.175758</td>\n",
       "      <td>-8.123253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736803</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>151.715485</td>\n",
       "      <td>130.792816</td>\n",
       "      <td>139.797287</td>\n",
       "      <td>-8.527348</td>\n",
       "      <td>-14.865891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736806</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-115.256683</td>\n",
       "      <td>-131.555984</td>\n",
       "      <td>-127.650993</td>\n",
       "      <td>-6.039019</td>\n",
       "      <td>-10.395432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736807</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-253.580444</td>\n",
       "      <td>-268.104340</td>\n",
       "      <td>-266.388824</td>\n",
       "      <td>-5.501382</td>\n",
       "      <td>-6.579582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736863</th>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>578.832947</td>\n",
       "      <td>494.954071</td>\n",
       "      <td>569.023071</td>\n",
       "      <td>-33.141899</td>\n",
       "      <td>-46.847927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68026 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exp  mini_exp   i   j   k  amp  neg        mean0        mean1  \\\n",
       "5437480   36         6   7  32  40    2    0  -183.593781  -184.268631   \n",
       "5437481   36         6   7  32  41    2    0  -383.761566  -383.066467   \n",
       "5437482   36         6   7  32  42    2    0  -779.052185  -785.270813   \n",
       "5437483   36         6   7  32  43    2    0 -1017.557312 -1026.294189   \n",
       "5437519   36         6   7  33  15    2    0 -1508.948120 -1513.032471   \n",
       "...      ...       ...  ..  ..  ..  ...  ...          ...          ...   \n",
       "5736792   36         6  36  37  24    2    1   -46.617607   -59.281792   \n",
       "5736803   36         6  36  37  35    2    1   151.715485   130.792816   \n",
       "5736806   36         6  36  37  38    2    1  -115.256683  -131.555984   \n",
       "5736807   36         6  36  37  39    2    1  -253.580444  -268.104340   \n",
       "5736863   36         6  36  38  31    2    1   578.832947   494.954071   \n",
       "\n",
       "               mean2     theory    predict  \n",
       "5437480  -199.538742   4.988159   7.116714  \n",
       "5437481  -395.916077   4.591202   5.983032  \n",
       "5437482  -803.943787   4.237324   5.576271  \n",
       "5437483 -1037.133667   3.918999   5.165688  \n",
       "5437519 -1527.041992   3.371496   4.489059  \n",
       "...              ...        ...        ...  \n",
       "5736792   -58.657875  -7.175758  -8.123253  \n",
       "5736803   139.797287  -8.527348 -14.865891  \n",
       "5736806  -127.650993  -6.039019 -10.395432  \n",
       "5736807  -266.388824  -5.501382  -6.579582  \n",
       "5736863   569.023071 -33.141899 -46.847927  \n",
       "\n",
       "[68026 rows x 12 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yd/vkjb_lqj2_30lnyhzhjqtg9w0000gp/T/ipykernel_15963/3257651465.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['predict'] = 0.00\n"
     ]
    }
   ],
   "source": [
    "df_test['predict'] = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_test[['i', 'j', 'k', 'neg', 'predict']]\n",
    "df_pre = df4_pred[['i', 'j', 'k', 'neg', 'predict']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 360448 entries, 5406720 to 5767167\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count   Dtype  \n",
      "---  ------   --------------   -----  \n",
      " 0   i        360448 non-null  int64  \n",
      " 1   j        360448 non-null  int64  \n",
      " 2   k        360448 non-null  int64  \n",
      " 3   neg      360448 non-null  int64  \n",
      " 4   predict  360448 non-null  float64\n",
      "dtypes: float64(1), int64(4)\n",
      "memory usage: 16.5 MB\n"
     ]
    }
   ],
   "source": [
    "df_out.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(df_pre)):\n",
    "    i = df_pre.iloc[x, :]['i'].astype(int)\n",
    "    j = df_pre.iloc[x, :]['j'].astype(int)\n",
    "    k = df_pre.iloc[x, :]['k'].astype(int)\n",
    "    neg = df_pre.iloc[x, :]['neg'].astype(int)\n",
    "    pred = df_pre.iloc[x, :]['predict']\n",
    "    idx = df_out[(df_out['i']==i) & (df_out['j']==j) & (df_out['k']==k) & (df_out['neg']==neg)].index\n",
    "    df_out.loc[idx, 'predict']= pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5437480</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>7.116714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437481</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>5.983032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437482</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>5.576271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437483</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>5.165688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5437519</th>\n",
       "      <td>7</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4.489059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736792</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.123253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736803</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>-14.865891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736806</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.395432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736807</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.579582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5736863</th>\n",
       "      <td>36</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>-46.847927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68026 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   j   k  neg    predict\n",
       "5437480   7  32  40    0   7.116714\n",
       "5437481   7  32  41    0   5.983032\n",
       "5437482   7  32  42    0   5.576271\n",
       "5437483   7  32  43    0   5.165688\n",
       "5437519   7  33  15    0   4.489059\n",
       "...      ..  ..  ..  ...        ...\n",
       "5736792  36  37  24    1  -8.123253\n",
       "5736803  36  37  35    1 -14.865891\n",
       "5736806  36  37  38    1 -10.395432\n",
       "5736807  36  37  39    1  -6.579582\n",
       "5736863  36  38  31    1 -46.847927\n",
       "\n",
       "[68026 rows x 5 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5437480</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>7.116714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617704</th>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.892632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         i   j   k  neg   predict\n",
       "5437480  7  32  40    0  7.116714\n",
       "5617704  7  32  40    1 -7.892632"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out[(df_out['i']==7) & (df_out['j']==32) & (df_out['k']==40)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5406720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406722</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406724</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767163</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767164</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767165</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767166</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5767167</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360448 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   j   k  neg  predict\n",
       "5406720   0   0   0    0      0.0\n",
       "5406721   0   0   1    0      0.0\n",
       "5406722   0   0   2    0      0.0\n",
       "5406723   0   0   3    0      0.0\n",
       "5406724   0   0   4    0      0.0\n",
       "...      ..  ..  ..  ...      ...\n",
       "5767163  43  63  59    1      0.0\n",
       "5767164  43  63  60    1      0.0\n",
       "5767165  43  63  61    1      0.0\n",
       "5767166  43  63  62    1      0.0\n",
       "5767167  43  63  63    1      0.0\n",
       "\n",
       "[360448 rows x 5 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = df_out[df_out['neg']==0]\n",
    "df_neg = df_out[df_out['neg']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>j</th>\n",
       "      <th>k</th>\n",
       "      <th>neg</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5406720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406721</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406722</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406723</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5406724</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586939</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586940</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586941</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586942</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586943</th>\n",
       "      <td>43</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180224 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          i   j   k  neg  predict\n",
       "5406720   0   0   0    0      0.0\n",
       "5406721   0   0   1    0      0.0\n",
       "5406722   0   0   2    0      0.0\n",
       "5406723   0   0   3    0      0.0\n",
       "5406724   0   0   4    0      0.0\n",
       "...      ..  ..  ..  ...      ...\n",
       "5586939  43  63  59    0      0.0\n",
       "5586940  43  63  60    0      0.0\n",
       "5586941  43  63  61    0      0.0\n",
       "5586942  43  63  62    0      0.0\n",
       "5586943  43  63  63    0      0.0\n",
       "\n",
       "[180224 rows x 5 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(img_path+\"nn_neg_both_nonzero_pos_300_sphere.txt\", df_pos[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")\n",
    "np.savetxt(img_path+\"nn_neg_both_nonzero_neg_300_sphere.txt\", df_neg[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(img_path+\"nn_neg_both_nonzero_pos_300_cyl.txt\", df_pos[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")\n",
    "np.savetxt(img_path+\"nn_neg_both_nonzero_neg_300_cyl.txt\", df_neg[['i', 'j', 'k', 'predict']], fmt=\"%i %i %i %s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
